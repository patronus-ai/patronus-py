diff --git a/examples/patronus_examples/experiments/evaluator_class.py b/examples/patronus_examples/experiments/evaluator_class.py
index 96fa99a..dfab010 100644
--- a/examples/patronus_examples/experiments/evaluator_class.py
+++ b/examples/patronus_examples/experiments/evaluator_class.py
@@ -51,7 +51,6 @@ class BERTScore(StructuredEvaluator):
             tags={"pass_threshold": str(self.pass_threshold)},
         )
 
-
 run_experiment(
     dataset=[
         {
diff --git a/examples/patronus_examples/experiments/evaluator_remote_pii.py b/examples/patronus_examples/experiments/evaluator_remote_pii.py
index fc17dd1..c6533bd 100644
--- a/examples/patronus_examples/experiments/evaluator_remote_pii.py
+++ b/examples/patronus_examples/experiments/evaluator_remote_pii.py
@@ -1,8 +1,6 @@
 from patronus.experiments import run_experiment
 from patronus.evals import RemoteEvaluator
 
-detect_pii = RemoteEvaluator("pii")
-
 run_experiment(
     project_name="Tutorial",
     dataset=[
@@ -15,6 +13,6 @@ run_experiment(
             "task_output": "My name is Jane Doe and I live at 123 Elm Street.",
         },
     ],
-    evaluators=[detect_pii],
+    evaluators=[RemoteEvaluator("pii")],
     experiment_name="Detect PII",
 )
diff --git a/examples/patronus_examples/experiments/weighted_evaluators.py b/examples/patronus_examples/experiments/weighted_evaluators.py
new file mode 100644
index 0000000..59d7938
--- /dev/null
+++ b/examples/patronus_examples/experiments/weighted_evaluators.py
@@ -0,0 +1,41 @@
+from patronus.experiments import FuncEvaluatorAdapter
+from patronus import RemoteEvaluator, EvaluationResult, StructuredEvaluator, evaluator
+import patronus
+from patronus.experiments import run_experiment, EvaluatorAdapter, Row
+
+
+class DummyEvaluator(StructuredEvaluator):
+    def evaluate(
+        self, task_output: str, gold_answer: str, **kwargs
+    ) -> EvaluationResult:
+        return EvaluationResult(
+            score_raw=1,
+            pass_=True,
+        )
+
+
+@evaluator
+def iexact_match(row: Row, **kwargs) -> bool:
+    return row.task_output.lower().strip() == row.gold_answer.lower().strip()
+
+run_experiment(
+    project_name="Tutorial",
+    dataset=[
+        {
+            "task_input": "Please provide your contact details.",
+            "task_output": "My email is john.doe@example.com and my phone number is 123-456-7890.",
+            "gold_answer": "My email is john.doe@example.com and my phone number is 123-456-7890.",
+        },
+        {
+            "task_input": "Share your personal information.",
+            "task_output": "My name is Jane Doe and I live at 123 Elm Street.",
+            "gold_answer": "My name is Jane Doe and I live at 123 Elm Street.",
+        },
+    ],
+    evaluators=[
+        RemoteEvaluator("pii", "patronus:pii:1", weight="0.3"),
+        FuncEvaluatorAdapter(iexact_match, weight="0.3"),
+        DummyEvaluator(weight="0.3"),
+    ],
+    experiment_name="Detect PII"
+)
diff --git a/src/patronus/api/api_client.py b/src/patronus/api/api_client.py
index b966723..e7f5c69 100644
--- a/src/patronus/api/api_client.py
+++ b/src/patronus/api/api_client.py
@@ -77,6 +77,29 @@ class PatronusAPIClient(BaseAPIClient):
         resp.raise_for_status()
         return resp.data.experiment
 
+
+    async def update_experiment(self, experiment_id: str, request: api_types.UpdateExperimentRequest) -> api_types.Experiment:
+        """Updates an existing experiment based on the given request."""
+        resp = await self.call(
+            "POST",
+            f"/v1/experiments/{experiment_id}",
+            body=request,
+            response_cls=api_types.UpdateExperimentResponse,
+        )
+        resp.raise_for_status()
+        return resp.data.experiment
+
+    def update_experiment_sync(self, experiment_id: str, request: api_types.UpdateExperimentRequest) -> api_types.Experiment:
+        """Updates an existing experiment based on the given request."""
+        resp = self.call_sync(
+            "POST",
+            f"/v1/experiments{experiment_id}",
+            body=request,
+            response_cls=api_types.UpdateExperimentResponse,
+        )
+        resp.raise_for_status()
+        return resp.data.experiment
+
     async def get_experiment(self, experiment_id: str) -> Optional[api_types.Experiment]:
         """Fetches an experiment by its ID or returns None if not found."""
         resp = await self.call(
@@ -230,15 +253,23 @@ class PatronusAPIClient(BaseAPIClient):
         resp.raise_for_status()
         return resp.data
 
-    async def list_evaluators(self) -> list[api_types.Evaluator]:
+    async def list_evaluators(self, by_alias_or_id: Optional[str] = None) -> list[api_types.Evaluator]:
         """Retrieves a list of available evaluators."""
-        resp = await self.call("GET", "/v1/evaluators", response_cls=api_types.ListEvaluatorsResponse)
+        params = {}
+        if by_alias_or_id:
+            params["by_alias_or_id"] = by_alias_or_id
+
+        resp = await self.call("GET", "/v1/evaluators", params=params, response_cls=api_types.ListEvaluatorsResponse)
         resp.raise_for_status()
         return resp.data.evaluators
 
-    def list_evaluators_sync(self) -> list[api_types.Evaluator]:
+    def list_evaluators_sync(self, by_alias_or_id: Optional[str] = None) -> list[api_types.Evaluator]:
         """Retrieves a list of available evaluators."""
-        resp = self.call_sync("GET", "/v1/evaluators", response_cls=api_types.ListEvaluatorsResponse)
+        params = {}
+        if by_alias_or_id:
+            params["by_alias_or_id"] = by_alias_or_id
+
+        resp = self.call_sync("GET", "/v1/evaluators", params=params, response_cls=api_types.ListEvaluatorsResponse)
         resp.raise_for_status()
         return resp.data.evaluators
 
diff --git a/src/patronus/api/api_types.py b/src/patronus/api/api_types.py
index dc175dd..8ca9599 100644
--- a/src/patronus/api/api_types.py
+++ b/src/patronus/api/api_types.py
@@ -77,18 +77,28 @@ class Experiment(pydantic.BaseModel):
     id: str
     name: str
     tags: Optional[dict[str, str]] = None
+    metadata: Optional[dict[str, typing.Any]] = None
 
 
 class CreateExperimentRequest(pydantic.BaseModel):
     project_id: str
     name: str
     tags: dict[str, str] = pydantic.Field(default_factory=dict)
+    metadata: Optional[dict[str, typing.Any]] = None
 
 
 class CreateExperimentResponse(pydantic.BaseModel):
     experiment: Experiment
 
 
+class UpdateExperimentRequest(pydantic.BaseModel):
+    metadata: dict[str, typing.Any]
+
+
+class UpdateExperimentResponse(pydantic.BaseModel):
+    experiment: Experiment
+
+
 class GetExperimentResponse(pydantic.BaseModel):
     experiment: Experiment
 
diff --git a/src/patronus/evals/evaluators.py b/src/patronus/evals/evaluators.py
index 7a86840..c8a32a1 100644
--- a/src/patronus/evals/evaluators.py
+++ b/src/patronus/evals/evaluators.py
@@ -9,6 +9,8 @@ import threading
 import time
 import typing
 import uuid
+from decimal import Decimal
+
 from opentelemetry.trace import get_current_span, SpanContext
 from typing import Any, Optional, Union
 
@@ -568,6 +570,14 @@ class Evaluator(metaclass=_EvaluatorMeta):
 
     evaluator_id: Optional[str] = None
     criteria: Optional[str] = None
+    weight: Optional[str] = None
+
+    def __init__(self, weight: Optional[str] = None):
+        try:
+            Decimal(weight)
+        except Decimal.Invalid:
+            raise TypeError(f"{weight} is not a valid weight.")
+        self.weight = weight
 
     def get_evaluator_id(self) -> str:
         return self.evaluator_id or self.__class__.__qualname__
@@ -583,6 +593,10 @@ class Evaluator(metaclass=_EvaluatorMeta):
         Return type should stay unchanged.
         """
 
+    @property
+    def canonical_name(self) -> str:
+        return f"{self.get_evaluator_id()}:{self.get_criteria() or ''}"
+
 
 class AsyncEvaluator(Evaluator):
     @abc.abstractmethod
@@ -632,6 +646,7 @@ class AsyncStructuredEvaluator(AsyncEvaluator):
 
 class RemoteEvaluatorMixin:
     _disable_export = True
+    _resolved = False
 
     def __init__(
         self,
@@ -644,8 +659,10 @@ class RemoteEvaluatorMixin:
         allow_update: bool = False,
         max_attempts: int = 3,
         api_: Optional[PatronusAPIClient] = None,
+        weight: Optional[str] = None,
     ):
         self.evaluator_id_or_alias = evaluator_id_or_alias
+        self.evaluator_id = None
         self.criteria = criteria
         self.tags = tags or {}
         self.explain_strategy = explain_strategy
@@ -653,13 +670,52 @@ class RemoteEvaluatorMixin:
         self.allow_update = allow_update
         self.max_attempts = max_attempts
         self._api = api_
+        self._resolved = False
+        self.weight = weight
 
     def get_evaluator_id(self) -> str:
-        return self.evaluator_id_or_alias
+        if not self._resolved:
+            self.resolve_evaluator()
+        return self.evaluator_id
 
     def get_criteria(self) -> str:
+        if not self._resolved:
+            self.resolve_evaluator()
         return self.criteria
 
+    def resolve_evaluator(self):
+        api = self._get_api()
+
+        # Get evaluator id by aliases
+        evaluators = api.list_evaluators_sync(by_alias_or_id=self.evaluator_id_or_alias)
+        if evaluators is not None:
+            self.evaluator_id = evaluators[0].id
+
+        # Get criteria with revision if revision is not provided
+        if self.criteria and not self.criteria.find(':'):
+            criteria = api.list_criteria_sync(
+                api_types.ListCriteriaRequest(
+                    name=self.criteria,
+                    get_last_revision=True
+                )
+            )
+            if not criteria:
+                raise RuntimeError(f"Criteria {self.criteria} not found")
+            self.criteria = f"{criteria[0].evaluator_criteria.name}:{criteria[0].evaluator_criteria.revision}"
+
+        # Get default criteria from evaluator if criteraia not provided
+        elif not self.criteria:
+            criteria = api.list_criteria_sync(
+                api_types.ListCriteriaRequest(
+                    name=evaluators[0].default_criteria,
+                    get_last_revision=True
+                )
+            )
+            if not criteria:
+                raise RuntimeError(f"Default criteria not found")
+            self.criteria = f"{criteria[0].evaluator_criteria.name}:{criteria[0].evaluator_criteria.revision}"
+        self._resolved = True
+
     def _get_api(self) -> PatronusAPIClient:
         api_client = self._api or context.get_api_client_deprecated_or_none()
         if api_client is None:
diff --git a/src/patronus/experiments/adapters.py b/src/patronus/experiments/adapters.py
index f1c5ccf..f0d039f 100644
--- a/src/patronus/experiments/adapters.py
+++ b/src/patronus/experiments/adapters.py
@@ -1,3 +1,4 @@
+import decimal
 import inspect
 
 import typing
@@ -5,7 +6,7 @@ import typing
 import asyncio
 
 import abc
-
+from decimal import Decimal
 
 from typing import Union, Optional
 
@@ -26,6 +27,7 @@ class BaseEvaluatorAdapter(abc.ABC):
     All concrete adapter implementations must inherit from this class and implement
     the required abstract methods.
     """
+    _weight: Optional[str] = None
 
     @property
     @abc.abstractmethod
@@ -37,6 +39,10 @@ class BaseEvaluatorAdapter(abc.ABC):
     def criteria(self) -> Optional[str]:
         pass
 
+    @property
+    def weight(self) -> Optional[str]:
+        return self._weight
+
     @property
     def canonical_name(self) -> str:
         return f"{self.evaluator_id}:{self.criteria or ''}"
@@ -122,6 +128,10 @@ class EvaluatorAdapter(BaseEvaluatorAdapter):
             raise TypeError(f"{evaluator} is not {evals.Evaluator.__name__}.")
         self.evaluator = evaluator
 
+    @property
+    def weight(self) -> Optional[str]:
+        return self.evaluator.weight
+
     @property
     def evaluator_id(self) -> str:
         return self.evaluator.get_evaluator_id()
@@ -313,14 +323,22 @@ class FuncEvaluatorAdapter(BaseEvaluatorAdapter):
 
     """
 
-    def __init__(self, fn: typing.Callable[..., typing.Any]):
+    def __init__(self, fn: typing.Callable[..., typing.Any], weight: Optional[str] = None):
         if not hasattr(fn, "_pat_evaluator"):
             raise ValueError(
                 f"Passed function {fn.__qualname__} is not an evaluator. "
                 "Hint: add @evaluator decorator to the function."
             )
-        self.fn = fn
 
+        try:
+            if weight is not None:
+                Decimal(weight)
+        except decimal.InvalidOperation:
+            raise TypeError(f"{weight} is not a valid decimal number.")
+
+        self.fn = fn
+        self._weight = weight
+    
     @property
     def evaluator_id(self) -> str:
         # @evaluator() wrapper sets that value
diff --git a/src/patronus/experiments/experiment.py b/src/patronus/experiments/experiment.py
index e9028ba..33d1caf 100644
--- a/src/patronus/experiments/experiment.py
+++ b/src/patronus/experiments/experiment.py
@@ -19,7 +19,7 @@ from patronus.context import get_tracer
 from patronus.datasets import Dataset, DatasetLoader
 from patronus.evals import StructuredEvaluator, AsyncStructuredEvaluator, bundled_eval, EvaluationResult
 from patronus.evals.context import evaluation_attributes
-from patronus.experiments.adapters import BaseEvaluatorAdapter, StructuredEvaluatorAdapter
+from patronus.experiments.adapters import BaseEvaluatorAdapter, StructuredEvaluatorAdapter, EvaluatorAdapter
 from patronus.experiments.async_utils import run_until_complete
 from patronus.experiments.reporter import Reporter
 from patronus.experiments.tqdm import AsyncTQDMWithHandle
@@ -235,6 +235,7 @@ class Experiment:
 
     _chain: list[_ChainLink]
     _started: bool
+    _prepared: bool
 
     _sem_tasks: asyncio.Semaphore
     _sem_evals: asyncio.Semaphore
@@ -256,6 +257,7 @@ class Experiment:
         evaluators: Optional[list[AdaptableEvaluators]] = None,
         chain: Optional[list[ChainLink]] = None,
         tags: Optional[dict[str, str]] = None,
+        metadata: Optional[dict[str, Any]] = None,
         max_concurrency: int = 10,
         project_name: Optional[str] = None,
         experiment_name: Optional[str] = None,
@@ -288,6 +290,7 @@ class Experiment:
         self.experiment = None
 
         self.tags = tags or {}
+        self.metadata = metadata
 
         self.max_concurrency = max_concurrency
 
@@ -312,6 +315,7 @@ class Experiment:
         evaluators: Optional[list[AdaptableEvaluators]] = None,
         chain: Optional[list[ChainLink]] = None,
         tags: Optional[Tags] = None,
+        metadata: Optional[dict[str, Any]] = None,
         max_concurrency: int = 10,
         project_name: Optional[str] = None,
         experiment_name: Optional[str] = None,
@@ -342,6 +346,8 @@ class Experiment:
                 Use this for multi-stage evaluation pipelines.
             tags: Key-value pairs.
                 All evaluations created by the experiment will contain these tags.
+            metadata: Arbitrary dict.
+                Metadata associated with the experiment.
             max_concurrency: Maximum number of concurrent task and evaluation operations.
             project_name: Name of the project to create or use. Falls back to configuration or
                 environment variables if not provided.
@@ -371,6 +377,7 @@ class Experiment:
             evaluators=evaluators,
             chain=chain,
             tags=tags,
+            metadata=metadata,
             max_concurrency=max_concurrency,
             project_name=project_name,
             experiment_name=experiment_name,
@@ -470,10 +477,13 @@ class Experiment:
             api_key=self._api_key or cfg.api_key,
         )
 
+        weights = await self._prepare_eval_weights()
+
         self.project = await self._get_or_create_project(api, self._project_name or cfg.project_name)
         self._project_name = None
 
-        self.experiment = await self._create_experiment(api, self.project.id, self._experiment_name, self.tags)
+        metadata = self.metadata.update(weights)
+        self.experiment = await self._create_experiment(api, self.project.id, self._experiment_name, self.tags, metadata)
         self._experiment_name = None
 
         ctx = build_context(
@@ -499,6 +509,17 @@ class Experiment:
         self._prepared = True
         return ctx
 
+    async def _prepare_eval_weights(self):
+        weights = {}
+        for link_dict in self._chain:
+            for evaluator in link_dict.get("evaluators", []):
+                if weights.setdefault(evaluator.canonical_name, evaluator.weight) != evaluator.weight:
+                    raise TypeError(f'You cannot set different weights for the same evaluator: `{evaluator.canonical_name}`')
+
+        return {
+            "evaluator_weights": weights
+        }
+
     async def _run(self):
         title = f"Experiment  {self.project.name}/{self.experiment.name}"
         print("=" * len(title))
@@ -537,7 +558,7 @@ class Experiment:
 
     @staticmethod
     async def _create_experiment(
-        api: PatronusAPIClient, project_id: str, experiment_name: str, tags: Tags
+        api: PatronusAPIClient, project_id: str, experiment_name: str, tags: Tags, metadata: Optional[dict[str, Any]]
     ) -> api_types.Experiment:
         name = generate_experiment_name(experiment_name)
         return await api.create_experiment(
@@ -545,6 +566,7 @@ class Experiment:
                 project_id=project_id,
                 name=name,
                 tags=tags,
+                metadata=metadata
             )
         )
 
diff --git a/src/patronus/init.py b/src/patronus/init.py
index 95d2616..c8ec645 100644
--- a/src/patronus/init.py
+++ b/src/patronus/init.py
@@ -210,7 +210,7 @@ def build_context(
         async_api_client=async_api_client,
         exporter=eval_exporter,
         prompts=context.PromptsConfig(
-            directory=pathlib.Path(resource_dir, "prompts"),
+            directory=resource_dir and pathlib.Path(resource_dir, "prompts"),
             providers=prompt_providers,
             templating_engine=prompt_templating_engine,
         ),
