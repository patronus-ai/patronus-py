{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Patronus Python SDK Documentation","text":"<p>The Patronus SDK provides tools for observability, evaluation, and experimentation with Large Language Models (LLMs), helping you build reliable and high-quality AI applications.</p> <p>This documentation covers only the Python SDK. For comprehensive documentation on the Patronus platform, evaluators, and best practices, please visit the official Patronus documentation.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation and Quickstart - Install the SDK and run your first evaluation</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>The Patronus Experimentation Framework offers several configuration options that can be set in the following ways:</p> <ol> <li>In Code</li> <li>Environment Variables</li> <li>YAML Configuration File</li> </ol> <p>Configuration options are prioritized in the order listed above.</p> Config name Environment Variable Default Value project_name PATRONUS_PROJECT_NAE <code>Global</code> app PATRONUS_APP <code>default</code> api_key PATRONUS_API_KEY api_url PATRONUS_API_URL <code>https://api.patronus.ai</code> ui_url PATRONUS_UI_URL <code>https://app.patronus.ai</code> otel_endpoint PATRONUS_OTEL_ENDPOINT <code>https://otel.patronus.ai:4317</code> timeout_s PATRONUS_TIMEOUT_S <code>300</code>"},{"location":"configuration/#configuration-file-patronusyaml","title":"Configuration File (<code>patronus.yaml</code>)","text":"<p>You can also provide configuration options using a patronus.yaml file. This file must be present in the working directory when executing your script.</p> <pre><code>project_name: \"my-project\"\napp: \"my-agent\"\n\napi_key: \"YOUR_API_KEY\"\napi_url: \"https://api.patronus.ai\"\nui_url: \"https://app.patronus.ai\"\n</code></pre>"},{"location":"api_ref/api_client/","title":"API","text":""},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient","title":"patronus.api.api_client.PatronusAPIClient","text":"<pre><code>PatronusAPIClient(*, client_http_async: AsyncClient, client_http: Client, base_url: str, api_key: str)\n</code></pre> <p>               Bases: <code>BaseAPIClient</code></p> Source code in <code>src/patronus/api/api_client_base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    client_http_async: httpx.AsyncClient,\n    client_http: httpx.Client,\n    base_url: str,\n    api_key: str,\n):\n    self.version = importlib.metadata.version(\"patronus\")\n    self.http = client_http_async\n    self.http_sync = client_http\n    self.base_url = base_url.rstrip(\"/\")\n    self.api_key = api_key\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.add_evaluator_criteria_revision","title":"add_evaluator_criteria_revision  <code>async</code>","text":"<pre><code>add_evaluator_criteria_revision(evaluator_criteria_id, request: AddEvaluatorCriteriaRevisionRequest) -&gt; AddEvaluatorCriteriaRevisionResponse\n</code></pre> <p>Adds a revision to existing evaluator criteria.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def add_evaluator_criteria_revision(\n    self,\n    evaluator_criteria_id,\n    request: api_types.AddEvaluatorCriteriaRevisionRequest,\n) -&gt; api_types.AddEvaluatorCriteriaRevisionResponse:\n    \"\"\"Adds a revision to existing evaluator criteria.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        f\"/v1/evaluator-criteria/{evaluator_criteria_id}/revision\",\n        body=request,\n        response_cls=api_types.AddEvaluatorCriteriaRevisionResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.add_evaluator_criteria_revision_sync","title":"add_evaluator_criteria_revision_sync","text":"<pre><code>add_evaluator_criteria_revision_sync(evaluator_criteria_id, request: AddEvaluatorCriteriaRevisionRequest) -&gt; AddEvaluatorCriteriaRevisionResponse\n</code></pre> <p>Adds a revision to existing evaluator criteria.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def add_evaluator_criteria_revision_sync(\n    self,\n    evaluator_criteria_id,\n    request: api_types.AddEvaluatorCriteriaRevisionRequest,\n) -&gt; api_types.AddEvaluatorCriteriaRevisionResponse:\n    \"\"\"Adds a revision to existing evaluator criteria.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        f\"/v1/evaluator-criteria/{evaluator_criteria_id}/revision\",\n        body=request,\n        response_cls=api_types.AddEvaluatorCriteriaRevisionResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.annotate","title":"annotate  <code>async</code>","text":"<pre><code>annotate(request: AnnotateRequest) -&gt; AnnotateResponse\n</code></pre> <p>Annotates log based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def annotate(self, request: api_types.AnnotateRequest) -&gt; api_types.AnnotateResponse:\n    \"\"\"Annotates log based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/annotate\",\n        body=request,\n        response_cls=api_types.AnnotateResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.annotate_sync","title":"annotate_sync","text":"<pre><code>annotate_sync(request: AnnotateRequest) -&gt; AnnotateResponse\n</code></pre> <p>Annotates log based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def annotate_sync(self, request: api_types.AnnotateRequest) -&gt; api_types.AnnotateResponse:\n    \"\"\"Annotates log based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/annotate\",\n        body=request,\n        response_cls=api_types.AnnotateResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.batch_create_evaluations","title":"batch_create_evaluations  <code>async</code>","text":"<pre><code>batch_create_evaluations(request: BatchCreateEvaluationsRequest) -&gt; BatchCreateEvaluationsResponse\n</code></pre> <p>Creates multiple evaluations in a single request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def batch_create_evaluations(\n    self, request: api_types.BatchCreateEvaluationsRequest\n) -&gt; api_types.BatchCreateEvaluationsResponse:\n    \"\"\"Creates multiple evaluations in a single request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluations/batch\",\n        body=request,\n        response_cls=api_types.BatchCreateEvaluationsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.batch_create_evaluations_sync","title":"batch_create_evaluations_sync","text":"<pre><code>batch_create_evaluations_sync(request: BatchCreateEvaluationsRequest) -&gt; BatchCreateEvaluationsResponse\n</code></pre> <p>Creates multiple evaluations in a single request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def batch_create_evaluations_sync(\n    self, request: api_types.BatchCreateEvaluationsRequest\n) -&gt; api_types.BatchCreateEvaluationsResponse:\n    \"\"\"Creates multiple evaluations in a single request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluations/batch\",\n        body=request,\n        response_cls=api_types.BatchCreateEvaluationsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_annotation_criteria","title":"create_annotation_criteria  <code>async</code>","text":"<pre><code>create_annotation_criteria(request: CreateAnnotationCriteriaRequest) -&gt; CreateAnnotationCriteriaResponse\n</code></pre> <p>Creates annotation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def create_annotation_criteria(\n    self, request: api_types.CreateAnnotationCriteriaRequest\n) -&gt; api_types.CreateAnnotationCriteriaResponse:\n    \"\"\"Creates annotation criteria based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/annotation-criteria\",\n        body=request,\n        response_cls=api_types.CreateAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_annotation_criteria_sync","title":"create_annotation_criteria_sync","text":"<pre><code>create_annotation_criteria_sync(request: CreateAnnotationCriteriaRequest) -&gt; CreateAnnotationCriteriaResponse\n</code></pre> <p>Creates annotation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def create_annotation_criteria_sync(\n    self, request: api_types.CreateAnnotationCriteriaRequest\n) -&gt; api_types.CreateAnnotationCriteriaResponse:\n    \"\"\"Creates annotation criteria based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/annotation-criteria\",\n        body=request,\n        response_cls=api_types.CreateAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_criteria","title":"create_criteria  <code>async</code>","text":"<pre><code>create_criteria(request: CreateCriteriaRequest) -&gt; CreateCriteriaResponse\n</code></pre> <p>Creates evaluation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def create_criteria(self, request: api_types.CreateCriteriaRequest) -&gt; api_types.CreateCriteriaResponse:\n    \"\"\"Creates evaluation criteria based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluator-criteria\",\n        body=request,\n        response_cls=api_types.CreateCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_criteria_sync","title":"create_criteria_sync","text":"<pre><code>create_criteria_sync(request: CreateCriteriaRequest) -&gt; CreateCriteriaResponse\n</code></pre> <p>Creates evaluation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def create_criteria_sync(self, request: api_types.CreateCriteriaRequest) -&gt; api_types.CreateCriteriaResponse:\n    \"\"\"Creates evaluation criteria based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluator-criteria\",\n        body=request,\n        response_cls=api_types.CreateCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_experiment","title":"create_experiment  <code>async</code>","text":"<pre><code>create_experiment(request: CreateExperimentRequest) -&gt; Experiment\n</code></pre> <p>Creates a new experiment based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def create_experiment(self, request: api_types.CreateExperimentRequest) -&gt; api_types.Experiment:\n    \"\"\"Creates a new experiment based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/experiments\",\n        body=request,\n        response_cls=api_types.CreateExperimentResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_experiment_sync","title":"create_experiment_sync","text":"<pre><code>create_experiment_sync(request: CreateExperimentRequest) -&gt; Experiment\n</code></pre> <p>Creates a new experiment based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def create_experiment_sync(self, request: api_types.CreateExperimentRequest) -&gt; api_types.Experiment:\n    \"\"\"Creates a new experiment based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/experiments\",\n        body=request,\n        response_cls=api_types.CreateExperimentResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_project","title":"create_project  <code>async</code>","text":"<pre><code>create_project(request: CreateProjectRequest) -&gt; Project\n</code></pre> <p>Creates a new project based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def create_project(self, request: api_types.CreateProjectRequest) -&gt; api_types.Project:\n    \"\"\"Creates a new project based on the given request.\"\"\"\n    resp = await self.call(\"POST\", \"/v1/projects\", body=request, response_cls=api_types.Project)\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_project_sync","title":"create_project_sync","text":"<pre><code>create_project_sync(request: CreateProjectRequest) -&gt; Project\n</code></pre> <p>Creates a new project based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def create_project_sync(self, request: api_types.CreateProjectRequest) -&gt; api_types.Project:\n    \"\"\"Creates a new project based on the given request.\"\"\"\n    resp = self.call_sync(\"POST\", \"/v1/projects\", body=request, response_cls=api_types.Project)\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.delete_annotation_criteria","title":"delete_annotation_criteria  <code>async</code>","text":"<pre><code>delete_annotation_criteria(criteria_id: str) -&gt; None\n</code></pre> <p>Deletes annotation criteria by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def delete_annotation_criteria(self, criteria_id: str) -&gt; None:\n    \"\"\"Deletes annotation criteria by its ID.\"\"\"\n    resp = await self.call(\n        \"DELETE\",\n        f\"/v1/annotation-criteria/{criteria_id}\",\n        response_cls=None,\n    )\n    resp.raise_for_status()\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.delete_annotation_criteria_sync","title":"delete_annotation_criteria_sync","text":"<pre><code>delete_annotation_criteria_sync(criteria_id: str) -&gt; None\n</code></pre> <p>Deletes annotation criteria by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def delete_annotation_criteria_sync(self, criteria_id: str) -&gt; None:\n    \"\"\"Deletes annotation criteria by its ID.\"\"\"\n    resp = self.call_sync(\n        \"DELETE\",\n        f\"/v1/annotation-criteria/{criteria_id}\",\n        response_cls=None,\n    )\n    resp.raise_for_status()\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(request: EvaluateRequest) -&gt; EvaluateResponse\n</code></pre> <p>Evaluates content using the specified evaluators.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def evaluate(self, request: api_types.EvaluateRequest) -&gt; api_types.EvaluateResponse:\n    \"\"\"Evaluates content using the specified evaluators.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluate\",\n        body=request,\n        response_cls=api_types.EvaluateResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.evaluate_one","title":"evaluate_one  <code>async</code>","text":"<pre><code>evaluate_one(request: EvaluateRequest) -&gt; EvaluationResult\n</code></pre> <p>Evaluates content using a single evaluator.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def evaluate_one(self, request: api_types.EvaluateRequest) -&gt; api_types.EvaluationResult:\n    \"\"\"Evaluates content using a single evaluator.\"\"\"\n    if len(request.evaluators) &gt; 1:\n        raise ValueError(\"'evaluate_one()' cannot accept more than one evaluator in the request body\")\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluate\",\n        body=request,\n        response_cls=api_types.EvaluateResponse,\n    )\n    return self._evaluate_one_process_resp(resp)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.evaluate_one_sync","title":"evaluate_one_sync","text":"<pre><code>evaluate_one_sync(request: EvaluateRequest) -&gt; EvaluationResult\n</code></pre> <p>Evaluates content using a single evaluator.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def evaluate_one_sync(self, request: api_types.EvaluateRequest) -&gt; api_types.EvaluationResult:\n    \"\"\"Evaluates content using a single evaluator.\"\"\"\n    if len(request.evaluators) &gt; 1:\n        raise ValueError(\"'evaluate_one_sync()' cannot accept more than one evaluator in the request body\")\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluate\",\n        body=request,\n        response_cls=api_types.EvaluateResponse,\n    )\n    return self._evaluate_one_process_resp(resp)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.evaluate_sync","title":"evaluate_sync","text":"<pre><code>evaluate_sync(request: EvaluateRequest) -&gt; EvaluateResponse\n</code></pre> <p>Evaluates content using the specified evaluators.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def evaluate_sync(self, request: api_types.EvaluateRequest) -&gt; api_types.EvaluateResponse:\n    \"\"\"Evaluates content using the specified evaluators.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluate\",\n        body=request,\n        response_cls=api_types.EvaluateResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.export_evaluations","title":"export_evaluations  <code>async</code>","text":"<pre><code>export_evaluations(request: ExportEvaluationRequest) -&gt; ExportEvaluationResponse\n</code></pre> <p>Exports evaluations based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def export_evaluations(\n    self, request: api_types.ExportEvaluationRequest\n) -&gt; api_types.ExportEvaluationResponse:\n    \"\"\"Exports evaluations based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluation-results/batch\",\n        body=request,\n        response_cls=api_types.ExportEvaluationResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.export_evaluations_sync","title":"export_evaluations_sync","text":"<pre><code>export_evaluations_sync(request: ExportEvaluationRequest) -&gt; ExportEvaluationResponse\n</code></pre> <p>Exports evaluations based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def export_evaluations_sync(self, request: api_types.ExportEvaluationRequest) -&gt; api_types.ExportEvaluationResponse:\n    \"\"\"Exports evaluations based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluation-results/batch\",\n        body=request,\n        response_cls=api_types.ExportEvaluationResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.get_experiment","title":"get_experiment  <code>async</code>","text":"<pre><code>get_experiment(experiment_id: str) -&gt; Optional[Experiment]\n</code></pre> <p>Fetches an experiment by its ID or returns None if not found.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def get_experiment(self, experiment_id: str) -&gt; Optional[api_types.Experiment]:\n    \"\"\"Fetches an experiment by its ID or returns None if not found.\"\"\"\n    resp = await self.call(\n        \"GET\",\n        f\"/v1/experiments/{experiment_id}\",\n        response_cls=api_types.GetExperimentResponse,\n    )\n    if resp.response.status_code == 404:\n        return None\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.get_experiment_sync","title":"get_experiment_sync","text":"<pre><code>get_experiment_sync(experiment_id: str) -&gt; Optional[Experiment]\n</code></pre> <p>Fetches an experiment by its ID or returns None if not found.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def get_experiment_sync(self, experiment_id: str) -&gt; Optional[api_types.Experiment]:\n    \"\"\"Fetches an experiment by its ID or returns None if not found.\"\"\"\n    resp = self.call_sync(\n        \"GET\",\n        f\"/v1/experiments/{experiment_id}\",\n        response_cls=api_types.GetExperimentResponse,\n    )\n    if resp.response.status_code == 404:\n        return None\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.get_project","title":"get_project  <code>async</code>","text":"<pre><code>get_project(project_id: str) -&gt; Project\n</code></pre> <p>Fetches a project by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def get_project(self, project_id: str) -&gt; api_types.Project:\n    \"\"\"Fetches a project by its ID.\"\"\"\n    resp = await self.call(\n        \"GET\",\n        f\"/v1/projects/{project_id}\",\n        response_cls=api_types.GetProjectResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.project\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.get_project_sync","title":"get_project_sync","text":"<pre><code>get_project_sync(project_id: str) -&gt; Project\n</code></pre> <p>Fetches a project by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def get_project_sync(self, project_id: str) -&gt; api_types.Project:\n    \"\"\"Fetches a project by its ID.\"\"\"\n    resp = self.call_sync(\n        \"GET\",\n        f\"/v1/projects/{project_id}\",\n        response_cls=api_types.GetProjectResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.project\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_annotation_criteria","title":"list_annotation_criteria  <code>async</code>","text":"<pre><code>list_annotation_criteria(*, project_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None) -&gt; ListAnnotationCriteriaResponse\n</code></pre> <p>Retrieves a list of annotation criteria with optional filtering.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_annotation_criteria(\n    self, *, project_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None\n) -&gt; api_types.ListAnnotationCriteriaResponse:\n    \"\"\"Retrieves a list of annotation criteria with optional filtering.\"\"\"\n    params = {}\n    if project_id is not None:\n        params[\"project_id\"] = project_id\n    if limit is not None:\n        params[\"limit\"] = limit\n    if offset is not None:\n        params[\"offset\"] = offset\n    resp = await self.call(\n        \"GET\",\n        \"/v1/annotation-criteria\",\n        params=params,\n        response_cls=api_types.ListAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_annotation_criteria_sync","title":"list_annotation_criteria_sync","text":"<pre><code>list_annotation_criteria_sync(*, project_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None) -&gt; ListAnnotationCriteriaResponse\n</code></pre> <p>Retrieves a list of annotation criteria with optional filtering.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_annotation_criteria_sync(\n    self, *, project_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None\n) -&gt; api_types.ListAnnotationCriteriaResponse:\n    \"\"\"Retrieves a list of annotation criteria with optional filtering.\"\"\"\n    params = {}\n    if project_id is not None:\n        params[\"project_id\"] = project_id\n    if limit is not None:\n        params[\"limit\"] = limit\n    if offset is not None:\n        params[\"offset\"] = offset\n    resp = self.call_sync(\n        \"GET\",\n        \"/v1/annotation-criteria\",\n        params=params,\n        response_cls=api_types.ListAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_criteria","title":"list_criteria  <code>async</code>","text":"<pre><code>list_criteria(request: ListCriteriaRequest) -&gt; ListCriteriaResponse\n</code></pre> <p>Retrieves a list of evaluation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_criteria(self, request: api_types.ListCriteriaRequest) -&gt; api_types.ListCriteriaResponse:\n    \"\"\"Retrieves a list of evaluation criteria based on the given request.\"\"\"\n    params = request.model_dump(exclude_none=True)\n    resp = await self.call(\n        \"GET\",\n        \"/v1/evaluator-criteria\",\n        params=params,\n        response_cls=api_types.ListCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_criteria_sync","title":"list_criteria_sync","text":"<pre><code>list_criteria_sync(request: ListCriteriaRequest) -&gt; ListCriteriaResponse\n</code></pre> <p>Retrieves a list of evaluation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_criteria_sync(self, request: api_types.ListCriteriaRequest) -&gt; api_types.ListCriteriaResponse:\n    \"\"\"Retrieves a list of evaluation criteria based on the given request.\"\"\"\n    params = request.model_dump(exclude_none=True)\n    resp = self.call_sync(\n        \"GET\",\n        \"/v1/evaluator-criteria\",\n        params=params,\n        response_cls=api_types.ListCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_dataset_data","title":"list_dataset_data  <code>async</code>","text":"<pre><code>list_dataset_data(dataset_id: str) -&gt; ListDatasetData\n</code></pre> <p>Retrieves data from a dataset by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_dataset_data(self, dataset_id: str) -&gt; api_types.ListDatasetData:\n    \"\"\"Retrieves data from a dataset by its ID.\"\"\"\n    resp = await self.call(\n        \"GET\",\n        f\"/v1/datasets/{dataset_id}/data\",\n        response_cls=api_types.ListDatasetData,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_dataset_data_sync","title":"list_dataset_data_sync","text":"<pre><code>list_dataset_data_sync(dataset_id: str) -&gt; ListDatasetData\n</code></pre> <p>Retrieves data from a dataset by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_dataset_data_sync(self, dataset_id: str) -&gt; api_types.ListDatasetData:\n    \"\"\"Retrieves data from a dataset by its ID.\"\"\"\n    resp = self.call_sync(\n        \"GET\",\n        f\"/v1/datasets/{dataset_id}/data\",\n        response_cls=api_types.ListDatasetData,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_evaluators","title":"list_evaluators  <code>async</code>","text":"<pre><code>list_evaluators() -&gt; list[Evaluator]\n</code></pre> <p>Retrieves a list of available evaluators.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_evaluators(self) -&gt; list[api_types.Evaluator]:\n    \"\"\"Retrieves a list of available evaluators.\"\"\"\n    resp = await self.call(\"GET\", \"/v1/evaluators\", response_cls=api_types.ListEvaluatorsResponse)\n    resp.raise_for_status()\n    return resp.data.evaluators\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_evaluators_sync","title":"list_evaluators_sync","text":"<pre><code>list_evaluators_sync() -&gt; list[Evaluator]\n</code></pre> <p>Retrieves a list of available evaluators.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_evaluators_sync(self) -&gt; list[api_types.Evaluator]:\n    \"\"\"Retrieves a list of available evaluators.\"\"\"\n    resp = self.call_sync(\"GET\", \"/v1/evaluators\", response_cls=api_types.ListEvaluatorsResponse)\n    resp.raise_for_status()\n    return resp.data.evaluators\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.search_evaluations","title":"search_evaluations  <code>async</code>","text":"<pre><code>search_evaluations(request: SearchEvaluationsRequest) -&gt; SearchEvaluationsResponse\n</code></pre> <p>Searches for evaluations based on the given criteria.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def search_evaluations(\n    self, request: api_types.SearchEvaluationsRequest\n) -&gt; api_types.SearchEvaluationsResponse:\n    \"\"\"Searches for evaluations based on the given criteria.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluations/search\",\n        body=request,\n        response_cls=api_types.SearchEvaluationsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.search_evaluations_sync","title":"search_evaluations_sync","text":"<pre><code>search_evaluations_sync(request: SearchEvaluationsRequest) -&gt; SearchEvaluationsResponse\n</code></pre> <p>Searches for evaluations based on the given criteria.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def search_evaluations_sync(\n    self, request: api_types.SearchEvaluationsRequest\n) -&gt; api_types.SearchEvaluationsResponse:\n    \"\"\"Searches for evaluations based on the given criteria.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluations/search\",\n        body=request,\n        response_cls=api_types.SearchEvaluationsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.search_logs","title":"search_logs  <code>async</code>","text":"<pre><code>search_logs(request: SearchLogsRequest) -&gt; SearchLogsResponse\n</code></pre> <p>Searches for logs based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def search_logs(self, request: api_types.SearchLogsRequest) -&gt; api_types.SearchLogsResponse:\n    \"\"\"Searches for logs based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/otel/logs/search\",\n        body=request,\n        response_cls=api_types.SearchLogsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.search_logs_sync","title":"search_logs_sync","text":"<pre><code>search_logs_sync(request: SearchLogsRequest) -&gt; SearchLogsResponse\n</code></pre> <p>Searches for logs based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def search_logs_sync(self, request: api_types.SearchLogsRequest) -&gt; api_types.SearchLogsResponse:\n    \"\"\"Searches for logs based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/otel/logs/search\",\n        body=request,\n        response_cls=api_types.SearchLogsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.update_annotation_criteria","title":"update_annotation_criteria  <code>async</code>","text":"<pre><code>update_annotation_criteria(criteria_id: str, request: UpdateAnnotationCriteriaRequest) -&gt; UpdateAnnotationCriteriaResponse\n</code></pre> <p>Creates annotation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def update_annotation_criteria(\n    self, criteria_id: str, request: api_types.UpdateAnnotationCriteriaRequest\n) -&gt; api_types.UpdateAnnotationCriteriaResponse:\n    \"\"\"Creates annotation criteria based on the given request.\"\"\"\n    resp = await self.call(\n        \"PUT\",\n        f\"/v1/annotation-criteria/{criteria_id}\",\n        body=request,\n        response_cls=api_types.UpdateAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.update_annotation_criteria_sync","title":"update_annotation_criteria_sync","text":"<pre><code>update_annotation_criteria_sync(criteria_id: str, request: UpdateAnnotationCriteriaRequest) -&gt; UpdateAnnotationCriteriaResponse\n</code></pre> <p>Creates annotation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def update_annotation_criteria_sync(\n    self, criteria_id: str, request: api_types.UpdateAnnotationCriteriaRequest\n) -&gt; api_types.UpdateAnnotationCriteriaResponse:\n    \"\"\"Creates annotation criteria based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"PUT\",\n        f\"/v1/annotation-criteria/{criteria_id}\",\n        body=request,\n        response_cls=api_types.UpdateAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.whoami","title":"whoami  <code>async</code>","text":"<pre><code>whoami() -&gt; WhoAmIResponse\n</code></pre> <p>Fetches information about the authenticated user.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def whoami(self) -&gt; api_types.WhoAmIResponse:\n    \"\"\"Fetches information about the authenticated user.\"\"\"\n    resp = await self.call(\"GET\", \"/v1/whoami\", response_cls=api_types.WhoAmIResponse)\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.whoami_sync","title":"whoami_sync","text":"<pre><code>whoami_sync() -&gt; WhoAmIResponse\n</code></pre> <p>Fetches information about the authenticated user.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def whoami_sync(self) -&gt; api_types.WhoAmIResponse:\n    \"\"\"Fetches information about the authenticated user.\"\"\"\n    resp = self.call_sync(\"GET\", \"/v1/whoami\", response_cls=api_types.WhoAmIResponse)\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/evals/","title":"evals","text":""},{"location":"api_ref/evals/#patronus.evals","title":"patronus.evals","text":""},{"location":"api_ref/evals/#patronus.evals.evaluators","title":"evaluators","text":""},{"location":"api_ref/evals/#patronus.evals.evaluators.Evaluator","title":"Evaluator","text":"<p>Base Evaluator Class</p>"},{"location":"api_ref/evals/#patronus.evals.evaluators.Evaluator.evaluate","title":"evaluate  <code>abstractmethod</code>","text":"<pre><code>evaluate(*args, **kwargs) -&gt; Optional[EvaluationResult]\n</code></pre> <p>Synchronous version of evaluate method. When inheriting directly from Evaluator class it's permitted to change parameters signature. Return type should stay unchanged.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>@abc.abstractmethod\ndef evaluate(self, *args, **kwargs) -&gt; Optional[EvaluationResult]:\n    \"\"\"\n    Synchronous version of evaluate method.\n    When inheriting directly from Evaluator class it's permitted to change parameters signature.\n    Return type should stay unchanged.\n    \"\"\"\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncEvaluator","title":"AsyncEvaluator","text":"<p>               Bases: <code>Evaluator</code></p>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncEvaluator.evaluate","title":"evaluate  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>evaluate(*args, **kwargs) -&gt; Optional[EvaluationResult]\n</code></pre> <p>Asynchronous version of evaluate method. When inheriting directly from Evaluator class it's permitted to change parameters signature. Return type should stay unchanged.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>@abc.abstractmethod\nasync def evaluate(self, *args, **kwargs) -&gt; Optional[EvaluationResult]:\n    \"\"\"\n    Asynchronous version of evaluate method.\n    When inheriting directly from Evaluator class it's permitted to change parameters signature.\n    Return type should stay unchanged.\n    \"\"\"\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.StructuredEvaluator","title":"StructuredEvaluator","text":"<p>               Bases: <code>Evaluator</code></p> <p>Base for structured evaluators</p>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncStructuredEvaluator","title":"AsyncStructuredEvaluator","text":"<p>               Bases: <code>AsyncEvaluator</code></p> <p>Base for async structured evaluators</p>"},{"location":"api_ref/evals/#patronus.evals.evaluators.RemoteEvaluator","title":"RemoteEvaluator","text":"<pre><code>RemoteEvaluator(evaluator_id_or_alias: str, criteria: Optional[str] = None, *, tags: Optional[dict[str, str]] = None, explain_strategy: Literal['never', 'on-fail', 'on-success', 'always'] = 'always', criteria_config: Optional[dict[str, Any]] = None, allow_update: bool = False, max_attempts: int = 3, api_: Optional[PatronusAPIClient] = None)\n</code></pre> <p>               Bases: <code>RemoteEvaluatorMixin</code>, <code>StructuredEvaluator</code></p> <p>Synchronous remote evaluator</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(\n    self,\n    evaluator_id_or_alias: str,\n    criteria: Optional[str] = None,\n    *,\n    tags: Optional[dict[str, str]] = None,\n    explain_strategy: typing.Literal[\"never\", \"on-fail\", \"on-success\", \"always\"] = \"always\",\n    criteria_config: Optional[dict[str, typing.Any]] = None,\n    allow_update: bool = False,\n    max_attempts: int = 3,\n    api_: Optional[PatronusAPIClient] = None,\n):\n    self.evaluator_id_or_alias = evaluator_id_or_alias\n    self.criteria = criteria\n    self.tags = tags or {}\n    self.explain_strategy = explain_strategy\n    self.criteria_config = criteria_config\n    self.allow_update = allow_update\n    self.max_attempts = max_attempts\n    self._api = api_\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.RemoteEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(*, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_attachments: Union[list[Any], None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -&gt; EvaluationResult\n</code></pre> <p>Evaluates data using remote Patronus Evaluator</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def evaluate(\n    self,\n    *,\n    system_prompt: Optional[str] = None,\n    task_context: Union[list[str], str, None] = None,\n    task_attachments: Union[list[Any], None] = None,\n    task_input: Optional[str] = None,\n    task_output: Optional[str] = None,\n    gold_answer: Optional[str] = None,\n    task_metadata: Optional[typing.Dict[str, typing.Any]] = None,\n    **kwargs: Any,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluates data using remote Patronus Evaluator\"\"\"\n    kws = {\n        \"system_prompt\": system_prompt,\n        \"task_context\": task_context,\n        \"task_attachments\": task_attachments,\n        \"task_input\": task_input,\n        \"task_output\": task_output,\n        \"gold_answer\": gold_answer,\n        \"task_metadata\": task_metadata,\n        **kwargs,\n    }\n    log_id = get_current_log_id(bound_arguments=kws)\n\n    attrs = get_context_evaluation_attributes()\n    tags = {**self.tags}\n    if t := attrs[\"tags\"]:\n        tags.update(t)\n    tags = merge_tags(tags, kwargs.get(\"tags\"), attrs[\"experiment_tags\"])\n    if tags:\n        kws[\"tags\"] = tags\n    if did := attrs[\"dataset_id\"]:\n        kws[\"dataset_id\"] = did\n    if sid := attrs[\"dataset_sample_id\"]:\n        kws[\"dataset_sample_id\"] = sid\n\n    resp = retry()(self._evaluate)(log_id=log_id, **kws)\n    return self._translate_response(resp)\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncRemoteEvaluator","title":"AsyncRemoteEvaluator","text":"<pre><code>AsyncRemoteEvaluator(evaluator_id_or_alias: str, criteria: Optional[str] = None, *, tags: Optional[dict[str, str]] = None, explain_strategy: Literal['never', 'on-fail', 'on-success', 'always'] = 'always', criteria_config: Optional[dict[str, Any]] = None, allow_update: bool = False, max_attempts: int = 3, api_: Optional[PatronusAPIClient] = None)\n</code></pre> <p>               Bases: <code>RemoteEvaluatorMixin</code>, <code>AsyncStructuredEvaluator</code></p> <p>Asynchronous remote evaluator</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(\n    self,\n    evaluator_id_or_alias: str,\n    criteria: Optional[str] = None,\n    *,\n    tags: Optional[dict[str, str]] = None,\n    explain_strategy: typing.Literal[\"never\", \"on-fail\", \"on-success\", \"always\"] = \"always\",\n    criteria_config: Optional[dict[str, typing.Any]] = None,\n    allow_update: bool = False,\n    max_attempts: int = 3,\n    api_: Optional[PatronusAPIClient] = None,\n):\n    self.evaluator_id_or_alias = evaluator_id_or_alias\n    self.criteria = criteria\n    self.tags = tags or {}\n    self.explain_strategy = explain_strategy\n    self.criteria_config = criteria_config\n    self.allow_update = allow_update\n    self.max_attempts = max_attempts\n    self._api = api_\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncRemoteEvaluator.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(*, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_attachments: Union[list[Any], None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -&gt; EvaluationResult\n</code></pre> <p>Evaluates data using remote Patronus Evaluator</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>async def evaluate(\n    self,\n    *,\n    system_prompt: Optional[str] = None,\n    task_context: Union[list[str], str, None] = None,\n    task_attachments: Union[list[Any], None] = None,\n    task_input: Optional[str] = None,\n    task_output: Optional[str] = None,\n    gold_answer: Optional[str] = None,\n    task_metadata: Optional[typing.Dict[str, typing.Any]] = None,\n    **kwargs: Any,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluates data using remote Patronus Evaluator\"\"\"\n    kws = {\n        \"system_prompt\": system_prompt,\n        \"task_context\": task_context,\n        \"task_attachments\": task_attachments,\n        \"task_input\": task_input,\n        \"task_output\": task_output,\n        \"gold_answer\": gold_answer,\n        \"task_metadata\": task_metadata,\n        **kwargs,\n    }\n    log_id = get_current_log_id(bound_arguments=kws)\n\n    attrs = get_context_evaluation_attributes()\n    tags = {**self.tags}\n    if t := attrs[\"tags\"]:\n        tags.update(t)\n    tags = merge_tags(tags, kwargs.get(\"tags\"), attrs[\"experiment_tags\"])\n    if tags:\n        kws[\"tags\"] = tags\n    if did := attrs[\"dataset_id\"]:\n        kws[\"dataset_id\"] = did\n    if sid := attrs[\"dataset_sample_id\"]:\n        kws[\"dataset_sample_id\"] = sid\n\n    resp = await retry()(self._evaluate)(log_id=log_id, **kws)\n    return self._translate_response(resp)\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.get_current_log_id","title":"get_current_log_id","text":"<pre><code>get_current_log_id(bound_arguments: dict[str, Any]) -&gt; Optional[LogID]\n</code></pre> <p>Return log_id for given arguments in current context. Returns None if there is no context - most likely SDK is not initialized.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def get_current_log_id(bound_arguments: dict[str, Any]) -&gt; Optional[LogID]:\n    \"\"\"\n    Return log_id for given arguments in current context.\n    Returns None if there is no context - most likely SDK is not initialized.\n    \"\"\"\n    eval_group = _ctx_evaluation_log_group.get(None)\n    if eval_group is None:\n        return None\n    log_id = eval_group.find_log(bound_arguments)\n    if log_id is None:\n        raise ValueError(\"Log not found for provided arguments\")\n    return log_id\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.bundled_eval","title":"bundled_eval","text":"<pre><code>bundled_eval(span_name: str = 'Evaluation bundle')\n</code></pre> <p>Start a span that would automatically bundle evaluations.</p> <p>Evaluations are passed by arguments passed to the evaluators called inside the context manager.</p> <p>The following example would create two bundles:</p> <ul> <li>fist with arguments <code>x=10, y=20</code></li> <li>second with arguments <code>spam=\"abc123\"</code></li> </ul> <pre><code>with bundled_eval():\n    foo_evaluator(x=10, y=20)\n    bar_evaluator(x=10, y=20)\n    tar_evaluator(spam=\"abc123\")\n</code></pre> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>@contextlib.contextmanager\ndef bundled_eval(span_name: str = \"Evaluation bundle\"):\n    \"\"\"\n    Start a span that would automatically bundle evaluations.\n\n    Evaluations are passed by arguments passed to the evaluators called inside the context manager.\n\n    The following example would create two bundles:\n\n    - fist with arguments `x=10, y=20`\n    - second with arguments `spam=\"abc123\"`\n\n    ```python\n    with bundled_eval():\n        foo_evaluator(x=10, y=20)\n        bar_evaluator(x=10, y=20)\n        tar_evaluator(spam=\"abc123\")\n    ```\n\n    \"\"\"\n    tracer = context.get_tracer_or_none()\n    if tracer is None:\n        yield\n        return\n\n    with tracer.start_as_current_span(span_name):\n        with _start_evaluation_log_group():\n            yield\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.evaluator","title":"evaluator","text":"<pre><code>evaluator(*, evaluator_id: Union[str, Callable[[], str], None] = None, criteria: Union[str, Callable[[], str], None] = None, metric_name: Optional[str] = None, metric_description: Optional[str] = None, is_method: bool = False, **kwargs)\n</code></pre> <p>Mark function as an evaluator.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def evaluator(\n    *,\n    # Name for the evaluator. Defaults to function name (or class name in case of class based evaluators).\n    evaluator_id: Union[str, typing.Callable[[], str], None] = None,\n    # Name of the criteria used by the evaluator.\n    # The use of the criteria is only recommended in more complex evaluator setups\n    # where evaluation algorithm changes depending on a criteria (think strategy pattern).\n    criteria: Union[str, typing.Callable[[], str], None] = None,\n    # Name for the evaluation metric. Defaults to evaluator_id value.\n    metric_name: Optional[str] = None,\n    # The description of the metric used for evaluation.\n    # If not provided then the docstring of the wrapped function is used for this value.\n    metric_description: Optional[str] = None,\n    # Whether the wrapped function is a method.\n    # This value is used to determine whether to remove \"self\" argument from the log.\n    # It also allows for dynamic evaluator_id and criteria discovery\n    # based on `get_evaluator_id()` and `get_criteria_id()` methods.\n    # User-code usually shouldn't use it as long as user defined class-based evaluators inherit from\n    # the library provided Evaluator base classes.\n    is_method: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Mark function as an evaluator.\n    \"\"\"\n\n    def decorator(fn):\n        fn_sign = inspect.signature(fn)\n\n        def _get_eval_id():\n            return (callable(evaluator_id) and evaluator_id()) or evaluator_id or fn.__qualname__\n\n        def _get_criteria():\n            return (callable(criteria) and criteria()) or criteria or None\n\n        def _prep(*fn_args, **fn_kwargs):\n            bound_args = fn_sign.bind(*fn_args, **fn_kwargs)\n            arguments_to_log = _as_applied_argument(fn_sign, bound_args)\n            bound_args.apply_defaults()\n            self_key_name = None\n            instance = None\n            if is_method:\n                self_key_name = next(iter(fn_sign.parameters.keys()))\n                instance = bound_args.arguments[self_key_name]\n\n            eval_id = None\n            eval_criteria = None\n            if isinstance(instance, Evaluator):\n                eval_id = instance.get_evaluator_id()\n                eval_criteria = instance.get_criteria()\n\n            if eval_id is None:\n                eval_id = _get_eval_id()\n            if eval_criteria is None:\n                eval_criteria = _get_criteria()\n\n            met_name = metric_name or eval_id\n            met_description = metric_description or inspect.getdoc(fn) or None\n\n            disable_export = isinstance(instance, RemoteEvaluatorMixin) and instance._disable_export\n\n            return PrepEval(\n                evaluator_id=eval_id,\n                criteria=eval_criteria,\n                metric_name=met_name,\n                metric_description=met_description,\n                self_key_name=self_key_name,\n                arguments=arguments_to_log,\n                disable_export=disable_export,\n            )\n\n        @functools.wraps(fn)\n        async def wrapper_async(*fn_args, **fn_kwargs):\n            ctx = context.get_current_context_or_none()\n            if ctx is None:\n                return await fn(*fn_args, **fn_kwargs)\n\n            prep = _prep(*fn_args, **fn_kwargs)\n\n            start = time.perf_counter()\n            try:\n                with start_span(prep.display_name(), attributes={GenAIAttributes.operation_name: OperationNames.eval}):\n                    with _get_or_start_evaluation_log_group() as log_group:\n                        log_id = log_group.log(\n                            logger=ctx.pat_logger,\n                            is_method=is_method,\n                            self_key_name=prep.self_key_name,\n                            bound_arguments=prep.arguments,\n                        )\n                        ret = await fn(*fn_args, **fn_kwargs)\n            except Exception as e:\n                ctx.logger.exception(f\"Evaluator raised an exception: {e}\")\n                raise e\n            if prep.disable_export:\n                return ret\n            elapsed = time.perf_counter() - start\n            handle_eval_output(\n                ctx=ctx,\n                log_id=log_id,\n                evaluator_id=prep.evaluator_id,\n                criteria=prep.criteria,\n                metric_name=prep.metric_name,\n                metric_description=prep.metric_description,\n                ret_value=ret,\n                duration=datetime.timedelta(seconds=elapsed),\n                qualname=fn.__qualname__,\n            )\n            return ret\n\n        @functools.wraps(fn)\n        def wrapper_sync(*fn_args, **fn_kwargs):\n            ctx = context.get_current_context_or_none()\n            if ctx is None:\n                return fn(*fn_args, **fn_kwargs)\n\n            prep = _prep(*fn_args, **fn_kwargs)\n\n            start = time.perf_counter()\n            try:\n                with start_span(prep.display_name(), attributes={GenAIAttributes.operation_name: OperationNames.eval}):\n                    with _get_or_start_evaluation_log_group() as log_group:\n                        log_id = log_group.log(\n                            logger=ctx.pat_logger,\n                            is_method=is_method,\n                            self_key_name=prep.self_key_name,\n                            bound_arguments=prep.arguments,\n                        )\n                        ret = fn(*fn_args, **fn_kwargs)\n            except Exception as e:\n                ctx.logger.exception(\"Evaluation failed\")\n                raise e\n            if prep.disable_export:\n                return ret\n            elapsed = time.perf_counter() - start\n            handle_eval_output(\n                ctx=ctx,\n                log_id=log_id,\n                evaluator_id=prep.evaluator_id,\n                criteria=prep.criteria,\n                metric_name=prep.metric_name,\n                metric_description=prep.metric_description,\n                ret_value=ret,\n                duration=datetime.timedelta(seconds=elapsed),\n                qualname=fn.__qualname__,\n            )\n            return ret\n\n        def _set_attrs(wrapper: Any):\n            wrapper._pat_evaluator = True\n\n            # _pat_evaluator_id and _pat_criteria_id may be a bit misleading since\n            # may not be correct since actually values for evaluator_id and criteria\n            # are dynamically dispatched for class-based evaluators.\n            # These values will be correct for function evaluators though.\n            wrapper._pat_evaluator_id = _get_eval_id()\n            wrapper._pat_criteria = _get_criteria()\n\n        if inspect.iscoroutinefunction(fn):\n            _set_attrs(wrapper_async)\n            return wrapper_async\n        else:\n            _set_attrs(wrapper_sync)\n            return wrapper_sync\n\n    return decorator\n</code></pre>"},{"location":"api_ref/experiments/","title":"experiments","text":""},{"location":"api_ref/experiments/#patronus.experiments","title":"patronus.experiments","text":""},{"location":"api_ref/experiments/#patronus.experiments.adapters","title":"adapters","text":""},{"location":"api_ref/experiments/#patronus.experiments.adapters.BaseEvaluatorAdapter","title":"BaseEvaluatorAdapter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all evaluator adapters.</p> <p>Evaluator adapters provide a standardized interface between the experiment framework and various types of evaluators (function-based, class-based, etc.).</p> <p>All concrete adapter implementations must inherit from this class and implement the required abstract methods.</p>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.EvaluatorAdapter","title":"EvaluatorAdapter","text":"<pre><code>EvaluatorAdapter(evaluator: Evaluator)\n</code></pre> <p>               Bases: <code>BaseEvaluatorAdapter</code></p> <p>Adapter for class-based evaluators conforming to the Evaluator or AsyncEvaluator protocol.</p> <p>This adapter enables the use of evaluator classes that implement either the Evaluator or AsyncEvaluator interface within the experiment framework.</p> <p>Attributes:</p> Name Type Description <code>evaluator</code> <code>Union[Evaluator, AsyncEvaluator]</code> <p>The evaluator instance to adapt.</p> <p>Examples:</p> <pre><code>```python\nimport typing\nfrom typing import Optional\n\nfrom patronus import datasets\nfrom patronus.evals import Evaluator, EvaluationResult\nfrom patronus.experiments import run_experiment\nfrom patronus.experiments.adapters import EvaluatorAdapter\nfrom patronus.experiments.types import TaskResult, EvalParent\n\n\nclass MatchEvaluator(Evaluator):\n    def __init__(self, sanitizer=None):\n        if sanitizer is None:\n            sanitizer = lambda x: x\n        self.sanitizer = sanitizer\n\n    def evaluate(self, actual: str, expected: str) -&gt; EvaluationResult:\n        matched = self.sanitizer(actual) == self.sanitizer(expected)\n        return EvaluationResult(pass_=matched, score=int(matched))\n\n\nexact_match = MatchEvaluator()\nfuzzy_match = MatchEvaluator(lambda x: x.strip().lower())\n\n\nclass MatchAdapter(EvaluatorAdapter):\n    def __init__(self, evaluator: MatchEvaluator):\n        super().__init__(evaluator)\n\n    def transform(\n        self,\n        row: datasets.Row,\n        task_result: Optional[TaskResult],\n        parent: EvalParent,\n        **kwargs\n    ) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n        args = [row.task_output, row.gold_answer]\n        kwargs = {}\n        # Passing arguments via kwargs would also work in this case.\n        # kwargs = {\"actual\": row.task_output, \"expected\": row.gold_answer}\n        return args, kwargs\n\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string    \", \"gold_answer\": \"string\"}],\n    evaluators=[MatchAdapter(exact_match), MatchAdapter(fuzzy_match)],\n)\n```\n</code></pre> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(self, evaluator: evals.Evaluator):\n    if not isinstance(evaluator, evals.Evaluator):\n        raise TypeError(f\"{evaluator} is not {evals.Evaluator.__name__}.\")\n    self.evaluator = evaluator\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.EvaluatorAdapter.transform","title":"transform","text":"<pre><code>transform(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs) -&gt; tuple[list[Any], dict[str, Any]]\n</code></pre> <p>Transform experiment framework arguments to evaluation method arguments.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[Any], dict[str, Any]]</code> <p>A tuple containing: - A list of positional arguments to pass to the evaluator function. - A dictionary of keyword arguments to pass to the evaluator function.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def transform(\n    self, row: datasets.Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs\n) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n    \"\"\"\n    Transform experiment framework arguments to evaluation method arguments.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        A tuple containing:\n          - A list of positional arguments to pass to the evaluator function.\n          - A dictionary of keyword arguments to pass to the evaluator function.\n    \"\"\"\n\n    return ([], {\"row\": row, \"task_result\": task_result, \"prent\": parent, **kwargs})\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.EvaluatorAdapter.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs) -&gt; EvaluationResult\n</code></pre> <p>Evaluate the given row and task result using the adapted evaluator function.</p> <p>This method implements the BaseEvaluatorAdapter.evaluate() protocol.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>An EvaluationResult containing the evaluation outcome.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>async def evaluate(\n    self, row: datasets.Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs\n) -&gt; EvaluationResult:\n    \"\"\"\n    Evaluate the given row and task result using the adapted evaluator function.\n\n    This method implements the BaseEvaluatorAdapter.evaluate() protocol.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        An EvaluationResult containing the evaluation outcome.\n    \"\"\"\n    ev_args, ev_kwargs = self.transform(row, task_result, parent, **kwargs)\n    return await self._evaluate(*ev_args, **ev_kwargs)\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.StructuredEvaluatorAdapter","title":"StructuredEvaluatorAdapter","text":"<pre><code>StructuredEvaluatorAdapter(evaluator: AsyncStructuredEvaluator)\n</code></pre> <p>               Bases: <code>EvaluatorAdapter</code></p> <p>Adapter for structured evaluators.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(self, evaluator: evals.AsyncStructuredEvaluator):\n    if not isinstance(evaluator, (evals.StructuredEvaluator, evals.AsyncStructuredEvaluator)):\n        raise TypeError(\n            f\"{type(evaluator)} is not \"\n            f\"{evals.AsyncStructuredEvaluator.__name__} nor {evals.StructuredEvaluator.__name__}.\"\n        )\n    super().__init__(evaluator)\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.FuncEvaluatorAdapter","title":"FuncEvaluatorAdapter","text":"<pre><code>FuncEvaluatorAdapter(fn: Callable[..., Any])\n</code></pre> <p>               Bases: <code>BaseEvaluatorAdapter</code></p> <p>Adapter class that allows using function-based evaluators with the experiment framework.</p> <p>This adapter serves as a bridge between function-based evaluators decorated with <code>@evaluator()</code> and the experiment framework's evaluation system. It handles both synchronous and asynchronous evaluator functions.</p> <p>Attributes:</p> Name Type Description <code>fn</code> <code>Callable</code> <p>The evaluator function to be adapted.</p> Notes <ul> <li>The function passed to this adapter must be decorated with <code>@evaluator()</code>.</li> <li>The adapter automatically handles the conversion between function results and proper   evaluation result objects.</li> </ul> <p>Examples:</p> <pre><code>Direct usage with a compatible evaluator function:\n\n```python\nfrom patronus import evaluator\nfrom patronus.experiments import FuncEvaluatorAdapter, run_experiment\nfrom patronus.datasets import Row\n\n\n@evaluator()\ndef exact_match(row: Row, **kwargs):\n    return row.task_output == row.gold_answer\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string\", \"gold_answer\": \"string\"}],\n    evaluators=[FuncEvaluatorAdapter(exact_match)]\n)\n```\n\nCustomized usage by overriding the `transform()` method:\n\n```python\nfrom typing import Optional\nimport typing\n\nfrom patronus import evaluator, datasets\nfrom patronus.experiments import FuncEvaluatorAdapter, run_experiment\nfrom patronus.experiments.types import TaskResult, EvalParent\n\n\n@evaluator()\ndef exact_match(actual, expected):\n    return actual == expected\n\n\nclass AdaptedExactMatch(FuncEvaluatorAdapter):\n    def __init__(self):\n        super().__init__(exact_match)\n\n    def transform(\n        self,\n        row: datasets.Row,\n        task_result: Optional[TaskResult],\n        parent: EvalParent,\n        **kwargs\n    ) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n        args = [row.task_output, row.gold_answer]\n        kwargs = {}\n\n        # Alternative: passing arguments via kwargs instead of args\n        # args = []\n        # kwargs = {\"actual\": row.task_output, \"expected\": row.gold_answer}\n\n        return args, kwargs\n\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string\", \"gold_answer\": \"string\"}],\n    evaluators=[AdaptedExactMatch()],\n)\n```\n</code></pre> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(self, fn: typing.Callable[..., typing.Any]):\n    if not hasattr(fn, \"_pat_evaluator\"):\n        raise ValueError(\n            f\"Passed function {fn.__qualname__} is not an evaluator. \"\n            \"Hint: add @evaluator decorator to the function.\"\n        )\n    self.fn = fn\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.FuncEvaluatorAdapter.transform","title":"transform","text":"<pre><code>transform(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs) -&gt; tuple[list[Any], dict[str, Any]]\n</code></pre> <p>Transform experiment framework parameters to evaluator function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[Any], dict[str, Any]]</code> <p>A tuple containing: - A list of positional arguments to pass to the evaluator function. - A dictionary of keyword arguments to pass to the evaluator function.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def transform(\n    self, row: datasets.Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs\n) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n    \"\"\"\n    Transform experiment framework parameters to evaluator function parameters.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        A tuple containing:\n          - A list of positional arguments to pass to the evaluator function.\n          - A dictionary of keyword arguments to pass to the evaluator function.\n    \"\"\"\n\n    return ([], {\"row\": row, \"task_result\": task_result, \"prent\": parent, **kwargs})\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.FuncEvaluatorAdapter.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs) -&gt; EvaluationResult\n</code></pre> <p>Evaluate the given row and task result using the adapted evaluator function.</p> <p>This method implements the BaseEvaluatorAdapter.evaluate() protocol.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>An EvaluationResult containing the evaluation outcome.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>async def evaluate(\n    self, row: datasets.Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs\n) -&gt; EvaluationResult:\n    \"\"\"\n    Evaluate the given row and task result using the adapted evaluator function.\n\n    This method implements the BaseEvaluatorAdapter.evaluate() protocol.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        An EvaluationResult containing the evaluation outcome.\n    \"\"\"\n    ev_args, ev_kwargs = self.transform(row, task_result, parent, **kwargs)\n    return await self._evaluate(*ev_args, **ev_kwargs)\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment","title":"experiment","text":""},{"location":"api_ref/experiments/#patronus.experiments.experiment.run_experiment","title":"run_experiment","text":"<pre><code>run_experiment(dataset: ExperimentDataset, task: Optional[Task] = None, evaluators: Optional[list[AdaptableEvaluators]] = None, chain: Optional[list[ChainLink]] = None, tags: Optional[Tags] = None, max_concurrency: int = 10, project_name: Optional[str] = None, experiment_name: Optional[str] = None, api_key: Optional[str] = None, **kwargs) -&gt; Experiment\n</code></pre> <p>Create and run an experiment.</p> <p>This function creates an experiment with the specified configuration and runs it to completion. The execution handling is context-aware:</p> <ul> <li>When called from an asynchronous context (with a running event loop), it returns an   awaitable that must be awaited.</li> <li>When called from a synchronous context (no running event loop), it blocks until the   experiment completes and returns the Experiment object.</li> </ul>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.run_experiment--returns","title":"Returns","text":"<p><code>Union[Experiment, Awaitable[Experiment]]</code></p> <ul> <li>In a synchronous context: the completed Experiment object.</li> <li>In an asynchronous context: an awaitable that resolves to the Experiment object.</li> </ul>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.run_experiment--examples","title":"Examples","text":"<p>Synchronous execution:</p> <pre><code>experiment = run_experiment(dataset, task=some_task)\n# Blocks until the experiment finishes.\n</code></pre> <p>Asynchronous execution (e.g., in a Jupyter Notebook):</p> <pre><code>experiment = await run_experiment(dataset, task=some_task)\n# Must be awaited within an async function or event loop.\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.run_experiment--notes","title":"Notes","text":"<p>For manual control of the event loop, you can create and run the experiment as follows:</p> <pre><code>experiment = await Experiment.create(...)\nawait experiment.run()\n</code></pre> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>def run_experiment(\n    dataset: ExperimentDataset,\n    task: Optional[Task] = None,\n    evaluators: Optional[list[AdaptableEvaluators]] = None,\n    chain: Optional[list[ChainLink]] = None,\n    tags: Optional[Tags] = None,\n    max_concurrency: int = 10,\n    project_name: Optional[str] = None,\n    experiment_name: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"Experiment\":\n    \"\"\"\n    Create and run an experiment.\n\n    This function creates an experiment with the specified configuration and runs it to completion.\n    The execution handling is context-aware:\n\n    - When called from an asynchronous context (with a running event loop), it returns an\n      awaitable that must be awaited.\n    - When called from a synchronous context (no running event loop), it blocks until the\n      experiment completes and returns the Experiment object.\n\n\n    Returns\n    -------\n    `Union[Experiment, Awaitable[Experiment]]`\n\n    - In a synchronous context: the completed Experiment object.\n    - In an asynchronous context: an awaitable that resolves to the Experiment object.\n\n    Examples\n    ---------\n\n    Synchronous execution:\n\n    ```python\n    experiment = run_experiment(dataset, task=some_task)\n    # Blocks until the experiment finishes.\n    ```\n\n    Asynchronous execution (e.g., in a Jupyter Notebook):\n\n    ```python\n    experiment = await run_experiment(dataset, task=some_task)\n    # Must be awaited within an async function or event loop.\n    ```\n\n    Notes\n    ------\n\n    For manual control of the event loop, you can create and run the experiment as follows:\n\n    ```python\n    experiment = await Experiment.create(...)\n    await experiment.run()\n    ```\n    \"\"\"\n\n    async def _run_experiment() -&gt; Union[Experiment, typing.Awaitable[Experiment]]:\n        ex = await Experiment.create(\n            dataset=dataset,\n            task=task,\n            evaluators=evaluators,\n            chain=chain,\n            tags=tags,\n            max_concurrency=max_concurrency,\n            project_name=project_name,\n            experiment_name=experiment_name,\n            api_key=api_key,\n            **kwargs,\n        )\n        return await ex.run()\n\n    return run_until_complete(_run_experiment())\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.tqdm","title":"tqdm","text":""},{"location":"api_ref/experiments/#patronus.experiments.tqdm.AsyncTQDMWithHandle","title":"AsyncTQDMWithHandle","text":"<p>               Bases: <code>tqdm</code></p> <p>Workaround for accessing tqdm instance with async tasks. Instead of calling gather which don't provide access to tqdm instance: <pre><code>tqdm_async.gather(features)\n</code></pre></p> <p>Call prep_gather() follow by gather() <pre><code>tqdm_instance = AsyncTQDMWithHandle.pre_gather(features)\n...\ntqdm_instance.gather()\n</code></pre></p> <p>tqdm_instance can be used to clear and display progress bar using tqdm_instance.clear() and tqdm_instance.display() methods.</p>"},{"location":"api_ref/pat_client/","title":"Patronus Objects","text":""},{"location":"api_ref/pat_client/#patronus.pat_client.client_async","title":"client_async","text":""},{"location":"api_ref/pat_client/#patronus.pat_client.client_async.AsyncPatronus","title":"AsyncPatronus","text":"<pre><code>AsyncPatronus(max_workers: int = 10)\n</code></pre> Source code in <code>src/patronus/pat_client/client_async.py</code> <pre><code>def __init__(self, max_workers: int = 10):\n    self._pending_tasks = collections.deque()\n    self._executor = ThreadPoolExecutor(max_workers=max_workers)\n    self._semaphore = asyncio.Semaphore(max_workers)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_async.AsyncPatronus.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(evaluators: Union[List[Evaluator], Evaluator], *, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[dict] = None, return_exceptions: bool = False) -&gt; EvaluationContainer\n</code></pre> <p>Run multiple evaluators in parallel.</p> Source code in <code>src/patronus/pat_client/client_async.py</code> <pre><code>async def evaluate(\n    self,\n    evaluators: Union[List[Evaluator], Evaluator],\n    *,\n    system_prompt: Optional[str] = None,\n    task_context: Union[list[str], str, None] = None,\n    task_input: Optional[str] = None,\n    task_output: Optional[str] = None,\n    gold_answer: Optional[str] = None,\n    task_metadata: Optional[dict] = None,\n    return_exceptions: bool = False,\n) -&gt; EvaluationContainer:\n    \"\"\"\n    Run multiple evaluators in parallel.\n    \"\"\"\n    singular_eval = not isinstance(evaluators, list)\n    if singular_eval:\n        evaluators = [evaluators]\n    evaluators = self._map_evaluators(evaluators)\n\n    def into_coro(fn, **kwargs):\n        if inspect.iscoroutinefunction(fn):\n            coro = fn(**kwargs)\n        else:\n            coro = asyncio.to_thread(fn, **kwargs)\n        return with_semaphore(self._semaphore, coro)\n\n    with bundled_eval():\n        results = await asyncio.gather(\n            *(\n                into_coro(\n                    ev.evaluate,\n                    system_prompt=system_prompt,\n                    task_context=task_context,\n                    task_input=task_input,\n                    task_output=task_output,\n                    gold_answer=gold_answer,\n                    task_metadata=task_metadata,\n                )\n                for ev in evaluators\n            ),\n            return_exceptions=return_exceptions,\n        )\n    return EvaluationContainer(results)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_async.AsyncPatronus.evaluate_bg","title":"evaluate_bg","text":"<pre><code>evaluate_bg(evaluators: Union[List[Evaluator], Evaluator], *, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[dict] = None) -&gt; Task[EvaluationContainer]\n</code></pre> <p>Run multiple evaluators in parallel. The returned task will be a background task.</p> Source code in <code>src/patronus/pat_client/client_async.py</code> <pre><code>def evaluate_bg(\n    self,\n    evaluators: Union[List[Evaluator], Evaluator],\n    *,\n    system_prompt: Optional[str] = None,\n    task_context: Union[list[str], str, None] = None,\n    task_input: Optional[str] = None,\n    task_output: Optional[str] = None,\n    gold_answer: Optional[str] = None,\n    task_metadata: Optional[dict] = None,\n) -&gt; Task[EvaluationContainer]:\n    \"\"\"\n    Run multiple evaluators in parallel. The returned task will be a background task.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    task = loop.create_task(\n        self.evaluate(\n            evaluators=evaluators,\n            system_prompt=system_prompt,\n            task_context=task_context,\n            task_input=task_input,\n            task_output=task_output,\n            gold_answer=gold_answer,\n            task_metadata=task_metadata,\n            return_exceptions=True,\n        ),\n        name=\"evaluate_bg\",\n    )\n    self._pending_tasks.append(task)\n    task.add_done_callback(self._consume_tasks)\n    return task\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_async.AsyncPatronus.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> <p>Gracefully close the client. This will wait for all background tasks to finish.</p> Source code in <code>src/patronus/pat_client/client_async.py</code> <pre><code>async def close(self):\n    \"\"\"\n    Gracefully close the client. This will wait for all background tasks to finish.\n    \"\"\"\n    while len(self._pending_tasks) != 0:\n        await self._pending_tasks.popleft()\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync","title":"client_sync","text":""},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync.Patronus","title":"Patronus","text":"<pre><code>Patronus(workers: int = 10, shutdown_on_exit: bool = True)\n</code></pre> Source code in <code>src/patronus/pat_client/client_sync.py</code> <pre><code>def __init__(self, workers: int = 10, shutdown_on_exit: bool = True):\n    self._worker_pool = ThreadPool(workers)\n    self._supervisor_pool = ThreadPool(workers)\n\n    self._at_exit_handler = None\n    if shutdown_on_exit:\n        self._at_exit_handler = atexit.register(self.close)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync.Patronus.evaluate","title":"evaluate","text":"<pre><code>evaluate(evaluators: Union[list[Evaluator], Evaluator], *, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[dict[str, Any]] = None, return_exceptions: bool = False) -&gt; EvaluationContainer\n</code></pre> <p>Run multiple evaluators in parallel.</p> Source code in <code>src/patronus/pat_client/client_sync.py</code> <pre><code>def evaluate(\n    self,\n    evaluators: typing.Union[list[Evaluator], Evaluator],\n    *,\n    system_prompt: typing.Optional[str] = None,\n    task_context: typing.Union[list[str], str, None] = None,\n    task_input: typing.Optional[str] = None,\n    task_output: typing.Optional[str] = None,\n    gold_answer: typing.Optional[str] = None,\n    task_metadata: typing.Optional[dict[str, typing.Any]] = None,\n    return_exceptions: bool = False,\n) -&gt; EvaluationContainer:\n    \"\"\"\n    Run multiple evaluators in parallel.\n    \"\"\"\n    if not isinstance(evaluators, list):\n        evaluators = [evaluators]\n    evaluators = self._map_evaluators(evaluators)\n\n    with bundled_eval():\n        callables = [\n            _into_thread_run_fn(\n                ev.evaluate,\n                system_prompt=system_prompt,\n                task_context=task_context,\n                task_input=task_input,\n                task_output=task_output,\n                gold_answer=gold_answer,\n                task_metadata=task_metadata,\n            )\n            for ev in evaluators\n        ]\n        results = self._process_batch(callables, return_exceptions=return_exceptions)\n        return EvaluationContainer(results)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync.Patronus.evaluate_bg","title":"evaluate_bg","text":"<pre><code>evaluate_bg(evaluators: list[StructuredEvaluator], *, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[dict[str, Any]] = None) -&gt; TypedAsyncResult[EvaluationContainer]\n</code></pre> <p>Run multiple evaluators in parallel. The returned task will be a background task.</p> Source code in <code>src/patronus/pat_client/client_sync.py</code> <pre><code>def evaluate_bg(\n    self,\n    evaluators: list[StructuredEvaluator],\n    *,\n    system_prompt: typing.Optional[str] = None,\n    task_context: typing.Union[list[str], str, None] = None,\n    task_input: typing.Optional[str] = None,\n    task_output: typing.Optional[str] = None,\n    gold_answer: typing.Optional[str] = None,\n    task_metadata: typing.Optional[dict[str, typing.Any]] = None,\n) -&gt; TypedAsyncResult[EvaluationContainer]:\n    \"\"\"\n    Run multiple evaluators in parallel. The returned task will be a background task.\n    \"\"\"\n\n    def _run():\n        with bundled_eval():\n            callables = [\n                _into_thread_run_fn(\n                    ev.evaluate,\n                    system_prompt=system_prompt,\n                    task_context=task_context,\n                    task_input=task_input,\n                    task_output=task_output,\n                    gold_answer=gold_answer,\n                    task_metadata=task_metadata,\n                )\n                for ev in evaluators\n            ]\n            results = self._process_batch(callables, return_exceptions=True)\n            return EvaluationContainer(results)\n\n    return typing.cast(\n        TypedAsyncResult[EvaluationContainer], self._supervisor_pool.apply_async(_into_thread_run_fn(_run))\n    )\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync.Patronus.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Gracefully close the client. This will wait for all background tasks to finish.</p> Source code in <code>src/patronus/pat_client/client_sync.py</code> <pre><code>def close(self):\n    \"\"\"\n    Gracefully close the client. This will wait for all background tasks to finish.\n    \"\"\"\n    self._close()\n    if self._at_exit_handler:\n        atexit.unregister(self._at_exit_handler)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container","title":"container","text":""},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer","title":"EvaluationContainer  <code>dataclass</code>","text":"<pre><code>EvaluationContainer(results: list[Union[EvaluationResult, None, Exception]])\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.has_exception","title":"has_exception","text":"<pre><code>has_exception() -&gt; bool\n</code></pre> <p>Checks if the results contain any exception.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def has_exception(self) -&gt; bool:\n    \"\"\"\n    Checks if the results contain any exception.\n    \"\"\"\n    return any(isinstance(r, Exception) for r in self.results)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.raise_on_exception","title":"raise_on_exception","text":"<pre><code>raise_on_exception() -&gt; None\n</code></pre> <p>Checks the results for any exceptions and raises them accordingly.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def raise_on_exception(self) -&gt; None:\n    \"\"\"\n    Checks the results for any exceptions and raises them accordingly.\n    \"\"\"\n    if not self.has_exception():\n        return None\n    exceptions = list(r for r in self.results if isinstance(r, Exception))\n    if len(exceptions) == 1:\n        raise exceptions[0]\n    raise MultiException(exceptions)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.all_succeeded","title":"all_succeeded","text":"<pre><code>all_succeeded(ignore_exceptions: bool = False) -&gt; bool\n</code></pre> <p>Check if all evaluations that were actually evaluated passed.</p> <p>Evaluations are only considered if they: - Have a non-None pass_ flag set - Are not None (skipped) - Are not exceptions (unless ignore_exceptions=True)</p> <p>Note: Returns True if no evaluations met the above criteria (empty case).</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def all_succeeded(self, ignore_exceptions: bool = False) -&gt; bool:\n    \"\"\"\n    Check if all evaluations that were actually evaluated passed.\n\n    Evaluations are only considered if they:\n    - Have a non-None pass_ flag set\n    - Are not None (skipped)\n    - Are not exceptions (unless ignore_exceptions=True)\n\n    Note: Returns True if no evaluations met the above criteria (empty case).\n    \"\"\"\n    for r in self.results:\n        if isinstance(r, Exception) and not ignore_exceptions:\n            self.raise_on_exception()\n        if r is not None and r.pass_ is False:\n            return False\n    return True\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.any_failed","title":"any_failed","text":"<pre><code>any_failed(ignore_exceptions: bool = False) -&gt; bool\n</code></pre> <p>Check if any evaluation that was actually evaluated failed.</p> <p>Evaluations are only considered if they: - Have a non-None pass_ flag set - Are not None (skipped) - Are not exceptions (unless ignore_exceptions=True)</p> <p>Note: Returns False if no evaluations met the above criteria (empty case).</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def any_failed(self, ignore_exceptions: bool = False) -&gt; bool:\n    \"\"\"\n    Check if any evaluation that was actually evaluated failed.\n\n    Evaluations are only considered if they:\n    - Have a non-None pass_ flag set\n    - Are not None (skipped)\n    - Are not exceptions (unless ignore_exceptions=True)\n\n    Note: Returns False if no evaluations met the above criteria (empty case).\n    \"\"\"\n    for r in self.results:\n        if isinstance(r, Exception) and not ignore_exceptions:\n            self.raise_on_exception()\n        if r is not None and r.pass_ is False:\n            return True\n    return False\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.failed_evaluations","title":"failed_evaluations","text":"<pre><code>failed_evaluations() -&gt; Generator[EvaluationResult, None, None]\n</code></pre> <p>Generates all failed evaluations from the results.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def failed_evaluations(self) -&gt; Generator[EvaluationResult, None, None]:\n    \"\"\"\n    Generates all failed evaluations from the results.\n    \"\"\"\n    return (r for r in self.results if not isinstance(r, (Exception, type(None))) and r.pass_ is False)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.succeeded_evaluations","title":"succeeded_evaluations","text":"<pre><code>succeeded_evaluations() -&gt; Generator[EvaluationResult, None, None]\n</code></pre> <p>Generates all successfully passed evaluations from the <code>results</code> attribute.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def succeeded_evaluations(self) -&gt; Generator[EvaluationResult, None, None]:\n    \"\"\"\n    Generates all successfully passed evaluations from the `results` attribute.\n    \"\"\"\n    return (r for r in self.results if not isinstance(r, (Exception, type(None))) and r.pass_ is True)\n</code></pre>"},{"location":"api_ref/tracing/","title":"Tracing","text":""},{"location":"api_ref/tracing/#patronus.tracing","title":"patronus.tracing","text":""},{"location":"api_ref/tracing/#patronus.tracing.decorators","title":"decorators","text":""},{"location":"api_ref/tracing/#patronus.tracing.decorators.start_span","title":"start_span","text":"<pre><code>start_span(name: str, *, record_exception: bool = True, attributes: Optional[Attributes] = None) -&gt; Iterator[Optional[Any]]\n</code></pre> <p>Context manager for creating and managing a trace span.</p> <p>This function is used to create a span within the current context using the tracer, allowing you to track execution timing or events within a specific block of code. The context is set by <code>patronus.init()</code> function. If SDK was not initialized, yielded value will be None.</p> <p>Example:</p> <pre><code>import patronus\n\npatronus.init()\n\n# Use context manager for finer-grained tracing\ndef complex_operation():\n    with patronus.start_span(\"Data preparation\"):\n        # Prepare data\n        pass\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the span.</p> required <code>record_exception</code> <code>bool</code> <p>Whether to record exceptions that occur within the span. Default is True.</p> <code>True</code> <code>attributes</code> <code>Optional[Attributes]</code> <p>Attributes to associate with the span, providing additional metadata.</p> <code>None</code> Source code in <code>src/patronus/tracing/decorators.py</code> <pre><code>@contextlib.contextmanager\ndef start_span(\n    name: str, *, record_exception: bool = True, attributes: Optional[Attributes] = None\n) -&gt; Iterator[Optional[typing.Any]]:\n    \"\"\"\n    Context manager for creating and managing a trace span.\n\n    This function is used to create a span within the current context using the tracer,\n    allowing you to track execution timing or events within a specific block of code.\n    The context is set by `patronus.init()` function. If SDK was not initialized, yielded value will be None.\n\n    Example:\n\n    ```python\n    import patronus\n\n    patronus.init()\n\n    # Use context manager for finer-grained tracing\n    def complex_operation():\n        with patronus.start_span(\"Data preparation\"):\n            # Prepare data\n            pass\n    ```\n\n\n    Args:\n        name (str): The name of the span.\n        record_exception (bool): Whether to record exceptions that occur within the span. Default is True.\n        attributes (Optional[Attributes]): Attributes to associate with the span, providing additional metadata.\n    \"\"\"\n    ctx = context.get_current_context_or_none()\n    if ctx is None:\n        yield\n        return\n    with ctx.tracer.start_as_current_span(\n        name,\n        record_exception=record_exception,\n        attributes=attributes,\n    ) as span:\n        yield span\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.decorators.traced","title":"traced","text":"<pre><code>traced(span_name: Optional[str] = None, *, log_args: bool = True, log_results: bool = True, log_exceptions: bool = True, disable_log: bool = False, attributes: Attributes = None, **kwargs)\n</code></pre> <p>A decorator to trace function execution by recording a span for the traced function.</p> <p>Example:</p> <pre><code>import patronus\n\npatronus.init()\n\n# Trace a function with the @traced decorator\n@patronus.traced()\ndef process_input(user_query):\n    # Process the input\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>span_name</code> <code>Optional[str]</code> <p>The name of the traced span. Defaults to the function name if not provided.</p> <code>None</code> <code>log_args</code> <code>bool</code> <p>Whether to log the arguments passed to the function. Default is True.</p> <code>True</code> <code>log_results</code> <code>bool</code> <p>Whether to log the function's return value. Default is True.</p> <code>True</code> <code>log_exceptions</code> <code>bool</code> <p>Whether to log any exceptions raised while executing the function. Default is True.</p> <code>True</code> <code>disable_log</code> <code>bool</code> <p>Whether to disable logging the trace information. Default is False.</p> <code>False</code> <code>attributes</code> <code>Attributes</code> <p>Attributes to attach to the traced span. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for the decorator.</p> <code>{}</code> Source code in <code>src/patronus/tracing/decorators.py</code> <pre><code>def traced(\n    # Give name for the traced span. Defaults to a function name if not provided.\n    span_name: Optional[str] = None,\n    *,\n    # Whether to log function arguments.\n    log_args: bool = True,\n    # Whether to log function output.\n    log_results: bool = True,\n    # Whether to log an exception if one was raised.\n    log_exceptions: bool = True,\n    # Whether to prevent a log message to be created.\n    disable_log: bool = False,\n    attributes: Attributes = None,\n    **kwargs,\n):\n    \"\"\"\n    A decorator to trace function execution by recording a span for the traced function.\n\n    Example:\n\n    ```python\n    import patronus\n\n    patronus.init()\n\n    # Trace a function with the @traced decorator\n    @patronus.traced()\n    def process_input(user_query):\n        # Process the input\n    ```\n\n    Args:\n        span_name (Optional[str]): The name of the traced span. Defaults to the function name if not provided.\n        log_args (bool): Whether to log the arguments passed to the function. Default is True.\n        log_results (bool): Whether to log the function's return value. Default is True.\n        log_exceptions (bool): Whether to log any exceptions raised while executing the function. Default is True.\n        disable_log (bool): Whether to disable logging the trace information. Default is False.\n        attributes (Attributes): Attributes to attach to the traced span. Default is None.\n        **kwargs: Additional arguments for the decorator.\n    \"\"\"\n\n    def decorator(func):\n        name = span_name or func.__qualname__\n        sig = inspect.signature(func)\n        record_exception = not disable_log and log_exceptions\n\n        def log_call(logger: Logger, fn_args: typing.Any, fn_kwargs: typing.Any, ret: typing.Any, exc: Exception):\n            if disable_log:\n                return\n\n            severity = SeverityNumber.INFO\n            body = {\"function.name\": name}\n            if log_args:\n                bound_args = sig.bind(*fn_args, **fn_kwargs)\n                body[\"function.arguments\"] = {**bound_args.arguments, **bound_args.arguments}\n            if log_results is not None and exc is None:\n                body[\"function.output\"] = ret\n            if log_exceptions and exc is not None:\n                module = type(exc).__module__\n                qualname = type(exc).__qualname__\n                exception_type = f\"{module}.{qualname}\" if module and module != \"builtins\" else qualname\n                body[\"exception.type\"] = exception_type\n                body[\"exception.message\"] = str(exc)\n                severity = SeverityNumber.ERROR\n            logger.log(body, log_type=LogTypes.trace, severity=severity)\n\n        @functools.wraps(func)\n        def wrapper_sync(*f_args, **f_kwargs):\n            ctx = context.get_current_context_or_none()\n            if ctx is None:\n                return func(*f_args, **f_kwargs)\n\n            exc = None\n            ret = None\n            with ctx.tracer.start_as_current_span(name, record_exception=record_exception, attributes=attributes):\n                try:\n                    ret = func(*f_args, **f_kwargs)\n                except Exception as e:\n                    exc = e\n                    raise exc\n                finally:\n                    log_call(ctx.pat_logger, f_args, f_kwargs, ret, exc)\n\n                return ret\n\n        @functools.wraps(func)\n        async def wrapper_async(*f_args, **f_kwargs):\n            ctx = context.get_current_context_or_none()\n            if ctx is None:\n                return await func(*f_args, **f_kwargs)\n\n            exc = None\n            ret = None\n            with ctx.tracer.start_as_current_span(name, record_exception=record_exception, attributes=attributes):\n                try:\n                    ret = await func(*f_args, **f_kwargs)\n                except Exception as e:\n                    exc = e\n                    raise exc\n                finally:\n                    log_call(ctx.pat_logger, f_args, f_kwargs, ret, exc)\n\n                return ret\n\n        if inspect.iscoroutinefunction(func):\n            wrapper_async._pat_traced = True\n            return wrapper_async\n        else:\n            wrapper_async._pat_traced = True\n            return wrapper_sync\n\n    return decorator\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.tracer","title":"tracer","text":"<p>This module provides the implementation for tracing support using the OpenTelemetry SDK.</p>"},{"location":"api_ref/tracing/#patronus.tracing.tracer.PatronusAttributesSpanProcessor","title":"PatronusAttributesSpanProcessor","text":"<pre><code>PatronusAttributesSpanProcessor(project_name: str, app: Optional[str] = None, experiment_id: Optional[str] = None)\n</code></pre> <p>               Bases: <code>SpanProcessor</code></p> <p>Processor that adds Patronus-specific attributes to all spans.</p> <p>This processor ensures that each span includes the mandatory attributes: <code>project_name</code>, and optionally adds <code>app</code> or <code>experiment_id</code> attributes if they are provided during initialization.</p> Source code in <code>src/patronus/tracing/tracer.py</code> <pre><code>def __init__(self, project_name: str, app: Optional[str] = None, experiment_id: Optional[str] = None):\n    self.project_name = project_name\n    self.experiment_id = None\n    self.app = None\n\n    if experiment_id is not None:\n        self.experiment_id = experiment_id\n    else:\n        self.app = app\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.tracer.create_tracer_provider","title":"create_tracer_provider  <code>cached</code>","text":"<pre><code>create_tracer_provider(exporter_endpoint: str, api_key: str, scope: PatronusScope) -&gt; TracerProvider\n</code></pre> <p>Creates and returns a cached TracerProvider configured with the specified exporter.</p> <p>The function utilizes an OpenTelemetry BatchSpanProcessor and an OTLPSpanExporter to initialize the tracer. The configuration is cached for reuse.</p> Source code in <code>src/patronus/tracing/tracer.py</code> <pre><code>@functools.lru_cache()\ndef create_tracer_provider(\n    exporter_endpoint: str,\n    api_key: str,\n    scope: context.PatronusScope,\n) -&gt; TracerProvider:\n    \"\"\"\n    Creates and returns a cached TracerProvider configured with the specified exporter.\n\n    The function utilizes an OpenTelemetry BatchSpanProcessor and an\n    OTLPSpanExporter to initialize the tracer. The configuration is cached for reuse.\n    \"\"\"\n    service_name = format_service_name(scope.project_name, scope.app, scope.experiment_id)\n    resource = Resource.create({\"service.name\": service_name})\n    provider = TracerProvider(resource=resource)\n    provider.add_span_processor(\n        PatronusAttributesSpanProcessor(\n            project_name=scope.project_name,\n            app=scope.app,\n            experiment_id=scope.experiment_id,\n        )\n    )\n    provider.add_span_processor(BatchSpanProcessor(_create_exporter(endpoint=exporter_endpoint, api_key=api_key)))\n    return provider\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.tracer.create_tracer","title":"create_tracer","text":"<pre><code>create_tracer(scope: PatronusScope, exporter_endpoint: str, api_key: str) -&gt; Tracer\n</code></pre> <p>Creates an OpenTelemetry (OTeL) tracer tied to the specified scope.</p> Source code in <code>src/patronus/tracing/tracer.py</code> <pre><code>def create_tracer(\n    scope: context.PatronusScope,\n    exporter_endpoint: str,\n    api_key: str,\n) -&gt; trace.Tracer:\n    \"\"\"\n    Creates an OpenTelemetry (OTeL) tracer tied to the specified scope.\n    \"\"\"\n    provider = create_tracer_provider(\n        exporter_endpoint=exporter_endpoint,\n        api_key=api_key,\n        scope=scope,\n    )\n    return provider.get_tracer(\"patronus.sdk\")\n</code></pre>"},{"location":"evaluations/batching/","title":"Batch Evaluations","text":"<p>When evaluating multiple outputs or using multiple evaluators, Patronus provides efficient batch evaluation capabilities. This page covers how to perform batch evaluations and manage evaluation groups.</p>"},{"location":"evaluations/batching/#using-patronus-client","title":"Using Patronus Client","text":"<p>For more advanced batch evaluation needs, use the <code>Patronus</code> client:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nwith Patronus() as client:\n    # Run multiple evaluators in parallel\n    results = client.evaluate(\n        evaluators=[\n            RemoteEvaluator(\"judge\", \"patronus:is-helpful\"),\n            RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n        ],\n        task_input=\"What is quantum computing?\",\n        task_output=\"Quantum computing uses quantum bits or qubits to perform computations...\",\n        gold_answer=\"Computing that uses quantum phenomena like superposition and entanglement\"\n    )\n\n    # Check if all evaluations passed\n    if results.all_succeeded():\n        print(\"All evaluations passed!\")\n    else:\n        print(\"Some evaluations failed:\")\n        for failed in results.failed_evaluations():\n            print(f\"  - {failed.text_output}\")\n</code></pre> <p>The <code>Patronus</code> client provides:</p> <ul> <li>Parallel evaluation execution</li> <li>Connection pooling</li> <li>Error handling</li> <li>Result aggregation</li> </ul>"},{"location":"evaluations/batching/#asynchronous-evaluation","title":"Asynchronous Evaluation","text":"<p>For asynchronous workflows, use <code>AsyncPatronus</code>:</p> <pre><code>import asyncio\nfrom patronus import init\nfrom patronus.pat_client import AsyncPatronus\nfrom patronus.evals import AsyncRemoteEvaluator\n\ninit()\n\n\nasync def evaluate_responses():\n    async with AsyncPatronus() as client:\n        # Run evaluations asynchronously\n        results = await client.evaluate(\n            evaluators=[\n                AsyncRemoteEvaluator(\"judge\", \"patronus:is-helpful\"),\n                AsyncRemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n            ],\n            task_input=\"What is quantum computing?\",\n            task_output=\"Quantum computing uses quantum bits or qubits to perform computations...\",\n            gold_answer=\"Computing that uses quantum phenomena like superposition and entanglement\"\n        )\n\n        print(f\"Number of evaluations: {len(results.results)}\")\n        print(f\"All passed: {results.all_succeeded()}\")\n\n# Run the async function\nasyncio.run(evaluate_responses())\n</code></pre>"},{"location":"evaluations/batching/#background-evaluation","title":"Background Evaluation","text":"<p>For non-blocking evaluation, use the <code>evaluate_bg()</code> method:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nwith Patronus() as client:\n    # Start background evaluation\n    future = client.evaluate_bg(\n        evaluators=[\n            RemoteEvaluator(\"judge\", \"factual-accuracy\"),\n            RemoteEvaluator(\"judge\", \"patronus:helpfulness\")\n        ],\n        task_input=\"Explain how vaccines work.\",\n        task_output=\"Vaccines work by training the immune system to recognize and combat pathogens...\"\n    )\n\n    # Do other work while evaluation happens in background\n    print(\"Continuing with other tasks...\")\n\n    results = future.get()  # Blocks until complete\n\n    print(f\"Evaluation complete: {results.all_succeeded()}\")\n</code></pre> <p>The async version works similarly:</p> <pre><code>async with AsyncPatronus() as client:\n    # Start background evaluation\n    task = client.evaluate_bg(\n        evaluators=[...],\n        task_input=\"...\",\n        task_output=\"...\"\n    )\n\n    # Do other async work\n    await some_other_async_function()\n\n    # Get results when needed\n    results = await task\n</code></pre>"},{"location":"evaluations/batching/#working-with-evaluation-results","title":"Working with Evaluation Results","text":"<p>The <code>evaluate()</code> method returns an <code>EvaluationContainer</code> with several useful methods:</p> <pre><code>results = client.evaluate(evaluators=[...], task_input=\"...\", task_output=\"...\")\n\nif results.any_failed():\n    print(\"Some evaluations failed\")\n\nif results.all_succeeded():\n    print(\"All evaluations passed\")\n\nfor failed in results.failed_evaluations():\n    print(f\"Failed: {failed.text_output}\")\n\nfor success in results.succeeded_evaluations():\n    print(f\"Passed: {success.text_output}\")\n\nif results.has_exception():\n    results.raise_on_exception()  # Re-raise any exceptions that occurred\n</code></pre>"},{"location":"evaluations/batching/#example-comprehensive-quality-check","title":"Example: Comprehensive Quality Check","text":"<p>Here's a complete example of batch evaluation for content quality:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\ndef check_content_quality(question, answer):\n    with Patronus() as client:\n        results = client.evaluate(\n            evaluators=[\n                RemoteEvaluator(\"judge\", \"factual-accuracy\"),\n                RemoteEvaluator(\"judge\", \"helpfulness\"),\n                RemoteEvaluator(\"judge\", \"coherence\"),\n                RemoteEvaluator(\"judge\", \"grammar\"),\n                RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n            ],\n            task_input=question,\n            task_output=answer\n        )\n\n        if results.any_failed():\n            print(\"Content quality check failed\")\n            for failed in results.failed_evaluations():\n                print(f\"- Failed check: {failed.text_output}\")\n                print(f\"  Explanation: {failed.explanation}\")\n            return False\n\n        print(\"Content passed all quality checks\")\n        return True\n\ncheck_content_quality(\n    \"What is the capital of France?\",\n    \"The capital of France is Paris, which is located on the Seine River.\"\n)\n</code></pre>"},{"location":"evaluations/batching/#using-the-bundled_eval-context-manager","title":"Using the <code>bundled_eval()</code> Context Manager","text":"<p>The <code>bundled_eval()</code> is a lower-level context manager that groups multiple evaluations together based on their arguments. This is particularly useful when working with multiple user-defined evaluators that don't conform to the Patronus structured evaluator format.</p> <pre><code>import patronus\nfrom patronus.evals import bundled_eval, evaluator\n\npatronus.init()\n\n@evaluator()\ndef exact_match(actual, expected) -&gt; bool:\n    return actual == expected\n\n@evaluator()\ndef iexact_match(actual: str, expected: str) -&gt; bool:\n    return actual.strip().lower() == expected.strip().lower()\n\n# Group these evaluations together in a single trace and single log record\nwith bundled_eval():\n    exact_match(\"string\", \"string\")\n    iexact_match(\"string\", \"string\")\n</code></pre>"},{"location":"evaluations/evaluators/","title":"User-Defined Evaluators","text":"<p>Evaluators are the core building blocks of Patronus's evaluation system. This page covers how to create and use your own custom evaluators to assess LLM outputs according to your specific criteria.</p>"},{"location":"evaluations/evaluators/#creating-basic-evaluators","title":"Creating Basic Evaluators","text":"<p>The simplest way to create an evaluator is with the <code>@evaluator()</code> decorator:</p> <pre><code>from patronus import evaluator\n\n@evaluator()\ndef keyword_match(text: str, keywords: list[str]) -&gt; float:\n    \"\"\"\n    Evaluates whether the text contains the specified keywords.\n    Returns a score between 0.0 and 1.0 based on the percentage of matched keywords.\n    \"\"\"\n    matches = sum(keyword.lower() in text.lower() for keyword in keywords)\n    return matches / len(keywords) if keywords else 0.0\n</code></pre> <p>This decorator automatically:</p> <ul> <li>Integrates with the Patronus tracing</li> <li>Exports evaluation results to the Patronus Platform</li> </ul>"},{"location":"evaluations/evaluators/#flexible-input-and-output","title":"Flexible Input and Output","text":"<p>User-defined evaluators can accept any parameters and return several types of results:</p> <pre><code># Boolean evaluator (pass/fail)\n@evaluator()\ndef contains_answer(text: str, answer: str) -&gt; bool:\n    return answer.lower() in text.lower()\n\n\n# Numeric evaluator (score)\n@evaluator()\ndef semantic_similarity(text1: str, text2: str) -&gt; float:\n    # Simple example - in practice use proper semantic similarity\n    words1, words2 = set(text1.lower().split()), set(text2.lower().split())\n    intersection = words1.intersection(words2)\n    union = words1.union(words2)\n    return len(intersection) / len(union) if union else 0.0\n\n\n# String evaluator\n@evaluator()\ndef tone_classifier(text: str) -&gt; str:\n    positive = ['good', 'excellent', 'great', 'helpful']\n    negative = ['bad', 'poor', 'unhelpful', 'wrong']\n\n    pos_count = sum(word in text.lower() for word in positive)\n    neg_count = sum(word in text.lower() for word in negative)\n\n    if pos_count &gt; neg_count:\n        return \"positive\"\n    elif neg_count &gt; pos_count:\n        return \"negative\"\n    else:\n        return \"neutral\"\n</code></pre>"},{"location":"evaluations/evaluators/#return-types","title":"Return Types","text":"<p>Evaluators can return different types which are automatically converted to <code>EvaluationResult</code> objects:</p> <ul> <li>Boolean: <code>True</code>/<code>False</code> indicating pass/fail</li> <li>Float/Integer: Numerical scores (typically between 0-1)</li> <li>String: Text output categorizing the result</li> <li>EvaluationResult: Complete evaluation with scores, explanations, etc.</li> </ul>"},{"location":"evaluations/evaluators/#using-evaluationresult","title":"Using EvaluationResult","text":"<p>For more detailed evaluations, return an <code>EvaluationResult</code> object:</p> <pre><code>from patronus import evaluator\nfrom patronus.evals import EvaluationResult\n\n@evaluator()\ndef comprehensive_evaluation(response: str, reference: str) -&gt; EvaluationResult:\n    # Example implementation - replace with actual logic\n    has_keywords = all(word in response.lower() for word in [\"important\", \"key\", \"concept\"])\n    accuracy = 0.85  # Calculated accuracy score\n\n    return EvaluationResult(\n        score=accuracy,  # Numeric score (typically 0-1)\n        pass_=accuracy &gt;= 0.7,  # Boolean pass/fail\n        text_output=\"Satisfactory\" if accuracy &gt;= 0.7 else \"Needs improvement\",  # Category\n        explanation=f\"Response {'contains' if has_keywords else 'is missing'} key terms. Accuracy: {accuracy:.2f}\",\n        metadata={  # Additional structured data\n            \"has_required_keywords\": has_keywords,\n            \"response_length\": len(response),\n            \"accuracy\": accuracy\n        }\n    )\n</code></pre> <p>The <code>EvaluationResult</code> object can include:</p> <ul> <li>score: Numerical assessment (typically 0-1)</li> <li>pass_: Boolean pass/fail status</li> <li>text_output: Categorical or textual result</li> <li>explanation: Human-readable explanation of the result</li> <li>metadata: Additional structured data for analysis</li> <li>tags: Key-value pairs for filtering and organization</li> </ul>"},{"location":"evaluations/evaluators/#using-evaluators","title":"Using Evaluators","text":"<p>Once defined, evaluators can be used directly:</p> <pre><code># Use evaluators as normal function\nresult = keyword_match(\"The capital of France is Paris\", [\"capital\", \"France\", \"Paris\"])\nprint(f\"Score: {result}\")  # Output: Score: 1.0\n\n\n# Using class-based evaluator\nsafety_check = ContentSafetyEvaluator()\nresult = safety_check.evaluate(\n    task_output=\"This is a helpful and safe response.\"\n)\nprint(f\"Safety check passed: {result.pass_}\")  # Output: Safety check passed: True\n</code></pre>"},{"location":"evaluations/patronus-evaluators/","title":"Patronus Evaluators","text":"<p>Patronus provides a suite of evaluators that help you assess LLM outputs without writing complex evaluation logic. These managed evaluators run on Patronus infrastructure. Visit Patronus Platform console to define your own criteria.</p>"},{"location":"evaluations/patronus-evaluators/#using-patronus-evaluators","title":"Using Patronus Evaluators","text":"<p>You can use Patronus evaluators through the <code>RemoteEvaluator</code> class:</p> <pre><code>from patronus import init\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nfactual_accuracy = RemoteEvaluator(\"judge\", \"factual-accuracy\")\n\n# Evaluate an LLM output\nresult = factual_accuracy.evaluate(\n    task_input=\"What is the capital of France?\",\n    task_output=\"The capital of France is Paris, which is located on the Seine River.\",\n    gold_answer=\"Paris\"\n)\n\nprint(f\"Passed: {result.pass_}\")\nprint(f\"Score: {result.score}\")\nprint(f\"Explanation: {result.explanation}\")\n</code></pre>"},{"location":"evaluations/patronus-evaluators/#synchronous-and-asynchronous-versions","title":"Synchronous and Asynchronous Versions","text":"<p>Patronus evaluators are available in both synchronous and asynchronous versions:</p> <pre><code># Synchronous usage (as shown above)\nfactual_accuracy = RemoteEvaluator(\"judge\", \"factual-accuracy\")\nresult = factual_accuracy.evaluate(...)\n\n# Asynchronous usage\nfrom patronus.evals import AsyncRemoteEvaluator\n\nasync_factual_accuracy = AsyncRemoteEvaluator(\"judge\", \"factual-accuracy\")\nresult = await async_factual_accuracy.evaluate(...)\n</code></pre>"},{"location":"getting-started/initialization/","title":"Initialization","text":""},{"location":"getting-started/initialization/#api-key","title":"API Key","text":"<p>To use the Patronus SDK, you'll need an API key from the Patronus platform. If you don't have one yet:</p> <ol> <li>Sign up at https://app.patronus.ai</li> <li>Navigate to \"API Keys\"</li> <li>Create a new API key</li> </ol>"},{"location":"getting-started/initialization/#configuration","title":"Configuration","text":"<p>There are several ways to configure the Patronus SDK:</p>"},{"location":"getting-started/initialization/#environment-variables","title":"Environment Variables","text":"<p>Set your API key as an environment variable:</p> <pre><code>export PATRONUS_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"getting-started/initialization/#configuration-file","title":"Configuration File","text":"<p>Create a <code>patronus.yaml</code> file in your project directory:</p> <pre><code>api_key: \"your-api-key\"\nproject_name: \"Global\"\napp: \"default\"\n</code></pre>"},{"location":"getting-started/initialization/#direct-configuration","title":"Direct Configuration","text":"<p>Pass configuration values directly when initializing the SDK:</p> <pre><code>import patronus\n\npatronus.init(\n    api_key=\"your-api-key\",\n    project_name=\"Global\",\n    app=\"default\",\n)\n</code></pre>"},{"location":"getting-started/initialization/#verification","title":"Verification","text":"<p>To verify your installation and configuration:</p> <pre><code>import patronus\n\npatronus.init()\n\n# Create a simple tracer\n@patronus.traced()\ndef test_function():\n    return \"Installation successful!\"\n\n# Call the function to test tracing\nresult = test_function()\nprint(result)\n</code></pre> <p>If no errors occur, your Patronus SDK is correctly installed and configured.</p>"},{"location":"getting-started/initialization/#next-steps","title":"Next Steps","text":"<p>Now that you've installed the Patronus SDK, proceed to the Quickstart guide to learn how to use it effectively.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>The Patronus SDK provides tools for evaluating, monitoring, and improving LLM applications. This guide will help you install and set up the SDK in your environment.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>The recommended way to install the Patronus SDK is via pip:</p> <pre><code>pip install patronus\n</code></pre> <p>For a specific version:</p> <pre><code>pip install patronus==0.1.0\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next steps:","text":"<ul> <li>Initialization</li> <li>Quickstart</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with the Patronus SDK through three practical examples. We'll explore tracing, evaluation, and experimentation to give you a hands-on introduction to the core features.</p>"},{"location":"getting-started/quickstart/#initialization","title":"Initialization","text":"<p>Before running any of the examples, initialize the Patronus SDK:</p> <pre><code>import os\nimport patronus\n\n# Initialize with your API key\npatronus.init(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"PATRONUS_API_KEY\")\n)\n</code></pre> <p>You can also use a configuration file instead of direct initialization:</p> <pre><code># patronus.yaml\n\napi_key: \"your-api-key\"\nproject_name:  \"Global\"\napp: \"default\"\n</code></pre>"},{"location":"getting-started/quickstart/#example-1-tracing-with-a-functional-evaluator","title":"Example 1: Tracing with a Functional Evaluator","text":"<p>This example demonstrates how to trace function execution and create a simple functional evaluator.</p> <pre><code>import patronus\nfrom patronus import evaluator, traced\n\npatronus.init()\n\n@evaluator()\ndef exact_match(expected: str, actual: str) -&gt; bool:\n    return expected.strip() == actual.strip()\n\n@traced()\ndef process_query(query: str) -&gt; str:\n    # In a real application, this would call an LLM\n    return f\"Processed response for: {query}\"\n\n# Use the traced function and evaluator together\n@traced()\ndef main():\n    query = \"What is machine learning?\"\n    response = process_query(query)\n    print(f\"Response: {response}\")\n\n    expected_response = \"Processed response for: What is machine learning?\"\n    result = exact_match(expected_response, response)\n    print(f\"Evaluation result: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>In this example:</p> <ol> <li>We created a simple <code>exact_match</code> evaluator using the <code>@evaluator()</code> decorator</li> <li>We traced the <code>process_query</code> function using the <code>@traced()</code> decorator</li> <li>We ran an evaluation by calling the evaluator function directly</li> </ol> <p>The tracing will automatically capture execution details, timing, and results, making them available in the Patronus platform.</p>"},{"location":"getting-started/quickstart/#example-2-using-a-patronus-evaluator","title":"Example 2: Using a Patronus Evaluator","text":"<p>This example shows how to use a Patronus Evaluator to assess model outputs for hallucinations.</p> <pre><code>import patronus\nfrom patronus import traced\nfrom patronus.evals import RemoteEvaluator\n\npatronus.init()\n\n\n@traced()\ndef generate_insurance_response(query: str) -&gt; str:\n    # In a real application, this would call an LLM\n    return \"To even qualify for our car insurance policy, you need to have a valid driver's license that expires later than 2028.\"\n\n\n@traced(\"Quickstart: detect hallucination\")\ndef main():\n    check_hallucinates = RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n\n    context = \"\"\"\n    To qualify for our car insurance policy, you need a way to show competence\n    in driving which can be accomplished through a valid driver's license.\n    You must have multiple years of experience and cannot be graduating from driving school before or on 2028.\n    \"\"\"\n\n    query = \"What is the car insurance policy?\"\n    response = generate_insurance_response(query)\n    print(f\"Query: {query}\")\n    print(f\"Response: {response}\")\n\n    # Evaluate the response for hallucinations\n    resp = check_hallucinates.evaluate(\n        task_input=query,\n        task_context=context,\n        task_output=response\n    )\n\n    # Print the evaluation results\n    print(f\"\"\"\nHallucination evaluation:\nPassed: {resp.pass_}\nScore: {resp.score}\nExplanation: {resp.explanation}\n\"\"\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>In this example:</p> <ol> <li>We created a traced function generate_insurance_response to simulate an LLM response</li> <li>We used the Patronus Lynx Evaluator</li> <li>We evaluated whether the response contains information not supported by the context</li> <li>We displayed the detailed evaluation results</li> </ol> <p>Patronus Evaluators run on Patronus infrastructure and provide sophisticated assessment capabilities without requiring you to implement complex evaluation logic.</p>"},{"location":"getting-started/quickstart/#example-3-running-an-experiment-with-openai","title":"Example 3: Running an Experiment with OpenAI","text":"<p>This example demonstrates how to run a comprehensive experiment to evaluate OpenAI model performance across multiple samples and criteria.</p> <p>Before running Example 3, you'll need to install Pandas and the OpenAI SDK and OpenInference instrumentation:</p> <pre><code>pip install pandas openai openinference-instrumentation-openai\n</code></pre> <p>The OpenInference instrumentation automatically adds spans for all OpenAI API calls, capturing prompts, responses, and model parameters without any code changes. These details will appear in your Patronus traces for complete visibility into model interactions.</p> <pre><code>from typing import Optional\nimport os\n\nimport patronus\nfrom patronus.evals import evaluator, RemoteEvaluator, EvaluationResult\nfrom patronus.experiments import run_experiment, FuncEvaluatorAdapter, Row, TaskResult\nfrom openai import OpenAI\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\noai = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\npatronus.init()\n\n\n@evaluator()\ndef fuzzy_match(row: Row, task_result: TaskResult, **kwargs) -&gt; Optional[EvaluationResult]:\n    if not row.gold_answer or not task_result:\n        return None\n\n    gold_answer = row.gold_answer.lower()\n    response = task_result.output.lower()\n\n    key_terms = [term.strip() for term in gold_answer.split(',')]\n    matches = sum(1 for term in key_terms if term in response)\n    match_ratio = matches / len(key_terms) if key_terms else 0\n\n    # Return a score between 0-1 indicating match quality\n    return EvaluationResult(\n        pass_=match_ratio &gt; 0.7,\n        score=match_ratio,\n    )\n\n\ndef rag_task(row, **kwargs):\n    # In a real RAG system, this would retrieve context before calling the LLM\n    prompt = f\"\"\"\n    Based on the following context, answer the question.\n\n    Context:\n    {row.task_context}\n\n    Question: {row.task_input}\n\n    Answer:\n    \"\"\"\n\n    # Call OpenAI to generate a response\n    response = oai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\",\n             \"content\": \"You are a helpful assistant that answers questions based only on the provided context.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=150\n    )\n\n    return response.choices[0].message.content\n\n\ntest_data = [\n    {\n        \"task_input\": \"What is the main impact of climate change on coral reefs?\",\n        \"task_context\": \"\"\"\n        Climate change affects coral reefs through several mechanisms. Rising sea temperatures can cause coral bleaching,\n        where corals expel their symbiotic algae and turn white, often leading to death. Ocean acidification, caused by\n        increased CO2 absorption, makes it harder for corals to build their calcium carbonate structures. Sea level rise\n        can reduce light availability for photosynthesis. More frequent and intense storms damage reef structures. The\n        combination of these stressors is devastating to coral reef ecosystems worldwide.\n        \"\"\",\n        \"gold_answer\": \"coral bleaching, ocean acidification, reduced calcification, habitat destruction\"\n    },\n    {\n        \"task_input\": \"How do quantum computers differ from classical computers?\",\n        \"task_context\": \"\"\"\n        Classical computers process information in bits (0s and 1s), while quantum computers use quantum bits or qubits.\n        Qubits can exist in multiple states simultaneously thanks to superposition, allowing quantum computers to process\n        vast amounts of information in parallel. Quantum entanglement enables qubits to be correlated in ways impossible\n        for classical bits. While classical computers excel at everyday tasks, quantum computers potentially have advantages\n        for specific problems like cryptography, simulation of quantum systems, and certain optimization tasks. However,\n        quantum computers face significant challenges including qubit stability, error correction, and scaling up to useful sizes.\n        \"\"\",\n        \"gold_answer\": \"qubits instead of bits, superposition, entanglement, parallel processing\"\n    }\n]\n\nevaluators = [\n    FuncEvaluatorAdapter(fuzzy_match),\n    RemoteEvaluator(\"answer-relevance\", \"patronus:answer-relevance\")\n]\n\n# Run the experiment with OpenInference instrumentation\nprint(\"Running RAG evaluation experiment...\")\nexperiment = run_experiment(\n    dataset=test_data,\n    task=rag_task,\n    evaluators=evaluators,\n    tags={\"system\": \"rag-prototype\", \"model\": \"gpt-3.5-turbo\"},\n    integrations=[OpenAIInstrumentor()]\n)\n\n# Export results to CSV (optional)\n# experiment.to_csv(\"rag_evaluation_results.csv\")\n</code></pre> <p>In this example:</p> <ol> <li>We defined a task function <code>answer_questions</code> that generates responses for our experiment</li> <li>We created a custom evaluator <code>contains_key_information</code> to check for specific content</li> <li>We set up an experiment with multiple evaluators (both remote and custom)</li> <li>We ran the experiment across a dataset of questions</li> </ol> <p>Experiments provide a powerful way to systematically evaluate your LLM applications across multiple samples and criteria, helping you identify strengths and weaknesses in your models.</p>"},{"location":"observability/logging/","title":"Logging","text":"<p>Logging is an essential feature of the Patronus SDK that allows you to record events, debug information, and track the execution of your LLM applications. This page covers how to set up and use logging in your code.</p>"},{"location":"observability/logging/#getting-started-with-logging","title":"Getting Started with Logging","text":"<p>The Patronus SDK provides a simple logging interface that integrates with Python's standard logging module while also automatically exporting logs to the Patronus AI Platform:</p> <pre><code>import patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\n# Basic logging\nlog.info(\"Processing user query\")\n\n# Different log levels are available\nlog.debug(\"Detailed debug information\")\nlog.warning(\"Something might be wrong\")\nlog.error(\"An error occurred\")\nlog.critical(\"System cannot continue\")\n</code></pre>"},{"location":"observability/logging/#configuring-console-output","title":"Configuring Console Output","text":"<p>By default, Patronus logs are sent to the Patronus AI Platform but are not printed to the console. To display logs in your console output, you can add a standard Python logging handler:</p> <pre><code>import sys\nimport logging\nimport patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\n# Add a console handler to see logs in your terminal\nconsole_handler = logging.StreamHandler(sys.stdout)\nlog.addHandler(console_handler)\n\n# Now logs will appear in both console and Patronus Platform\nlog.info(\"This message appears in the console and is sent to Patronus\")\n</code></pre> <p>You can also customize the format of console logs:</p> <pre><code>import sys\nimport logging\nimport patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\nformatter = logging.Formatter('[%(asctime)s] %(levelname)-8s: %(message)s')\n\nconsole_handler = logging.StreamHandler(sys.stdout)\nconsole_handler.setFormatter(formatter)\nlog.addHandler(console_handler)\n\n# Logs will now include timestamp and level\nlog.info(\"Formatted log message\")\n</code></pre>"},{"location":"observability/logging/#advanced-configuration","title":"Advanced Configuration","text":"<p>Patronus integrates with Python's logging module, allowing for advanced configuration options. The SDK uses two main loggers:</p> <ul> <li><code>patronus.sdk</code> - For client-emitted messages that are automatically exported to the Patronus AI Platform</li> <li><code>patronus.core</code> - For library-emitted messages related to the SDK's internal operations</li> </ul> <p>Here's how to configure these loggers using standard library methods:</p> <pre><code>import logging\nimport patronus\n\n# Initialize Patronus before configuring logging\npatronus.init()\n\n# Configure the root Patronus logger\npatronus_root_logger = logging.getLogger(\"patronus\")\npatronus_root_logger.setLevel(logging.WARNING)  # Set base level for all Patronus loggers\n\n# Add a console handler with custom formatting\nconsole_handler = logging.StreamHandler()\nformatter = logging.Formatter(\n    fmt='[%(asctime)s] %(levelname)-8s %(name)s: %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nconsole_handler.setFormatter(formatter)\npatronus_root_logger.addHandler(console_handler)\n\n# Configure specific loggers\npatronus_core_logger = logging.getLogger(\"patronus.core\")\npatronus_core_logger.setLevel(logging.WARNING)  # Only show warnings and above for internal SDK messages\n\npatronus_sdk_logger = logging.getLogger(\"patronus.sdk\")\npatronus_sdk_logger.setLevel(logging.INFO)  # Show info and above for your application logs\n</code></pre>"},{"location":"observability/logging/#logging-with-traces","title":"Logging with Traces","text":"<p>Patronus logging integrates seamlessly with the tracing system, allowing you to correlate logs with specific spans in your application flow:</p> <pre><code>import patronus\nfrom patronus import traced, start_span\n\npatronus.init()\nlog = patronus.get_logger()\n\n@traced()\ndef process_user_query(query):\n    log.info(\"Processing query\")\n\n    with start_span(\"Query Analysis\"):\n        log.info(\"Analyzing query intent\")\n        ...\n\n    with start_span(\"Response Generation\"):\n        log.info(\"Generating LLM response\")\n        ...\n\n    return \"Response to: \" + query\n\n# Logs will be associated with the appropriate spans\nresult = process_user_query(\"Tell me about machine learning\")\n</code></pre>"},{"location":"observability/tracing/","title":"Tracing","text":"<p>Tracing is a core feature of the Patronus SDK that allows you to monitor and understand the behavior of your LLM applications. This page covers how to set up and use tracing in your code.</p>"},{"location":"observability/tracing/#getting-started-with-tracing","title":"Getting Started with Tracing","text":"<p>Tracing in Patronus works through two main mechanisms:</p> <ol> <li>Function decorators: Easily trace entire functions</li> <li>Context managers: Trace specific code blocks within functions</li> </ol>"},{"location":"observability/tracing/#using-the-traced-decorator","title":"Using the <code>@traced()</code> Decorator","text":"<p>The simplest way to add tracing is with the <code>@traced()</code> decorator:</p> <pre><code>import patronus\nfrom patronus import traced\n\npatronus.init()\n\n@traced()\ndef generate_response(prompt: str) -&gt; str:\n    # Your LLM call or processing logic here\n    return f\"Response to: {prompt}\"\n\n# Call the traced function\nresult = generate_response(\"Tell me about machine learning\")\n</code></pre>"},{"location":"observability/tracing/#decorator-options","title":"Decorator Options","text":"<p>The <code>@traced()</code> decorator accepts several parameters for customization:</p> <pre><code>@traced(\n    span_name=\"Custom span name\",   # Default: function name\n    log_args=True,                  # Whether to log function arguments\n    log_results=True,               # Whether to log function return values\n    log_exceptions=True,            # Whether to log exceptions\n    disable_log=False,              # Completely disable logging (maintains spans)\n    attributes={\"key\": \"value\"}     # Custom attributes to add to the span\n)\ndef my_function():\n    pass\n</code></pre> <p>See the API Reference for complete details.</p>"},{"location":"observability/tracing/#using-the-start_span-context-manager","title":"Using the <code>start_span()</code> Context Manager","text":"<p>For more granular control, use the <code>start_span()</code> context manager to trace specific blocks of code:</p> <pre><code>import patronus\nfrom patronus.tracing import start_span\n\npatronus.init()\n\ndef complex_workflow(data):\n    # First phase\n    with start_span(\"Data preparation\", attributes={\"data_size\": len(data)}):\n        prepared_data = preprocess(data)\n\n    # Second phase\n    with start_span(\"Model inference\"):\n        results = run_model(prepared_data)\n\n    # Third phase\n    with start_span(\"Post-processing\"):\n        final_results = postprocess(results)\n\n    return final_results\n</code></pre>"},{"location":"observability/tracing/#context-manager-options","title":"Context Manager Options","text":"<p>The <code>start_span()</code> context manager accepts these parameters:</p> <pre><code>with start_span(\n    \"Span name\",                        # Name of the span (required)\n    record_exception=False,             # Whether to record exceptions\n    attributes={\"custom\": \"attribute\"}  # Custom attributes to add\n) as span:\n    # Your code here\n    # You can also add attributes during execution:\n    span.set_attribute(\"dynamic_value\", 42)\n</code></pre> <p>See the API Reference for complete details.</p>"},{"location":"observability/tracing/#custom-attributes","title":"Custom Attributes","text":"<p>Both tracing methods allow you to add custom attributes that provide additional context for your traces:</p> <pre><code>@traced(attributes={\n    \"model\": \"gpt-4\",\n    \"version\": \"1.0\",\n    \"temperature\": 0.7\n})\ndef generate_with_gpt4(prompt):\n    # Function implementation\n    pass\n\n# Or with context manager\nwith start_span(\"Query processing\", attributes={\n    \"query_type\": \"search\",\n    \"filters_applied\": True,\n    \"result_limit\": 10\n}):\n    # Processing code\n    pass\n</code></pre>"}]}