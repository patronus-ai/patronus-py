{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Patronus Python SDK Documentation","text":"<p>The Patronus SDK provides tools for observability, evaluation, and experimentation with Large Language Models (LLMs), helping you build reliable and high-quality AI applications.</p> <p>This documentation covers only the Python SDK. For comprehensive documentation on the Patronus platform, evaluators, and best practices, please visit the official Patronus documentation.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation and Quickstart - Install the SDK and run your first evaluation</li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#patronus.evals","title":"evals","text":""},{"location":"api/#patronus.evals.evaluators","title":"evaluators","text":""},{"location":"api/#patronus.evals.evaluators.EvaluationDataRecord","title":"EvaluationDataRecord","text":"<p>               Bases: <code>NamedTuple</code></p> <p>EvaluationDataRecord holds evaluation log data and ID of the log. Log data consists of positional arguments (args) and keyword arguments (kwargs) that were passed to an evaluator.</p>"},{"location":"api/#patronus.evals.evaluators.UniqueEvaluationDataSet","title":"UniqueEvaluationDataSet","text":"<pre><code>UniqueEvaluationDataSet(span_context: SpanContext)\n</code></pre> <p>UniqueEvaluationDataSet holds unique list evaluation log data. This container is used to track and ensure that duplicated evaluation data logs are not emitted.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(self, span_context: SpanContext):\n    self.span_context = span_context\n    self.logs = []\n    self.lock = threading.Lock()\n</code></pre>"},{"location":"api/#patronus.evals.evaluators.UniqueEvaluationDataSet.find_log","title":"find_log","text":"<pre><code>find_log(bound_arguments) -&gt; Optional[LogID]\n</code></pre> <p>Must be used with lock</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def find_log(self, bound_arguments) -&gt; Optional[LogID]:\n    \"\"\"Must be used with lock\"\"\"\n    for lg in self.logs:\n        if bound_arguments == lg.arguments:\n            return lg.log_id\n    return None\n</code></pre>"},{"location":"api/#patronus.evals.evaluators.Evaluator","title":"Evaluator","text":""},{"location":"api/#patronus.evals.evaluators.Evaluator.evaluate","title":"evaluate  <code>abstractmethod</code>","text":"<pre><code>evaluate(*args, **kwargs) -&gt; Optional[EvaluationResult]\n</code></pre> <p>Synchronous version of evaluate method. When inheriting directly from Evaluator class it's permitted to change parameters signature. Return type should stay unchanged.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>@abc.abstractmethod\ndef evaluate(self, *args, **kwargs) -&gt; Optional[EvaluationResult]:\n    \"\"\"\n    Synchronous version of evaluate method.\n    When inheriting directly from Evaluator class it's permitted to change parameters signature.\n    Return type should stay unchanged.\n    \"\"\"\n</code></pre>"},{"location":"api/#patronus.evals.evaluators.AsyncEvaluator","title":"AsyncEvaluator","text":"<p>               Bases: <code>Evaluator</code></p>"},{"location":"api/#patronus.evals.evaluators.AsyncEvaluator.evaluate","title":"evaluate  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>evaluate(*args, **kwargs) -&gt; Optional[EvaluationResult]\n</code></pre> <p>Asynchronous version of evaluate method. When inheriting directly from Evaluator class it's permitted to change parameters signature. Return type should stay unchanged.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>@abc.abstractmethod\nasync def evaluate(self, *args, **kwargs) -&gt; Optional[EvaluationResult]:\n    \"\"\"\n    Asynchronous version of evaluate method.\n    When inheriting directly from Evaluator class it's permitted to change parameters signature.\n    Return type should stay unchanged.\n    \"\"\"\n</code></pre>"},{"location":"api/#patronus.evals.evaluators.get_current_log_id","title":"get_current_log_id","text":"<pre><code>get_current_log_id(bound_arguments: dict[str, Any]) -&gt; Optional[LogID]\n</code></pre> <p>Return log_id for given arguments in current context. Returns None if there is no context - most likely SDK is not initialized.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def get_current_log_id(bound_arguments: dict[str, Any]) -&gt; Optional[LogID]:\n    \"\"\"\n    Return log_id for given arguments in current context.\n    Returns None if there is no context - most likely SDK is not initialized.\n    \"\"\"\n    eval_group = _ctx_evaluation_log_group.get(None)\n    if eval_group is None:\n        return None\n    log_id = eval_group.find_log(bound_arguments)\n    if log_id is None:\n        raise ValueError(\"Log not found for provided arguments\")\n    return log_id\n</code></pre>"},{"location":"api/#patronus.evals.exporter","title":"exporter","text":""},{"location":"api/#patronus.evals.exporter.Once","title":"Once","text":"<pre><code>Once()\n</code></pre> <p>Execute a function exactly once and block all callers until the function returns</p> <p>Same as golang's <code>sync.Once &lt;https://pkg.go.dev/sync#Once&gt;</code>_</p> Source code in <code>src/patronus/evals/exporter.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._lock = Lock()\n    self._done = False\n</code></pre>"},{"location":"api/#patronus.evals.exporter.Once.do_once","title":"do_once","text":"<pre><code>do_once(func: Callable[[], None]) -&gt; bool\n</code></pre> <p>Execute <code>func</code> if it hasn't been executed or return.</p> <p>Will block until <code>func</code> has been called by one thread.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether <code>func</code> was executed in this call</p> Source code in <code>src/patronus/evals/exporter.py</code> <pre><code>def do_once(self, func: Callable[[], None]) -&gt; bool:\n    \"\"\"Execute ``func`` if it hasn't been executed or return.\n\n    Will block until ``func`` has been called by one thread.\n\n    Returns:\n        Whether ``func`` was executed in this call\n    \"\"\"\n\n    # fast path, try to avoid locking\n    if self._done:\n        return False\n\n    with self._lock:\n        if not self._done:\n            func()\n            self._done = True\n            return True\n    return False\n</code></pre>"},{"location":"api/#patronus.experiments","title":"experiments","text":""},{"location":"api/#patronus.experiments.adapters","title":"adapters","text":""},{"location":"api/#patronus.experiments.adapters.BaseEvaluatorAdapter","title":"BaseEvaluatorAdapter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all evaluator adapters.</p> <p>Evaluator adapters provide a standardized interface between the experiment framework and various types of evaluators (function-based, class-based, etc.).</p> <p>All concrete adapter implementations must inherit from this class and implement the required abstract methods.</p>"},{"location":"api/#patronus.experiments.adapters.EvaluatorAdapter","title":"EvaluatorAdapter","text":"<pre><code>EvaluatorAdapter(evaluator: Evaluator)\n</code></pre> <p>               Bases: <code>BaseEvaluatorAdapter</code></p> <p>Adapter for class-based evaluators conforming to the Evaluator or AsyncEvaluator protocol.</p> <p>This adapter enables the use of evaluator classes that implement either the Evaluator or AsyncEvaluator interface within the experiment framework.</p> <p>Attributes:</p> Name Type Description <code>evaluator</code> <code>Union[Evaluator, AsyncEvaluator]</code> <p>The evaluator instance to adapt.</p> <p>Examples:</p> <pre><code>```python\nimport typing\nfrom typing import Optional\n\nfrom patronus import datasets\nfrom patronus.evals import Evaluator, EvaluationResult\nfrom patronus.experiments import run_experiment\nfrom patronus.experiments.adapters import EvaluatorAdapter\nfrom patronus.experiments.types import TaskResult, EvalParent\n\n\nclass MatchEvaluator(Evaluator):\n    def __init__(self, sanitizer=None):\n        if sanitizer is None:\n            sanitizer = lambda x: x\n        self.sanitizer = sanitizer\n\n    def evaluate(self, actual: str, expected: str) -&gt; EvaluationResult:\n        matched = self.sanitizer(actual) == self.sanitizer(expected)\n        return EvaluationResult(pass_=matched, score=int(matched))\n\n\nexact_match = MatchEvaluator()\nfuzzy_match = MatchEvaluator(lambda x: x.strip().lower())\n\n\nclass MatchAdapter(EvaluatorAdapter):\n    def __init__(self, evaluator: MatchEvaluator):\n        super().__init__(evaluator)\n\n    def transform(\n        self,\n        row: datasets.Row,\n        task_result: Optional[TaskResult],\n        parent: EvalParent,\n        **kwargs\n    ) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n        args = [row.task_output, row.gold_answer]\n        kwargs = {}\n        # Passing arguments via kwargs would also work in this case.\n        # kwargs = {\"actual\": row.task_output, \"expected\": row.gold_answer}\n        return args, kwargs\n\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string    \", \"gold_answer\": \"string\"}],\n    evaluators=[MatchAdapter(exact_match), MatchAdapter(fuzzy_match)],\n)\n```\n</code></pre> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(self, evaluator: evals.Evaluator):\n    if not isinstance(evaluator, evals.Evaluator):\n        raise TypeError(f\"{evaluator} is not {evals.Evaluator.__name__}.\")\n    self.evaluator = evaluator\n</code></pre>"},{"location":"api/#patronus.experiments.adapters.EvaluatorAdapter.transform","title":"transform","text":"<pre><code>transform(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs) -&gt; tuple[list[Any], dict[str, Any]]\n</code></pre> <p>Transform experiment framework arguments to evaluation method arguments.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[Any], dict[str, Any]]</code> <p>A tuple containing: - A list of positional arguments to pass to the evaluator function. - A dictionary of keyword arguments to pass to the evaluator function.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def transform(\n    self, row: datasets.Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs\n) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n    \"\"\"\n    Transform experiment framework arguments to evaluation method arguments.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        A tuple containing:\n          - A list of positional arguments to pass to the evaluator function.\n          - A dictionary of keyword arguments to pass to the evaluator function.\n    \"\"\"\n\n    return ([], {\"row\": row, \"task_result\": task_result, \"prent\": parent, **kwargs})\n</code></pre>"},{"location":"api/#patronus.experiments.adapters.EvaluatorAdapter.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs) -&gt; EvaluationResult\n</code></pre> <p>Evaluate the given row and task result using the adapted evaluator function.</p> <p>This method implements the BaseEvaluatorAdapter.evaluate() protocol.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>An EvaluationResult containing the evaluation outcome.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>async def evaluate(\n    self, row: datasets.Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs\n) -&gt; EvaluationResult:\n    \"\"\"\n    Evaluate the given row and task result using the adapted evaluator function.\n\n    This method implements the BaseEvaluatorAdapter.evaluate() protocol.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        An EvaluationResult containing the evaluation outcome.\n    \"\"\"\n    ev_args, ev_kwargs = self.transform(row, task_result, parent, **kwargs)\n    return await self._evaluate(*ev_args, **ev_kwargs)\n</code></pre>"},{"location":"api/#patronus.experiments.adapters.StructuredEvaluatorAdapter","title":"StructuredEvaluatorAdapter","text":"<pre><code>StructuredEvaluatorAdapter(evaluator: AsyncStructuredEvaluator)\n</code></pre> <p>               Bases: <code>EvaluatorAdapter</code></p> <p>Adapter for structured evaluators.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(self, evaluator: evals.AsyncStructuredEvaluator):\n    if not isinstance(evaluator, (evals.StructuredEvaluator, evals.AsyncStructuredEvaluator)):\n        raise TypeError(\n            f\"{type(evaluator)} is not \"\n            f\"{evals.AsyncStructuredEvaluator.__name__} nor {evals.StructuredEvaluator.__name__}.\"\n        )\n    super().__init__(evaluator)\n</code></pre>"},{"location":"api/#patronus.experiments.adapters.FuncEvaluatorAdapter","title":"FuncEvaluatorAdapter","text":"<pre><code>FuncEvaluatorAdapter(fn: Callable[..., Any])\n</code></pre> <p>               Bases: <code>BaseEvaluatorAdapter</code></p> <p>Adapter class that allows using function-based evaluators with the experiment framework.</p> <p>This adapter serves as a bridge between function-based evaluators decorated with <code>@evaluator()</code> and the experiment framework's evaluation system. It handles both synchronous and asynchronous evaluator functions.</p> <p>Attributes:</p> Name Type Description <code>fn</code> <code>Callable</code> <p>The evaluator function to be adapted.</p> Notes <ul> <li>The function passed to this adapter must be decorated with <code>@evaluator()</code>.</li> <li>The adapter automatically handles the conversion between function results and proper   evaluation result objects.</li> </ul> <p>Examples:</p> <pre><code>Direct usage with a compatible evaluator function:\n\n```python\nfrom patronus import evaluator\nfrom patronus.experiments import FuncEvaluatorAdapter, run_experiment\nfrom patronus.datasets import Row\n\n\n@evaluator()\ndef exact_match(row: Row, **kwargs):\n    return row.task_output == row.gold_answer\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string\", \"gold_answer\": \"string\"}],\n    evaluators=[FuncEvaluatorAdapter(exact_match)]\n)\n```\n\nCustomized usage by overriding the `transform()` method:\n\n```python\nfrom typing import Optional\nimport typing\n\nfrom patronus import evaluator, datasets\nfrom patronus.experiments import FuncEvaluatorAdapter, run_experiment\nfrom patronus.experiments.types import TaskResult, EvalParent\n\n\n@evaluator()\ndef exact_match(actual, expected):\n    return actual == expected\n\n\nclass AdaptedExactMatch(FuncEvaluatorAdapter):\n    def __init__(self):\n        super().__init__(exact_match)\n\n    def transform(\n        self,\n        row: datasets.Row,\n        task_result: Optional[TaskResult],\n        parent: EvalParent,\n        **kwargs\n    ) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n        args = [row.task_output, row.gold_answer]\n        kwargs = {}\n\n        # Alternative: passing arguments via kwargs instead of args\n        # args = []\n        # kwargs = {\"actual\": row.task_output, \"expected\": row.gold_answer}\n\n        return args, kwargs\n\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string\", \"gold_answer\": \"string\"}],\n    evaluators=[AdaptedExactMatch()],\n)\n```\n</code></pre> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(self, fn: typing.Callable[..., typing.Any]):\n    if not hasattr(fn, \"_pat_evaluator\"):\n        raise ValueError(\n            f\"Passed function {fn.__qualname__} is not an evaluator. \"\n            \"Hint: add @evaluator decorator to the function.\"\n        )\n    self.fn = fn\n</code></pre>"},{"location":"api/#patronus.experiments.adapters.FuncEvaluatorAdapter.transform","title":"transform","text":"<pre><code>transform(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs) -&gt; tuple[list[Any], dict[str, Any]]\n</code></pre> <p>Transform experiment framework parameters to evaluator function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[Any], dict[str, Any]]</code> <p>A tuple containing: - A list of positional arguments to pass to the evaluator function. - A dictionary of keyword arguments to pass to the evaluator function.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def transform(\n    self, row: datasets.Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs\n) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n    \"\"\"\n    Transform experiment framework parameters to evaluator function parameters.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        A tuple containing:\n          - A list of positional arguments to pass to the evaluator function.\n          - A dictionary of keyword arguments to pass to the evaluator function.\n    \"\"\"\n\n    return ([], {\"row\": row, \"task_result\": task_result, \"prent\": parent, **kwargs})\n</code></pre>"},{"location":"api/#patronus.experiments.adapters.FuncEvaluatorAdapter.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs) -&gt; EvaluationResult\n</code></pre> <p>Evaluate the given row and task result using the adapted evaluator function.</p> <p>This method implements the BaseEvaluatorAdapter.evaluate() protocol.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>An EvaluationResult containing the evaluation outcome.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>async def evaluate(\n    self, row: datasets.Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs\n) -&gt; EvaluationResult:\n    \"\"\"\n    Evaluate the given row and task result using the adapted evaluator function.\n\n    This method implements the BaseEvaluatorAdapter.evaluate() protocol.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        An EvaluationResult containing the evaluation outcome.\n    \"\"\"\n    ev_args, ev_kwargs = self.transform(row, task_result, parent, **kwargs)\n    return await self._evaluate(*ev_args, **ev_kwargs)\n</code></pre>"},{"location":"api/#patronus.experiments.experiment","title":"experiment","text":""},{"location":"api/#patronus.experiments.experiment.run_experiment","title":"run_experiment","text":"<pre><code>run_experiment(dataset: ExperimentDataset, task: Optional[Task] = None, evaluators: Optional[list[AdaptableEvaluators]] = None, chain: Optional[list[ChainLink]] = None, tags: Optional[Tags] = None, max_concurrency: int = 10, project_name: Optional[str] = None, experiment_name: Optional[str] = None, api_key: Optional[str] = None, **kwargs) -&gt; Experiment\n</code></pre> <p>Create and run an experiment.</p> <p>This function creates an experiment with the specified configuration and runs it to completion. The execution handling is context-aware:</p> <ul> <li>When called from an asynchronous context (with a running event loop), it returns an   awaitable that must be awaited.</li> <li>When called from a synchronous context (no running event loop), it blocks until the   experiment completes and returns the Experiment object.</li> </ul>"},{"location":"api/#patronus.experiments.experiment.run_experiment--returns","title":"Returns","text":"<p>Union[Experiment, Awaitable[Experiment]]     In a synchronous context: the completed Experiment object.     In an asynchronous context: an awaitable that resolves to the Experiment object.</p> <p>Examples:</p> <p>Synchronous execution:     experiment = run_experiment(dataset, task=some_task)     # Blocks until the experiment finishes.</p> <p>Asynchronous execution (e.g., in a Jupyter Notebook):     experiment = await run_experiment(dataset, task=some_task)     # Must be awaited within an async function or event loop.</p> Notes <p>For manual control of the event loop, you can create and run the experiment as follows:     experiment = await Experiment.create(...)     await experiment.run()</p> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>def run_experiment(\n    dataset: ExperimentDataset,\n    task: Optional[Task] = None,\n    evaluators: Optional[list[AdaptableEvaluators]] = None,\n    chain: Optional[list[ChainLink]] = None,\n    tags: Optional[Tags] = None,\n    max_concurrency: int = 10,\n    project_name: Optional[str] = None,\n    experiment_name: Optional[str] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"Experiment\":\n    \"\"\"\n    Create and run an experiment.\n\n    This function creates an experiment with the specified configuration and runs it to completion.\n    The execution handling is context-aware:\n\n    - When called from an asynchronous context (with a running event loop), it returns an\n      awaitable that must be awaited.\n    - When called from a synchronous context (no running event loop), it blocks until the\n      experiment completes and returns the Experiment object.\n\n\n    Returns\n    -------\n    Union[Experiment, Awaitable[Experiment]]\n        In a synchronous context: the completed Experiment object.\n        In an asynchronous context: an awaitable that resolves to the Experiment object.\n\n    Examples:\n        Synchronous execution:\n            experiment = run_experiment(dataset, task=some_task)\n            # Blocks until the experiment finishes.\n\n        Asynchronous execution (e.g., in a Jupyter Notebook):\n            experiment = await run_experiment(dataset, task=some_task)\n            # Must be awaited within an async function or event loop.\n\n    Notes:\n        For manual control of the event loop, you can create and run the experiment as follows:\n            experiment = await Experiment.create(...)\n            await experiment.run()\n\n    \"\"\"\n\n    async def _run_experiment() -&gt; Union[Experiment, typing.Awaitable[Experiment]]:\n        ex = await Experiment.create(\n            dataset=dataset,\n            task=task,\n            evaluators=evaluators,\n            chain=chain,\n            tags=tags,\n            max_concurrency=max_concurrency,\n            project_name=project_name,\n            experiment_name=experiment_name,\n            api_key=api_key,\n            **kwargs,\n        )\n        return await ex.run()\n\n    return run_until_complete(_run_experiment())\n</code></pre>"},{"location":"api/#patronus.experiments.tqdm","title":"tqdm","text":""},{"location":"api/#patronus.experiments.tqdm.AsyncTQDMWithHandle","title":"AsyncTQDMWithHandle","text":"<p>               Bases: <code>tqdm</code></p> <p>Workaround for accessing tqdm instance with async tasks. Instead of calling gather which don't provide access to tqdm instance: <pre><code>tqdm_async.gather(features)\n</code></pre></p> <p>Call prep_gather() follow by gather() <pre><code>tqdm_instance = AsyncTQDMWithHandle.pre_gather(features)\n...\ntqdm_instance.gather()\n</code></pre></p> <p>tqdm_instance can be used to clear and display progress bar using tqdm_instance.clear() and tqdm_instance.display() methods.</p>"},{"location":"api/#patronus.pat_client","title":"pat_client","text":""},{"location":"api/#patronus.pat_client.client_sync","title":"client_sync","text":""},{"location":"api/#patronus.pat_client.container","title":"container","text":""},{"location":"api/#patronus.pat_client.container.EvaluationContainer","title":"EvaluationContainer  <code>dataclass</code>","text":"<pre><code>EvaluationContainer(results: list[Union[EvaluationResult, None, Exception]])\n</code></pre>"},{"location":"api/#patronus.pat_client.container.EvaluationContainer.all_succeeded","title":"all_succeeded","text":"<pre><code>all_succeeded(ignore_exceptions: bool = False) -&gt; bool\n</code></pre> <p>Check if all evaluations that were actually evaluated passed.</p> <p>Evaluations are only considered if they: - Have a non-None pass_ flag set - Are not None (skipped) - Are not exceptions (unless ignore_exceptions=True)</p> <p>Note: Returns True if no evaluations met the above criteria (empty case).</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def all_succeeded(self, ignore_exceptions: bool = False) -&gt; bool:\n    \"\"\"\n    Check if all evaluations that were actually evaluated passed.\n\n    Evaluations are only considered if they:\n    - Have a non-None pass_ flag set\n    - Are not None (skipped)\n    - Are not exceptions (unless ignore_exceptions=True)\n\n    Note: Returns True if no evaluations met the above criteria (empty case).\n    \"\"\"\n    for r in self.results:\n        if isinstance(r, Exception) and not ignore_exceptions:\n            self.raise_on_exception()\n        if r is not None and r.pass_ is False:\n            return False\n    return True\n</code></pre>"},{"location":"api/#patronus.pat_client.container.EvaluationContainer.any_failed","title":"any_failed","text":"<pre><code>any_failed(ignore_exceptions: bool = False) -&gt; bool\n</code></pre> <p>Check if any evaluation that was actually evaluated failed.</p> <p>Evaluations are only considered if they: - Have a non-None pass_ flag set - Are not None (skipped) - Are not exceptions (unless ignore_exceptions=True)</p> <p>Note: Returns False if no evaluations met the above criteria (empty case).</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def any_failed(self, ignore_exceptions: bool = False) -&gt; bool:\n    \"\"\"\n    Check if any evaluation that was actually evaluated failed.\n\n    Evaluations are only considered if they:\n    - Have a non-None pass_ flag set\n    - Are not None (skipped)\n    - Are not exceptions (unless ignore_exceptions=True)\n\n    Note: Returns False if no evaluations met the above criteria (empty case).\n    \"\"\"\n    for r in self.results:\n        if isinstance(r, Exception) and not ignore_exceptions:\n            self.raise_on_exception()\n        if r is not None and r.pass_ is False:\n            return True\n    return False\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"<p>The Patronus Experimentation Framework offers several configuration options that can be set in the following ways:</p> <ol> <li>In Code</li> <li>Environment Variables</li> <li>YAML Configuration File</li> </ol> <p>Configuration options are prioritized in the order listed above.</p> Config name Environment Variable Default Value project_name PATRONUS_PROJECT_NAE <code>Global</code> app PATRONUS_APP <code>default</code> api_key PATRONUS_API_KEY api_url PATRONUS_API_URL <code>https://api.patronus.ai</code> ui_url PATRONUS_UI_URL <code>https://app.patronus.ai</code> otel_endpoint PATRONUS_OTEL_ENDPOINT <code>https://otel.patronus.ai:4317</code> timeout_s PATRONUS_TIMEOUT_S <code>300</code>"},{"location":"configuration/#configuration-file-patronusyaml","title":"Configuration File (<code>patronus.yaml</code>)","text":"<p>You can also provide configuration options using a patronus.yaml file. This file must be present in the working directory when executing your script.</p> <pre><code>project_name: \"my-project\"\napp: \"my-agent\"\n\napi_key: \"YOUR_API_KEY\"\napi_url: \"https://api.patronus.ai\"\nui_url: \"https://app.patronus.ai\"\n</code></pre>"},{"location":"evaluations/batching/","title":"Batch Evaluations","text":"<p>When evaluating multiple outputs or using multiple evaluators, Patronus provides efficient batch evaluation capabilities. This page covers how to perform batch evaluations and manage evaluation groups.</p>"},{"location":"evaluations/batching/#using-patronus-client","title":"Using Patronus Client","text":"<p>For more advanced batch evaluation needs, use the <code>Patronus</code> client:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nwith Patronus() as client:\n    # Run multiple evaluators in parallel\n    results = client.evaluate(\n        evaluators=[\n            RemoteEvaluator(\"judge\", \"patronus:is-helpful\"),\n            RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n        ],\n        task_input=\"What is quantum computing?\",\n        task_output=\"Quantum computing uses quantum bits or qubits to perform computations...\",\n        gold_answer=\"Computing that uses quantum phenomena like superposition and entanglement\"\n    )\n\n    # Check if all evaluations passed\n    if results.all_succeeded():\n        print(\"All evaluations passed!\")\n    else:\n        print(\"Some evaluations failed:\")\n        for failed in results.failed_evaluations():\n            print(f\"  - {failed.text_output}\")\n</code></pre> <p>The <code>Patronus</code> client provides:</p> <ul> <li>Parallel evaluation execution</li> <li>Connection pooling</li> <li>Error handling</li> <li>Result aggregation</li> </ul>"},{"location":"evaluations/batching/#asynchronous-evaluation","title":"Asynchronous Evaluation","text":"<p>For asynchronous workflows, use <code>AsyncPatronus</code>:</p> <pre><code>import asyncio\nfrom patronus import init\nfrom patronus.pat_client import AsyncPatronus\nfrom patronus.evals import AsyncRemoteEvaluator\n\ninit()\n\n\nasync def evaluate_responses():\n    async with AsyncPatronus() as client:\n        # Run evaluations asynchronously\n        results = await client.evaluate(\n            evaluators=[\n                AsyncRemoteEvaluator(\"judge\", \"patronus:is-helpful\"),\n                AsyncRemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n            ],\n            task_input=\"What is quantum computing?\",\n            task_output=\"Quantum computing uses quantum bits or qubits to perform computations...\",\n            gold_answer=\"Computing that uses quantum phenomena like superposition and entanglement\"\n        )\n\n        print(f\"Number of evaluations: {len(results.results)}\")\n        print(f\"All passed: {results.all_succeeded()}\")\n\n# Run the async function\nasyncio.run(evaluate_responses())\n</code></pre>"},{"location":"evaluations/batching/#background-evaluation","title":"Background Evaluation","text":"<p>For non-blocking evaluation, use the <code>evaluate_bg()</code> method:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nwith Patronus() as client:\n    # Start background evaluation\n    future = client.evaluate_bg(\n        evaluators=[\n            RemoteEvaluator(\"judge\", \"factual-accuracy\"),\n            RemoteEvaluator(\"judge\", \"patronus:helpfulness\")\n        ],\n        task_input=\"Explain how vaccines work.\",\n        task_output=\"Vaccines work by training the immune system to recognize and combat pathogens...\"\n    )\n\n    # Do other work while evaluation happens in background\n    print(\"Continuing with other tasks...\")\n\n    results = future.get()  # Blocks until complete\n\n    print(f\"Evaluation complete: {results.all_succeeded()}\")\n</code></pre> <p>The async version works similarly:</p> <pre><code>async with AsyncPatronus() as client:\n    # Start background evaluation\n    task = client.evaluate_bg(\n        evaluators=[...],\n        task_input=\"...\",\n        task_output=\"...\"\n    )\n\n    # Do other async work\n    await some_other_async_function()\n\n    # Get results when needed\n    results = await task\n</code></pre>"},{"location":"evaluations/batching/#working-with-evaluation-results","title":"Working with Evaluation Results","text":"<p>The <code>evaluate()</code> method returns an <code>EvaluationContainer</code> with several useful methods:</p> <pre><code>results = client.evaluate(evaluators=[...], task_input=\"...\", task_output=\"...\")\n\nif results.any_failed():\n    print(\"Some evaluations failed\")\n\nif results.all_succeeded():\n    print(\"All evaluations passed\")\n\nfor failed in results.failed_evaluations():\n    print(f\"Failed: {failed.text_output}\")\n\nfor success in results.succeeded_evaluations():\n    print(f\"Passed: {success.text_output}\")\n\nif results.has_exception():\n    results.raise_on_exception()  # Re-raise any exceptions that occurred\n</code></pre>"},{"location":"evaluations/batching/#example-comprehensive-quality-check","title":"Example: Comprehensive Quality Check","text":"<p>Here's a complete example of batch evaluation for content quality:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\ndef check_content_quality(question, answer):\n    with Patronus() as client:\n        results = client.evaluate(\n            evaluators=[\n                RemoteEvaluator(\"judge\", \"factual-accuracy\"),\n                RemoteEvaluator(\"judge\", \"helpfulness\"),\n                RemoteEvaluator(\"judge\", \"coherence\"),\n                RemoteEvaluator(\"judge\", \"grammar\"),\n                RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n            ],\n            task_input=question,\n            task_output=answer\n        )\n\n        if results.any_failed():\n            print(\"Content quality check failed\")\n            for failed in results.failed_evaluations():\n                print(f\"- Failed check: {failed.text_output}\")\n                print(f\"  Explanation: {failed.explanation}\")\n            return False\n\n        print(\"Content passed all quality checks\")\n        return True\n\ncheck_content_quality(\n    \"What is the capital of France?\",\n    \"The capital of France is Paris, which is located on the Seine River.\"\n)\n</code></pre>"},{"location":"evaluations/batching/#using-the-bundled_eval-context-manager","title":"Using the <code>bundled_eval()</code> Context Manager","text":"<p>The <code>bundled_eval()</code> is a lower-level context manager that groups multiple evaluations together based on their arguments. This is particularly useful when working with multiple user-defined evaluators that don't conform to the Patronus structured evaluator format.</p> <pre><code>import patronus\nfrom patronus.evals import bundled_eval, evaluator\n\npatronus.init()\n\n@evaluator()\ndef exact_match(actual, expected) -&gt; bool:\n    return actual == expected\n\n@evaluator()\ndef iexact_match(actual: str, expected: str) -&gt; bool:\n    return actual.strip().lower() == expected.strip().lower()\n\n# Group these evaluations together in a single trace and single log record\nwith bundled_eval():\n    exact_match(\"string\", \"string\")\n    iexact_match(\"string\", \"string\")\n</code></pre>"},{"location":"evaluations/evaluators/","title":"User-Defined Evaluators","text":"<p>Evaluators are the core building blocks of Patronus's evaluation system. This page covers how to create and use your own custom evaluators to assess LLM outputs according to your specific criteria.</p>"},{"location":"evaluations/evaluators/#creating-basic-evaluators","title":"Creating Basic Evaluators","text":"<p>The simplest way to create an evaluator is with the <code>@evaluator()</code> decorator:</p> <pre><code>from patronus import evaluator\n\n@evaluator()\ndef keyword_match(text: str, keywords: list[str]) -&gt; float:\n    \"\"\"\n    Evaluates whether the text contains the specified keywords.\n    Returns a score between 0.0 and 1.0 based on the percentage of matched keywords.\n    \"\"\"\n    matches = sum(keyword.lower() in text.lower() for keyword in keywords)\n    return matches / len(keywords) if keywords else 0.0\n</code></pre> <p>This decorator automatically: - Integrates with the Patronus tracing - Exports evaluation results to the Patronus Platform</p>"},{"location":"evaluations/evaluators/#flexible-input-and-output","title":"Flexible Input and Output","text":"<p>User-defined evaluators can accept any parameters and return several types of results:</p> <pre><code># Boolean evaluator (pass/fail)\n@evaluator()\ndef contains_answer(text: str, answer: str) -&gt; bool:\n    return answer.lower() in text.lower()\n\n\n# Numeric evaluator (score)\n@evaluator()\ndef semantic_similarity(text1: str, text2: str) -&gt; float:\n    # Simple example - in practice use proper semantic similarity\n    words1, words2 = set(text1.lower().split()), set(text2.lower().split())\n    intersection = words1.intersection(words2)\n    union = words1.union(words2)\n    return len(intersection) / len(union) if union else 0.0\n\n\n# String evaluator\n@evaluator()\ndef tone_classifier(text: str) -&gt; str:\n    positive = ['good', 'excellent', 'great', 'helpful']\n    negative = ['bad', 'poor', 'unhelpful', 'wrong']\n\n    pos_count = sum(word in text.lower() for word in positive)\n    neg_count = sum(word in text.lower() for word in negative)\n\n    if pos_count &gt; neg_count:\n        return \"positive\"\n    elif neg_count &gt; pos_count:\n        return \"negative\"\n    else:\n        return \"neutral\"\n</code></pre>"},{"location":"evaluations/evaluators/#return-types","title":"Return Types","text":"<p>Evaluators can return different types which are automatically converted to <code>EvaluationResult</code> objects:</p> <ul> <li>Boolean: <code>True</code>/<code>False</code> indicating pass/fail</li> <li>Float/Integer: Numerical scores (typically between 0-1)</li> <li>String: Text output categorizing the result</li> <li>EvaluationResult: Complete evaluation with scores, explanations, etc.</li> </ul>"},{"location":"evaluations/evaluators/#using-evaluationresult","title":"Using EvaluationResult","text":"<p>For more detailed evaluations, return an <code>EvaluationResult</code> object:</p> <pre><code>from patronus import evaluator\nfrom patronus.evals import EvaluationResult\n\n@evaluator()\ndef comprehensive_evaluation(response: str, reference: str) -&gt; EvaluationResult:\n    # Example implementation - replace with actual logic\n    has_keywords = all(word in response.lower() for word in [\"important\", \"key\", \"concept\"])\n    accuracy = 0.85  # Calculated accuracy score\n\n    return EvaluationResult(\n        score=accuracy,  # Numeric score (typically 0-1)\n        pass_=accuracy &gt;= 0.7,  # Boolean pass/fail\n        text_output=\"Satisfactory\" if accuracy &gt;= 0.7 else \"Needs improvement\",  # Category\n        explanation=f\"Response {'contains' if has_keywords else 'is missing'} key terms. Accuracy: {accuracy:.2f}\",\n        metadata={  # Additional structured data\n            \"has_required_keywords\": has_keywords,\n            \"response_length\": len(response),\n            \"accuracy\": accuracy\n        }\n    )\n</code></pre> <p>The <code>EvaluationResult</code> object can include:</p> <ul> <li>score: Numerical assessment (typically 0-1)</li> <li>pass_: Boolean pass/fail status</li> <li>text_output: Categorical or textual result</li> <li>explanation: Human-readable explanation of the result</li> <li>metadata: Additional structured data for analysis</li> <li>tags: Key-value pairs for filtering and organization</li> </ul>"},{"location":"evaluations/evaluators/#using-evaluators","title":"Using Evaluators","text":"<p>Once defined, evaluators can be used directly:</p> <pre><code># Use evaluators as normal function\nresult = keyword_match(\"The capital of France is Paris\", [\"capital\", \"France\", \"Paris\"])\nprint(f\"Score: {result}\")  # Output: Score: 1.0\n\n\n# Using class-based evaluator\nsafety_check = ContentSafetyEvaluator()\nresult = safety_check.evaluate(\n    task_output=\"This is a helpful and safe response.\"\n)\nprint(f\"Safety check passed: {result.pass_}\")  # Output: Safety check passed: True\n</code></pre>"},{"location":"evaluations/patronus-evaluators/","title":"Patronus Evaluators","text":"<p>Patronus provides a suite of evaluators that help you assess LLM outputs without writing complex evaluation logic. These managed evaluators run on Patronus infrastructure. Visit Patronus Platform console to define your own criteria.</p>"},{"location":"evaluations/patronus-evaluators/#using-patronus-evaluators","title":"Using Patronus Evaluators","text":"<p>You can use Patronus evaluators through the <code>RemoteEvaluator</code> class:</p> <pre><code>from patronus import init\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nfactual_accuracy = RemoteEvaluator(\"judge\", \"factual-accuracy\")\n\n# Evaluate an LLM output\nresult = factual_accuracy.evaluate(\n    task_input=\"What is the capital of France?\",\n    task_output=\"The capital of France is Paris, which is located on the Seine River.\",\n    gold_answer=\"Paris\"\n)\n\nprint(f\"Passed: {result.pass_}\")\nprint(f\"Score: {result.score}\")\nprint(f\"Explanation: {result.explanation}\")\n</code></pre>"},{"location":"evaluations/patronus-evaluators/#synchronous-and-asynchronous-versions","title":"Synchronous and Asynchronous Versions","text":"<p>Patronus evaluators are available in both synchronous and asynchronous versions:</p> <pre><code># Synchronous usage (as shown above)\nfactual_accuracy = RemoteEvaluator(\"judge\", \"factual-accuracy\")\nresult = factual_accuracy.evaluate(...)\n\n# Asynchronous usage\nfrom patronus.evals import AsyncRemoteEvaluator\n\nasync_factual_accuracy = AsyncRemoteEvaluator(\"judge\", \"factual-accuracy\")\nresult = await async_factual_accuracy.evaluate(...)\n</code></pre>"},{"location":"getting-started/initialization/","title":"Initialization","text":""},{"location":"getting-started/initialization/#api-key","title":"API Key","text":"<p>To use the Patronus SDK, you'll need an API key from the Patronus platform. If you don't have one yet:</p> <ol> <li>Sign up at https://app.patronus.ai</li> <li>Navigate to \"API Keys\"</li> <li>Create a new API key</li> </ol>"},{"location":"getting-started/initialization/#configuration","title":"Configuration","text":"<p>There are several ways to configure the Patronus SDK:</p>"},{"location":"getting-started/initialization/#environment-variables","title":"Environment Variables","text":"<p>Set your API key as an environment variable:</p> <pre><code>export PATRONUS_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"getting-started/initialization/#configuration-file","title":"Configuration File","text":"<p>Create a <code>patronus.yaml</code> file in your project directory:</p> <pre><code>api_key: \"your-api-key\"\nproject_name: \"Global\"\napp: \"default\"\n</code></pre>"},{"location":"getting-started/initialization/#direct-configuration","title":"Direct Configuration","text":"<p>Pass configuration values directly when initializing the SDK:</p> <pre><code>import patronus\n\npatronus.init(\n    api_key=\"your-api-key\",\n    project_name=\"Global\",\n    app=\"default\",\n)\n</code></pre>"},{"location":"getting-started/initialization/#verification","title":"Verification","text":"<p>To verify your installation and configuration:</p> <pre><code>import patronus\n\npatronus.init()\n\n# Create a simple tracer\n@patronus.traced()\ndef test_function():\n    return \"Installation successful!\"\n\n# Call the function to test tracing\nresult = test_function()\nprint(result)\n</code></pre> <p>If no errors occur, your Patronus SDK is correctly installed and configured.</p>"},{"location":"getting-started/initialization/#next-steps","title":"Next Steps","text":"<p>Now that you've installed the Patronus SDK, proceed to the Quickstart guide to learn how to use it effectively.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>The Patronus SDK provides tools for evaluating, monitoring, and improving LLM applications. This guide will help you install and set up the SDK in your environment.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>The recommended way to install the Patronus SDK is via pip:</p> <pre><code>pip install patronus\n</code></pre> <p>For a specific version:</p> <pre><code>pip install patronus==0.1.0\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next steps:","text":"<ul> <li>Initialization</li> <li>Quickstart</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with the Patronus SDK through three practical examples. We'll explore tracing, evaluation, and experimentation to give you a hands-on introduction to the core features.</p>"},{"location":"getting-started/quickstart/#initialization","title":"Initialization","text":"<p>Before running any of the examples, initialize the Patronus SDK:</p> <pre><code>import os\nimport patronus\n\n# Initialize with your API key\npatronus.init(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"PATRONUS_API_KEY\")\n)\n</code></pre> <p>You can also use a configuration file instead of direct initialization:</p> <pre><code># patronus.yaml\n\napi_key: \"your-api-key\"\nproject_name:  \"Global\"\napp: \"default\"\n</code></pre>"},{"location":"getting-started/quickstart/#example-1-tracing-with-a-functional-evaluator","title":"Example 1: Tracing with a Functional Evaluator","text":"<p>This example demonstrates how to trace function execution and create a simple functional evaluator.</p> <pre><code>import patronus\nfrom patronus import evaluator, traced\n\npatronus.init()\n\n@evaluator()\ndef exact_match(expected: str, actual: str) -&gt; bool:\n    return expected.strip() == actual.strip()\n\n@traced()\ndef process_query(query: str) -&gt; str:\n    # In a real application, this would call an LLM\n    return f\"Processed response for: {query}\"\n\n# Use the traced function and evaluator together\n@traced()\ndef main():\n    query = \"What is machine learning?\"\n    response = process_query(query)\n    print(f\"Response: {response}\")\n\n    expected_response = \"Processed response for: What is machine learning?\"\n    result = exact_match(expected_response, response)\n    print(f\"Evaluation result: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>In this example:</p> <ol> <li>We created a simple <code>exact_match</code> evaluator using the <code>@evaluator()</code> decorator</li> <li>We traced the <code>process_query</code> function using the <code>@traced()</code> decorator</li> <li>We ran an evaluation by calling the evaluator function directly</li> </ol> <p>The tracing will automatically capture execution details, timing, and results, making them available in the Patronus platform.</p>"},{"location":"getting-started/quickstart/#example-2-using-a-patronus-evaluator","title":"Example 2: Using a Patronus Evaluator","text":"<p>This example shows how to use a Patronus Evaluator to assess model outputs for hallucinations.</p> <pre><code>import patronus\nfrom patronus import traced\nfrom patronus.evals import RemoteEvaluator\n\npatronus.init()\n\n\n@traced()\ndef generate_insurance_response(query: str) -&gt; str:\n    # In a real application, this would call an LLM\n    return \"To even qualify for our car insurance policy, you need to have a valid driver's license that expires later than 2028.\"\n\n\n@traced(\"Quickstart: detect hallucination\")\ndef main():\n    check_hallucinates = RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n\n    context = \"\"\"\n    To qualify for our car insurance policy, you need a way to show competence\n    in driving which can be accomplished through a valid driver's license.\n    You must have multiple years of experience and cannot be graduating from driving school before or on 2028.\n    \"\"\"\n\n    query = \"What is the car insurance policy?\"\n    response = generate_insurance_response(query)\n    print(f\"Query: {query}\")\n    print(f\"Response: {response}\")\n\n    # Evaluate the response for hallucinations\n    resp = check_hallucinates.evaluate(\n        task_input=query,\n        task_context=context,\n        task_output=response\n    )\n\n    # Print the evaluation results\n    print(f\"\"\"\nHallucination evaluation:\nPassed: {resp.pass_}\nScore: {resp.score}\nExplanation: {resp.explanation}\n\"\"\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>In this example:</p> <ol> <li>We created a traced function generate_insurance_response to simulate an LLM response</li> <li>We used the Patronus Lynx Evaluator</li> <li>We evaluated whether the response contains information not supported by the context</li> <li>We displayed the detailed evaluation results</li> </ol> <p>Patronus Evaluators run on Patronus infrastructure and provide sophisticated assessment capabilities without requiring you to implement complex evaluation logic.</p>"},{"location":"getting-started/quickstart/#example-3-running-an-experiment-with-openai","title":"Example 3: Running an Experiment with OpenAI","text":"<p>This example demonstrates how to run a comprehensive experiment to evaluate OpenAI model performance across multiple samples and criteria.</p> <p>Before running Example 3, you'll need to install Pandas and the OpenAI SDK and OpenInference instrumentation:</p> <pre><code>pip install pandas openai openinference-instrumentation-openai\n</code></pre> <p>The OpenInference instrumentation automatically adds spans for all OpenAI API calls, capturing prompts, responses, and model parameters without any code changes. These details will appear in your Patronus traces for complete visibility into model interactions.</p> <pre><code>from typing import Optional\nimport os\n\nimport patronus\nfrom patronus.evals import evaluator, RemoteEvaluator, EvaluationResult\nfrom patronus.experiments import run_experiment, FuncEvaluatorAdapter, Row, TaskResult\nfrom openai import OpenAI\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\noai = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\npatronus.init()\n\n\n@evaluator()\ndef fuzzy_match(row: Row, task_result: TaskResult, **kwargs) -&gt; Optional[EvaluationResult]:\n    if not row.gold_answer or not task_result:\n        return None\n\n    gold_answer = row.gold_answer.lower()\n    response = task_result.output.lower()\n\n    key_terms = [term.strip() for term in gold_answer.split(',')]\n    matches = sum(1 for term in key_terms if term in response)\n    match_ratio = matches / len(key_terms) if key_terms else 0\n\n    # Return a score between 0-1 indicating match quality\n    return EvaluationResult(\n        pass_=match_ratio &gt; 0.7,\n        score=match_ratio,\n    )\n\n\ndef rag_task(row, **kwargs):\n    # In a real RAG system, this would retrieve context before calling the LLM\n    prompt = f\"\"\"\n    Based on the following context, answer the question.\n\n    Context:\n    {row.task_context}\n\n    Question: {row.task_input}\n\n    Answer:\n    \"\"\"\n\n    # Call OpenAI to generate a response\n    response = oai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\",\n             \"content\": \"You are a helpful assistant that answers questions based only on the provided context.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=150\n    )\n\n    return response.choices[0].message.content\n\n\ntest_data = [\n    {\n        \"task_input\": \"What is the main impact of climate change on coral reefs?\",\n        \"task_context\": \"\"\"\n        Climate change affects coral reefs through several mechanisms. Rising sea temperatures can cause coral bleaching,\n        where corals expel their symbiotic algae and turn white, often leading to death. Ocean acidification, caused by\n        increased CO2 absorption, makes it harder for corals to build their calcium carbonate structures. Sea level rise\n        can reduce light availability for photosynthesis. More frequent and intense storms damage reef structures. The\n        combination of these stressors is devastating to coral reef ecosystems worldwide.\n        \"\"\",\n        \"gold_answer\": \"coral bleaching, ocean acidification, reduced calcification, habitat destruction\"\n    },\n    {\n        \"task_input\": \"How do quantum computers differ from classical computers?\",\n        \"task_context\": \"\"\"\n        Classical computers process information in bits (0s and 1s), while quantum computers use quantum bits or qubits.\n        Qubits can exist in multiple states simultaneously thanks to superposition, allowing quantum computers to process\n        vast amounts of information in parallel. Quantum entanglement enables qubits to be correlated in ways impossible\n        for classical bits. While classical computers excel at everyday tasks, quantum computers potentially have advantages\n        for specific problems like cryptography, simulation of quantum systems, and certain optimization tasks. However,\n        quantum computers face significant challenges including qubit stability, error correction, and scaling up to useful sizes.\n        \"\"\",\n        \"gold_answer\": \"qubits instead of bits, superposition, entanglement, parallel processing\"\n    }\n]\n\nevaluators = [\n    FuncEvaluatorAdapter(fuzzy_match),\n    RemoteEvaluator(\"answer-relevance\", \"patronus:answer-relevance\")\n]\n\n# Run the experiment with OpenInference instrumentation\nprint(\"Running RAG evaluation experiment...\")\nexperiment = run_experiment(\n    dataset=test_data,\n    task=rag_task,\n    evaluators=evaluators,\n    tags={\"system\": \"rag-prototype\", \"model\": \"gpt-3.5-turbo\"},\n    integrations=[OpenAIInstrumentor()]\n)\n\n# Export results to CSV (optional)\n# experiment.to_csv(\"rag_evaluation_results.csv\")\n</code></pre> <p>In this example:</p> <ol> <li>We defined a task function <code>answer_questions</code> that generates responses for our experiment</li> <li>We created a custom evaluator <code>contains_key_information</code> to check for specific content</li> <li>We set up an experiment with multiple evaluators (both remote and custom)</li> <li>We ran the experiment across a dataset of questions</li> </ol> <p>Experiments provide a powerful way to systematically evaluate your LLM applications across multiple samples and criteria, helping you identify strengths and weaknesses in your models.</p>"},{"location":"observability/logging/","title":"Logging","text":"<p>Logging is an essential feature of the Patronus SDK that allows you to record events, debug information, and track the execution of your LLM applications. This page covers how to set up and use logging in your code.</p>"},{"location":"observability/logging/#getting-started-with-logging","title":"Getting Started with Logging","text":"<p>The Patronus SDK provides a simple logging interface that integrates with Python's standard logging module while also automatically exporting logs to the Patronus AI Platform:</p> <pre><code>import patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\n# Basic logging\nlog.info(\"Processing user query\")\n\n# Different log levels are available\nlog.debug(\"Detailed debug information\")\nlog.warning(\"Something might be wrong\")\nlog.error(\"An error occurred\")\nlog.critical(\"System cannot continue\")\n</code></pre>"},{"location":"observability/logging/#configuring-console-output","title":"Configuring Console Output","text":"<p>By default, Patronus logs are sent to the Patronus AI Platform but are not printed to the console. To display logs in your console output, you can add a standard Python logging handler:</p> <pre><code>import sys\nimport logging\nimport patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\n# Add a console handler to see logs in your terminal\nconsole_handler = logging.StreamHandler(sys.stdout)\nlog.addHandler(console_handler)\n\n# Now logs will appear in both console and Patronus Platform\nlog.info(\"This message appears in the console and is sent to Patronus\")\n</code></pre> <p>You can also customize the format of console logs:</p> <pre><code>import sys\nimport logging\nimport patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\nformatter = logging.Formatter('[%(asctime)s] %(levelname)-8s: %(message)s')\n\nconsole_handler = logging.StreamHandler(sys.stdout)\nconsole_handler.setFormatter(formatter)\nlog.addHandler(console_handler)\n\n# Logs will now include timestamp and level\nlog.info(\"Formatted log message\")\n</code></pre>"},{"location":"observability/logging/#advanced-configuration","title":"Advanced Configuration","text":"<p>Patronus integrates with Python's logging module, allowing for advanced configuration options. The SDK uses two main loggers:</p> <ul> <li><code>patronus.sdk</code> - For client-emitted messages that are automatically exported to the Patronus AI Platform</li> <li><code>patronus.core</code> - For library-emitted messages related to the SDK's internal operations</li> </ul> <p>Here's how to configure these loggers using standard library methods:</p> <pre><code>import logging\nimport patronus\n\n# Initialize Patronus before configuring logging\npatronus.init()\n\n# Configure the root Patronus logger\npatronus_root_logger = logging.getLogger(\"patronus\")\npatronus_root_logger.setLevel(logging.WARNING)  # Set base level for all Patronus loggers\n\n# Add a console handler with custom formatting\nconsole_handler = logging.StreamHandler()\nformatter = logging.Formatter(\n    fmt='[%(asctime)s] %(levelname)-8s %(name)s: %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nconsole_handler.setFormatter(formatter)\npatronus_root_logger.addHandler(console_handler)\n\n# Configure specific loggers\npatronus_core_logger = logging.getLogger(\"patronus.core\")\npatronus_core_logger.setLevel(logging.WARNING)  # Only show warnings and above for internal SDK messages\n\npatronus_sdk_logger = logging.getLogger(\"patronus.sdk\")\npatronus_sdk_logger.setLevel(logging.INFO)  # Show info and above for your application logs\n</code></pre>"},{"location":"observability/logging/#logging-with-traces","title":"Logging with Traces","text":"<p>Patronus logging integrates seamlessly with the tracing system, allowing you to correlate logs with specific spans in your application flow:</p> <pre><code>import patronus\nfrom patronus import traced, start_span\n\npatronus.init()\nlog = patronus.get_logger()\n\n@traced()\ndef process_user_query(query):\n    log.info(\"Processing query\")\n\n    with start_span(\"Query Analysis\"):\n        log.info(\"Analyzing query intent\")\n        ...\n\n    with start_span(\"Response Generation\"):\n        log.info(\"Generating LLM response\")\n        ...\n\n    return \"Response to: \" + query\n\n# Logs will be associated with the appropriate spans\nresult = process_user_query(\"Tell me about machine learning\")\n</code></pre>"},{"location":"observability/tracing/","title":"Tracing","text":"<p>Tracing is a core feature of the Patronus SDK that allows you to monitor and understand the behavior of your LLM applications. This page covers how to set up and use tracing in your code.</p>"},{"location":"observability/tracing/#getting-started-with-tracing","title":"Getting Started with Tracing","text":"<p>Tracing in Patronus works through two main mechanisms:</p> <ol> <li>Function decorators: Easily trace entire functions</li> <li>Context managers: Trace specific code blocks within functions</li> </ol>"},{"location":"observability/tracing/#using-the-traced-decorator","title":"Using the <code>@traced()</code> Decorator","text":"<p>The simplest way to add tracing is with the <code>@traced()</code> decorator:</p> <pre><code>import patronus\nfrom patronus import traced\n\npatronus.init()\n\n@traced()\ndef generate_response(prompt: str) -&gt; str:\n    # Your LLM call or processing logic here\n    return f\"Response to: {prompt}\"\n\n# Call the traced function\nresult = generate_response(\"Tell me about machine learning\")\n</code></pre>"},{"location":"observability/tracing/#decorator-options","title":"Decorator Options","text":"<p>The <code>@traced()</code> decorator accepts several parameters for customization:</p> <pre><code>@traced(\n    span_name=\"Custom span name\",   # Default: function name\n    log_args=True,                  # Whether to log function arguments\n    log_results=True,               # Whether to log function return values\n    log_exceptions=True,            # Whether to log exceptions\n    disable_log=False,              # Completely disable logging (maintains spans)\n    attributes={\"key\": \"value\"}     # Custom attributes to add to the span\n)\ndef my_function():\n    pass\n</code></pre> <p>See the API Reference for complete details.</p>"},{"location":"observability/tracing/#using-the-start_span-context-manager","title":"Using the <code>start_span()</code> Context Manager","text":"<p>For more granular control, use the <code>start_span()</code> context manager to trace specific blocks of code:</p> <pre><code>import patronus\nfrom patronus.tracing import start_span\n\npatronus.init()\n\ndef complex_workflow(data):\n    # First phase\n    with start_span(\"Data preparation\", attributes={\"data_size\": len(data)}):\n        prepared_data = preprocess(data)\n\n    # Second phase\n    with start_span(\"Model inference\"):\n        results = run_model(prepared_data)\n\n    # Third phase\n    with start_span(\"Post-processing\"):\n        final_results = postprocess(results)\n\n    return final_results\n</code></pre>"},{"location":"observability/tracing/#context-manager-options","title":"Context Manager Options","text":"<p>The <code>start_span()</code> context manager accepts these parameters:</p> <pre><code>with start_span(\n    \"Span name\",                        # Name of the span (required)\n    record_exception=False,             # Whether to record exceptions\n    attributes={\"custom\": \"attribute\"}  # Custom attributes to add\n) as span:\n    # Your code here\n    # You can also add attributes during execution:\n    span.set_attribute(\"dynamic_value\", 42)\n</code></pre> <p>See the API Reference for complete details.</p>"},{"location":"observability/tracing/#custom-attributes","title":"Custom Attributes","text":"<p>Both tracing methods allow you to add custom attributes that provide additional context for your traces:</p> <pre><code>@traced(attributes={\n    \"model\": \"gpt-4\",\n    \"version\": \"1.0\",\n    \"temperature\": 0.7\n})\ndef generate_with_gpt4(prompt):\n    # Function implementation\n    pass\n\n# Or with context manager\nwith start_span(\"Query processing\", attributes={\n    \"query_type\": \"search\",\n    \"filters_applied\": True,\n    \"result_limit\": 10\n}):\n    # Processing code\n    pass\n</code></pre>"}]}