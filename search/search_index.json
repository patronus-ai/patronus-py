{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Patronus Python SDK Documentation","text":"<p>The Patronus SDK provides tools for observability, evaluation, and experimentation with Large Language Models (LLMs), helping you build reliable and high-quality AI applications.</p> <p>This documentation covers only the Python SDK. For comprehensive documentation on the Patronus platform, evaluators, and best practices, please visit the official Patronus documentation.</p>"},{"location":"#llmstxt","title":"llms.txt","text":"<p>The Patronus Python SDK documentation is available in the llms.txt format. This format is defined in Markdown and suited for large language models.</p> <p>Two formats are available:</p> <ul> <li>llms.txt: a file containing a brief description of the SDK, along with links to the different sections of the documentation. The structure of this file is described in detail here.</li> <li>llms-full.txt: Similar to the <code>llms.txt</code> file, but every link content is included. Note that this file may be too large for some LLMs.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation and Quickstart - Install the SDK and run your first evaluation</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>The Patronus Experimentation Framework offers several configuration options that can be set in the following ways:</p> <ol> <li>Through function parameters (in code)</li> <li>Environment variables</li> <li>YAML configuration file</li> </ol> <p>Configuration options are prioritized in the order listed above, meaning that if a configuration value is provided through function parameters, it will override values from environment variables or the YAML file.</p>"},{"location":"configuration/#configuration-options","title":"Configuration Options","text":"Config name Environment Variable Default Value service PATRONUS_SERVICE Defaults to value retrieved from <code>OTEL_SERVICE_NAME</code> env var or <code>platform.node()</code>. project_name PATRONUS_PROJECT_NAME <code>Global</code> app PATRONUS_APP <code>default</code> api_key PATRONUS_API_KEY api_url PATRONUS_API_URL <code>https://api.patronus.ai</code> ui_url PATRONUS_UI_URL <code>https://app.patronus.ai</code> otel_endpoint PATRONUS_OTEL_ENDPOINT <code>https://otel.patronus.ai:4317</code> otel_exporter_otlp_protocol PATRONUS_OTEL_EXPORTER_OTLP_PROTOCOL Falls back to OTEL env vars, defaults to <code>grpc</code> timeout_s PATRONUS_TIMEOUT_S <code>300</code> prompt_templating_engine PATRONUS_PROMPT_TEMPLATING_ENGINE <code>f-string</code> prompt_providers PATRONUS_PROMPT_PROVIDERS <code>[\"local\", \"api\"]</code> resource_dir PATRONUS_RESOURCE_DIR <code>./patronus</code>"},{"location":"configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"configuration/#1-function-parameters","title":"1. Function Parameters","text":"<p>You can provide configuration options directly through function parameters when calling key Patronus functions.</p>"},{"location":"configuration/#using-init","title":"Using <code>init()</code>","text":"<p>Use the <code>init()</code> function when you need to set up the Patronus SDK for evaluations, logging, and tracing outside of experiments. This initializes the global context used by the SDK.</p> <pre><code>import patronus\n\n# Initialize with specific configuration\npatronus.init(\n    project_name=\"my-project\",\n    app=\"recommendation-service\",\n    api_key=\"your-api-key\",\n    api_url=\"https://api.patronus.ai\",\n    service=\"my-service\",\n    prompt_templating_engine=\"mustache\"\n)\n</code></pre>"},{"location":"configuration/#using-run_experiment-or-experimentcreate","title":"Using <code>run_experiment()</code> or <code>Experiment.create()</code>","text":"<p>Use these functions when running experiments. They handle their own initialization, so you don't need to call <code>init()</code> separately. Experiments create their own context scoped to the experiment.</p> <pre><code>from patronus import run_experiment\n\n# Run experiment with specific configuration\nexperiment = run_experiment(\n    dataset=my_dataset,\n    task=my_task,\n    evaluators=[my_evaluator],\n    project_name=\"my-project\",\n    api_key=\"your-api-key\",\n    service=\"my-service\"\n)\n</code></pre>"},{"location":"configuration/#2-environment-variables","title":"2. Environment Variables","text":"<p>You can set configuration options using environment variables with the prefix <code>PATRONUS_</code>:</p> <pre><code>export PATRONUS_API_KEY=\"your-api-key\"\nexport PATRONUS_PROJECT_NAME=\"my-project\"\nexport PATRONUS_SERVICE=\"my-service\"\n</code></pre>"},{"location":"configuration/#3-yaml-configuration-file-patronusyaml","title":"3. YAML Configuration File (<code>patronus.yaml</code>)","text":"<p>You can also provide configuration options using a <code>patronus.yaml</code> file. This file must be present in the working directory when executing your script.</p> <pre><code>service: \"my-service\"\nproject_name: \"my-project\"\napp: \"my-agent\"\n\napi_key: \"YOUR_API_KEY\"\napi_url: \"https://api.patronus.ai\"\nui_url: \"https://app.patronus.ai\"\notel_endpoint: \"https://otel.patronus.ai:4317\"\notel_exporter_otlp_protocol: \"grpc\"  # or \"http/protobuf\"\ntimeout_s: 300\n\n# Prompt management configuration\nprompt_templating_engine: \"mustache\"\nprompt_providers: [ \"local\", \"api\" ]\nresource_dir: \"./my-resources\"\n</code></pre>"},{"location":"configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>When determining the value for a configuration option, Patronus follows this order of precedence:</p> <ol> <li>Function parameter values (highest priority)</li> <li>Environment variables</li> <li>YAML configuration file</li> <li>Default values (lowest priority)</li> </ol> <p>For example, if you provide <code>project_name</code> as a function parameter and also have it defined in your environment variables and YAML file, the function parameter value will be used.</p>"},{"location":"configuration/#programmatic-configuration-access","title":"Programmatic Configuration Access","text":"<p>For more advanced use cases, you can directly access the configuration system through the <code>Config</code> class and the <code>config()</code> function:</p> <pre><code>from patronus.config import config\n\n# Access the configuration singleton\ncfg = config()\n\n# Read configuration values\napi_key = cfg.api_key\nproject_name = cfg.project_name\n\n# Check for specific conditions\nif cfg.api_url != \"https://api.patronus.ai\":\n    print(\"Using custom API endpoint\")\n</code></pre> <p>This approach is particularly useful when you need to inspect or log the current configuration state.</p>"},{"location":"configuration/#observability-configuration","title":"Observability Configuration","text":"<p>For detailed information about configuring observability features like tracing and logging, including exporter protocol selection and endpoint configuration, see the Observability Configuration guide.</p>"},{"location":"api_ref/api_client/","title":"API","text":""},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient","title":"patronus.api.api_client.PatronusAPIClient","text":"<pre><code>PatronusAPIClient(*, client_http_async: AsyncClient, client_http: Client, base_url: str, api_key: str)\n</code></pre> <p>               Bases: <code>BaseAPIClient</code></p> Source code in <code>src/patronus/api/api_client_base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    client_http_async: httpx.AsyncClient,\n    client_http: httpx.Client,\n    base_url: str,\n    api_key: str,\n):\n    self.version = importlib.metadata.version(\"patronus\")\n    self.http = client_http_async\n    self.http_sync = client_http\n    self.base_url = base_url.rstrip(\"/\")\n    self.api_key = api_key\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.add_evaluator_criteria_revision","title":"add_evaluator_criteria_revision  <code>async</code>","text":"<pre><code>add_evaluator_criteria_revision(evaluator_criteria_id, request: AddEvaluatorCriteriaRevisionRequest) -&gt; api_types.AddEvaluatorCriteriaRevisionResponse\n</code></pre> <p>Adds a revision to existing evaluator criteria.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def add_evaluator_criteria_revision(\n    self,\n    evaluator_criteria_id,\n    request: api_types.AddEvaluatorCriteriaRevisionRequest,\n) -&gt; api_types.AddEvaluatorCriteriaRevisionResponse:\n    \"\"\"Adds a revision to existing evaluator criteria.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        f\"/v1/evaluator-criteria/{evaluator_criteria_id}/revision\",\n        body=request,\n        response_cls=api_types.AddEvaluatorCriteriaRevisionResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.add_evaluator_criteria_revision_sync","title":"add_evaluator_criteria_revision_sync","text":"<pre><code>add_evaluator_criteria_revision_sync(evaluator_criteria_id, request: AddEvaluatorCriteriaRevisionRequest) -&gt; api_types.AddEvaluatorCriteriaRevisionResponse\n</code></pre> <p>Adds a revision to existing evaluator criteria.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def add_evaluator_criteria_revision_sync(\n    self,\n    evaluator_criteria_id,\n    request: api_types.AddEvaluatorCriteriaRevisionRequest,\n) -&gt; api_types.AddEvaluatorCriteriaRevisionResponse:\n    \"\"\"Adds a revision to existing evaluator criteria.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        f\"/v1/evaluator-criteria/{evaluator_criteria_id}/revision\",\n        body=request,\n        response_cls=api_types.AddEvaluatorCriteriaRevisionResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.annotate","title":"annotate  <code>async</code>","text":"<pre><code>annotate(request: AnnotateRequest) -&gt; api_types.AnnotateResponse\n</code></pre> <p>Annotates log based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def annotate(self, request: api_types.AnnotateRequest) -&gt; api_types.AnnotateResponse:\n    \"\"\"Annotates log based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/annotate\",\n        body=request,\n        response_cls=api_types.AnnotateResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.annotate_sync","title":"annotate_sync","text":"<pre><code>annotate_sync(request: AnnotateRequest) -&gt; api_types.AnnotateResponse\n</code></pre> <p>Annotates log based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def annotate_sync(self, request: api_types.AnnotateRequest) -&gt; api_types.AnnotateResponse:\n    \"\"\"Annotates log based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/annotate\",\n        body=request,\n        response_cls=api_types.AnnotateResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.batch_create_evaluations","title":"batch_create_evaluations  <code>async</code>","text":"<pre><code>batch_create_evaluations(request: BatchCreateEvaluationsRequest) -&gt; api_types.BatchCreateEvaluationsResponse\n</code></pre> <p>Creates multiple evaluations in a single request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def batch_create_evaluations(\n    self, request: api_types.BatchCreateEvaluationsRequest\n) -&gt; api_types.BatchCreateEvaluationsResponse:\n    \"\"\"Creates multiple evaluations in a single request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluations/batch\",\n        body=request,\n        response_cls=api_types.BatchCreateEvaluationsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.batch_create_evaluations_sync","title":"batch_create_evaluations_sync","text":"<pre><code>batch_create_evaluations_sync(request: BatchCreateEvaluationsRequest) -&gt; api_types.BatchCreateEvaluationsResponse\n</code></pre> <p>Creates multiple evaluations in a single request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def batch_create_evaluations_sync(\n    self, request: api_types.BatchCreateEvaluationsRequest\n) -&gt; api_types.BatchCreateEvaluationsResponse:\n    \"\"\"Creates multiple evaluations in a single request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluations/batch\",\n        body=request,\n        response_cls=api_types.BatchCreateEvaluationsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_annotation_criteria","title":"create_annotation_criteria  <code>async</code>","text":"<pre><code>create_annotation_criteria(request: CreateAnnotationCriteriaRequest) -&gt; api_types.CreateAnnotationCriteriaResponse\n</code></pre> <p>Creates annotation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def create_annotation_criteria(\n    self, request: api_types.CreateAnnotationCriteriaRequest\n) -&gt; api_types.CreateAnnotationCriteriaResponse:\n    \"\"\"Creates annotation criteria based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/annotation-criteria\",\n        body=request,\n        response_cls=api_types.CreateAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_annotation_criteria_sync","title":"create_annotation_criteria_sync","text":"<pre><code>create_annotation_criteria_sync(request: CreateAnnotationCriteriaRequest) -&gt; api_types.CreateAnnotationCriteriaResponse\n</code></pre> <p>Creates annotation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def create_annotation_criteria_sync(\n    self, request: api_types.CreateAnnotationCriteriaRequest\n) -&gt; api_types.CreateAnnotationCriteriaResponse:\n    \"\"\"Creates annotation criteria based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/annotation-criteria\",\n        body=request,\n        response_cls=api_types.CreateAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_criteria","title":"create_criteria  <code>async</code>","text":"<pre><code>create_criteria(request: CreateCriteriaRequest) -&gt; api_types.CreateCriteriaResponse\n</code></pre> <p>Creates evaluation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def create_criteria(self, request: api_types.CreateCriteriaRequest) -&gt; api_types.CreateCriteriaResponse:\n    \"\"\"Creates evaluation criteria based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluator-criteria\",\n        body=request,\n        response_cls=api_types.CreateCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_criteria_sync","title":"create_criteria_sync","text":"<pre><code>create_criteria_sync(request: CreateCriteriaRequest) -&gt; api_types.CreateCriteriaResponse\n</code></pre> <p>Creates evaluation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def create_criteria_sync(self, request: api_types.CreateCriteriaRequest) -&gt; api_types.CreateCriteriaResponse:\n    \"\"\"Creates evaluation criteria based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluator-criteria\",\n        body=request,\n        response_cls=api_types.CreateCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_experiment","title":"create_experiment  <code>async</code>","text":"<pre><code>create_experiment(request: CreateExperimentRequest) -&gt; api_types.Experiment\n</code></pre> <p>Creates a new experiment based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def create_experiment(self, request: api_types.CreateExperimentRequest) -&gt; api_types.Experiment:\n    \"\"\"Creates a new experiment based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/experiments\",\n        body=request,\n        response_cls=api_types.CreateExperimentResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_experiment_sync","title":"create_experiment_sync","text":"<pre><code>create_experiment_sync(request: CreateExperimentRequest) -&gt; api_types.Experiment\n</code></pre> <p>Creates a new experiment based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def create_experiment_sync(self, request: api_types.CreateExperimentRequest) -&gt; api_types.Experiment:\n    \"\"\"Creates a new experiment based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/experiments\",\n        body=request,\n        response_cls=api_types.CreateExperimentResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_project","title":"create_project  <code>async</code>","text":"<pre><code>create_project(request: CreateProjectRequest) -&gt; api_types.Project\n</code></pre> <p>Creates a new project based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def create_project(self, request: api_types.CreateProjectRequest) -&gt; api_types.Project:\n    \"\"\"Creates a new project based on the given request.\"\"\"\n    resp = await self.call(\"POST\", \"/v1/projects\", body=request, response_cls=api_types.Project)\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.create_project_sync","title":"create_project_sync","text":"<pre><code>create_project_sync(request: CreateProjectRequest) -&gt; api_types.Project\n</code></pre> <p>Creates a new project based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def create_project_sync(self, request: api_types.CreateProjectRequest) -&gt; api_types.Project:\n    \"\"\"Creates a new project based on the given request.\"\"\"\n    resp = self.call_sync(\"POST\", \"/v1/projects\", body=request, response_cls=api_types.Project)\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.delete_annotation_criteria","title":"delete_annotation_criteria  <code>async</code>","text":"<pre><code>delete_annotation_criteria(criteria_id: str) -&gt; None\n</code></pre> <p>Deletes annotation criteria by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def delete_annotation_criteria(self, criteria_id: str) -&gt; None:\n    \"\"\"Deletes annotation criteria by its ID.\"\"\"\n    resp = await self.call(\n        \"DELETE\",\n        f\"/v1/annotation-criteria/{criteria_id}\",\n        response_cls=None,\n    )\n    resp.raise_for_status()\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.delete_annotation_criteria_sync","title":"delete_annotation_criteria_sync","text":"<pre><code>delete_annotation_criteria_sync(criteria_id: str) -&gt; None\n</code></pre> <p>Deletes annotation criteria by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def delete_annotation_criteria_sync(self, criteria_id: str) -&gt; None:\n    \"\"\"Deletes annotation criteria by its ID.\"\"\"\n    resp = self.call_sync(\n        \"DELETE\",\n        f\"/v1/annotation-criteria/{criteria_id}\",\n        response_cls=None,\n    )\n    resp.raise_for_status()\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(request: EvaluateRequest) -&gt; api_types.EvaluateResponse\n</code></pre> <p>Evaluates content using the specified evaluators.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def evaluate(self, request: api_types.EvaluateRequest) -&gt; api_types.EvaluateResponse:\n    \"\"\"Evaluates content using the specified evaluators.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluate\",\n        body=request,\n        response_cls=api_types.EvaluateResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.evaluate_one","title":"evaluate_one  <code>async</code>","text":"<pre><code>evaluate_one(request: EvaluateRequest) -&gt; api_types.EvaluationResult\n</code></pre> <p>Evaluates content using a single evaluator.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def evaluate_one(self, request: api_types.EvaluateRequest) -&gt; api_types.EvaluationResult:\n    \"\"\"Evaluates content using a single evaluator.\"\"\"\n    if len(request.evaluators) &gt; 1:\n        raise ValueError(\"'evaluate_one()' cannot accept more than one evaluator in the request body\")\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluate\",\n        body=request,\n        response_cls=api_types.EvaluateResponse,\n    )\n    return self._evaluate_one_process_resp(resp)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.evaluate_one_sync","title":"evaluate_one_sync","text":"<pre><code>evaluate_one_sync(request: EvaluateRequest) -&gt; api_types.EvaluationResult\n</code></pre> <p>Evaluates content using a single evaluator.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def evaluate_one_sync(self, request: api_types.EvaluateRequest) -&gt; api_types.EvaluationResult:\n    \"\"\"Evaluates content using a single evaluator.\"\"\"\n    if len(request.evaluators) &gt; 1:\n        raise ValueError(\"'evaluate_one_sync()' cannot accept more than one evaluator in the request body\")\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluate\",\n        body=request,\n        response_cls=api_types.EvaluateResponse,\n    )\n    return self._evaluate_one_process_resp(resp)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.evaluate_sync","title":"evaluate_sync","text":"<pre><code>evaluate_sync(request: EvaluateRequest) -&gt; api_types.EvaluateResponse\n</code></pre> <p>Evaluates content using the specified evaluators.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def evaluate_sync(self, request: api_types.EvaluateRequest) -&gt; api_types.EvaluateResponse:\n    \"\"\"Evaluates content using the specified evaluators.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluate\",\n        body=request,\n        response_cls=api_types.EvaluateResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.export_evaluations","title":"export_evaluations  <code>async</code>","text":"<pre><code>export_evaluations(request: ExportEvaluationRequest) -&gt; api_types.ExportEvaluationResponse\n</code></pre> <p>Exports evaluations based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def export_evaluations(\n    self, request: api_types.ExportEvaluationRequest\n) -&gt; api_types.ExportEvaluationResponse:\n    \"\"\"Exports evaluations based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluation-results/batch\",\n        body=request,\n        response_cls=api_types.ExportEvaluationResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.export_evaluations_sync","title":"export_evaluations_sync","text":"<pre><code>export_evaluations_sync(request: ExportEvaluationRequest) -&gt; api_types.ExportEvaluationResponse\n</code></pre> <p>Exports evaluations based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def export_evaluations_sync(self, request: api_types.ExportEvaluationRequest) -&gt; api_types.ExportEvaluationResponse:\n    \"\"\"Exports evaluations based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluation-results/batch\",\n        body=request,\n        response_cls=api_types.ExportEvaluationResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.get_experiment","title":"get_experiment  <code>async</code>","text":"<pre><code>get_experiment(experiment_id: str) -&gt; Optional[api_types.Experiment]\n</code></pre> <p>Fetches an experiment by its ID or returns None if not found.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def get_experiment(self, experiment_id: str) -&gt; Optional[api_types.Experiment]:\n    \"\"\"Fetches an experiment by its ID or returns None if not found.\"\"\"\n    resp = await self.call(\n        \"GET\",\n        f\"/v1/experiments/{experiment_id}\",\n        response_cls=api_types.GetExperimentResponse,\n    )\n    if resp.response.status_code == 404:\n        return None\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.get_experiment_sync","title":"get_experiment_sync","text":"<pre><code>get_experiment_sync(experiment_id: str) -&gt; Optional[api_types.Experiment]\n</code></pre> <p>Fetches an experiment by its ID or returns None if not found.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def get_experiment_sync(self, experiment_id: str) -&gt; Optional[api_types.Experiment]:\n    \"\"\"Fetches an experiment by its ID or returns None if not found.\"\"\"\n    resp = self.call_sync(\n        \"GET\",\n        f\"/v1/experiments/{experiment_id}\",\n        response_cls=api_types.GetExperimentResponse,\n    )\n    if resp.response.status_code == 404:\n        return None\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.get_project","title":"get_project  <code>async</code>","text":"<pre><code>get_project(project_id: str) -&gt; api_types.Project\n</code></pre> <p>Fetches a project by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def get_project(self, project_id: str) -&gt; api_types.Project:\n    \"\"\"Fetches a project by its ID.\"\"\"\n    resp = await self.call(\n        \"GET\",\n        f\"/v1/projects/{project_id}\",\n        response_cls=api_types.GetProjectResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.project\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.get_project_sync","title":"get_project_sync","text":"<pre><code>get_project_sync(project_id: str) -&gt; api_types.Project\n</code></pre> <p>Fetches a project by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def get_project_sync(self, project_id: str) -&gt; api_types.Project:\n    \"\"\"Fetches a project by its ID.\"\"\"\n    resp = self.call_sync(\n        \"GET\",\n        f\"/v1/projects/{project_id}\",\n        response_cls=api_types.GetProjectResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.project\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_annotation_criteria","title":"list_annotation_criteria  <code>async</code>","text":"<pre><code>list_annotation_criteria(*, project_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None) -&gt; api_types.ListAnnotationCriteriaResponse\n</code></pre> <p>Retrieves a list of annotation criteria with optional filtering.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_annotation_criteria(\n    self, *, project_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None\n) -&gt; api_types.ListAnnotationCriteriaResponse:\n    \"\"\"Retrieves a list of annotation criteria with optional filtering.\"\"\"\n    params = {}\n    if project_id is not None:\n        params[\"project_id\"] = project_id\n    if limit is not None:\n        params[\"limit\"] = limit\n    if offset is not None:\n        params[\"offset\"] = offset\n    resp = await self.call(\n        \"GET\",\n        \"/v1/annotation-criteria\",\n        params=params,\n        response_cls=api_types.ListAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_annotation_criteria_sync","title":"list_annotation_criteria_sync","text":"<pre><code>list_annotation_criteria_sync(*, project_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None) -&gt; api_types.ListAnnotationCriteriaResponse\n</code></pre> <p>Retrieves a list of annotation criteria with optional filtering.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_annotation_criteria_sync(\n    self, *, project_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None\n) -&gt; api_types.ListAnnotationCriteriaResponse:\n    \"\"\"Retrieves a list of annotation criteria with optional filtering.\"\"\"\n    params = {}\n    if project_id is not None:\n        params[\"project_id\"] = project_id\n    if limit is not None:\n        params[\"limit\"] = limit\n    if offset is not None:\n        params[\"offset\"] = offset\n    resp = self.call_sync(\n        \"GET\",\n        \"/v1/annotation-criteria\",\n        params=params,\n        response_cls=api_types.ListAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_criteria","title":"list_criteria  <code>async</code>","text":"<pre><code>list_criteria(request: ListCriteriaRequest) -&gt; api_types.ListCriteriaResponse\n</code></pre> <p>Retrieves a list of evaluation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_criteria(self, request: api_types.ListCriteriaRequest) -&gt; api_types.ListCriteriaResponse:\n    \"\"\"Retrieves a list of evaluation criteria based on the given request.\"\"\"\n    params = request.model_dump(exclude_none=True)\n    resp = await self.call(\n        \"GET\",\n        \"/v1/evaluator-criteria\",\n        params=params,\n        response_cls=api_types.ListCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_criteria_sync","title":"list_criteria_sync","text":"<pre><code>list_criteria_sync(request: ListCriteriaRequest) -&gt; api_types.ListCriteriaResponse\n</code></pre> <p>Retrieves a list of evaluation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_criteria_sync(self, request: api_types.ListCriteriaRequest) -&gt; api_types.ListCriteriaResponse:\n    \"\"\"Retrieves a list of evaluation criteria based on the given request.\"\"\"\n    params = request.model_dump(exclude_none=True)\n    resp = self.call_sync(\n        \"GET\",\n        \"/v1/evaluator-criteria\",\n        params=params,\n        response_cls=api_types.ListCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_dataset_data","title":"list_dataset_data  <code>async</code>","text":"<pre><code>list_dataset_data(dataset_id: str) -&gt; api_types.ListDatasetData\n</code></pre> <p>Retrieves data from a dataset by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_dataset_data(self, dataset_id: str) -&gt; api_types.ListDatasetData:\n    \"\"\"Retrieves data from a dataset by its ID.\"\"\"\n    resp = await self.call(\n        \"GET\",\n        f\"/v1/datasets/{dataset_id}/data\",\n        response_cls=api_types.ListDatasetData,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_dataset_data_sync","title":"list_dataset_data_sync","text":"<pre><code>list_dataset_data_sync(dataset_id: str) -&gt; api_types.ListDatasetData\n</code></pre> <p>Retrieves data from a dataset by its ID.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_dataset_data_sync(self, dataset_id: str) -&gt; api_types.ListDatasetData:\n    \"\"\"Retrieves data from a dataset by its ID.\"\"\"\n    resp = self.call_sync(\n        \"GET\",\n        f\"/v1/datasets/{dataset_id}/data\",\n        response_cls=api_types.ListDatasetData,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_datasets","title":"list_datasets  <code>async</code>","text":"<pre><code>list_datasets(dataset_type: Optional[str] = None) -&gt; list[api_types.Dataset]\n</code></pre> <p>Retrieves a list of datasets, optionally filtered by type.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_datasets(self, dataset_type: Optional[str] = None) -&gt; list[api_types.Dataset]:\n    \"\"\"\n    Retrieves a list of datasets, optionally filtered by type.\n    \"\"\"\n    params = {}\n    if dataset_type is not None:\n        params[\"type\"] = dataset_type\n\n    resp = await self.call(\n        \"GET\",\n        \"/v1/datasets\",\n        params=params,\n        response_cls=api_types.ListDatasetsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.datasets\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_datasets_sync","title":"list_datasets_sync","text":"<pre><code>list_datasets_sync(dataset_type: Optional[str] = None) -&gt; list[api_types.Dataset]\n</code></pre> <p>Retrieves a list of datasets, optionally filtered by type.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_datasets_sync(self, dataset_type: Optional[str] = None) -&gt; list[api_types.Dataset]:\n    \"\"\"\n    Retrieves a list of datasets, optionally filtered by type.\n    \"\"\"\n    params = {}\n    if dataset_type is not None:\n        params[\"type\"] = dataset_type\n\n    resp = self.call_sync(\n        \"GET\",\n        \"/v1/datasets\",\n        params=params,\n        response_cls=api_types.ListDatasetsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.datasets\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_evaluators","title":"list_evaluators  <code>async</code>","text":"<pre><code>list_evaluators(by_alias_or_id: Optional[str] = None) -&gt; list[api_types.Evaluator]\n</code></pre> <p>Retrieves a list of available evaluators.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def list_evaluators(self, by_alias_or_id: Optional[str] = None) -&gt; list[api_types.Evaluator]:\n    \"\"\"Retrieves a list of available evaluators.\"\"\"\n    params = {}\n    if by_alias_or_id:\n        params[\"by_alias_or_id\"] = by_alias_or_id\n\n    resp = await self.call(\"GET\", \"/v1/evaluators\", params=params, response_cls=api_types.ListEvaluatorsResponse)\n    resp.raise_for_status()\n    return resp.data.evaluators\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.list_evaluators_sync","title":"list_evaluators_sync","text":"<pre><code>list_evaluators_sync(by_alias_or_id: Optional[str] = None) -&gt; list[api_types.Evaluator]\n</code></pre> <p>Retrieves a list of available evaluators.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def list_evaluators_sync(self, by_alias_or_id: Optional[str] = None) -&gt; list[api_types.Evaluator]:\n    \"\"\"Retrieves a list of available evaluators.\"\"\"\n    params = {}\n    if by_alias_or_id:\n        params[\"by_alias_or_id\"] = by_alias_or_id\n\n    resp = self.call_sync(\"GET\", \"/v1/evaluators\", params=params, response_cls=api_types.ListEvaluatorsResponse)\n    resp.raise_for_status()\n    return resp.data.evaluators\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.search_evaluations","title":"search_evaluations  <code>async</code>","text":"<pre><code>search_evaluations(request: SearchEvaluationsRequest) -&gt; api_types.SearchEvaluationsResponse\n</code></pre> <p>Searches for evaluations based on the given criteria.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def search_evaluations(\n    self, request: api_types.SearchEvaluationsRequest\n) -&gt; api_types.SearchEvaluationsResponse:\n    \"\"\"Searches for evaluations based on the given criteria.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/evaluations/search\",\n        body=request,\n        response_cls=api_types.SearchEvaluationsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.search_evaluations_sync","title":"search_evaluations_sync","text":"<pre><code>search_evaluations_sync(request: SearchEvaluationsRequest) -&gt; api_types.SearchEvaluationsResponse\n</code></pre> <p>Searches for evaluations based on the given criteria.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def search_evaluations_sync(\n    self, request: api_types.SearchEvaluationsRequest\n) -&gt; api_types.SearchEvaluationsResponse:\n    \"\"\"Searches for evaluations based on the given criteria.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/evaluations/search\",\n        body=request,\n        response_cls=api_types.SearchEvaluationsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.search_logs","title":"search_logs  <code>async</code>","text":"<pre><code>search_logs(request: SearchLogsRequest) -&gt; api_types.SearchLogsResponse\n</code></pre> <p>Searches for logs based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def search_logs(self, request: api_types.SearchLogsRequest) -&gt; api_types.SearchLogsResponse:\n    \"\"\"Searches for logs based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        \"/v1/otel/logs/search\",\n        body=request,\n        response_cls=api_types.SearchLogsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.search_logs_sync","title":"search_logs_sync","text":"<pre><code>search_logs_sync(request: SearchLogsRequest) -&gt; api_types.SearchLogsResponse\n</code></pre> <p>Searches for logs based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def search_logs_sync(self, request: api_types.SearchLogsRequest) -&gt; api_types.SearchLogsResponse:\n    \"\"\"Searches for logs based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        \"/v1/otel/logs/search\",\n        body=request,\n        response_cls=api_types.SearchLogsResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.update_annotation_criteria","title":"update_annotation_criteria  <code>async</code>","text":"<pre><code>update_annotation_criteria(criteria_id: str, request: UpdateAnnotationCriteriaRequest) -&gt; api_types.UpdateAnnotationCriteriaResponse\n</code></pre> <p>Creates annotation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def update_annotation_criteria(\n    self, criteria_id: str, request: api_types.UpdateAnnotationCriteriaRequest\n) -&gt; api_types.UpdateAnnotationCriteriaResponse:\n    \"\"\"Creates annotation criteria based on the given request.\"\"\"\n    resp = await self.call(\n        \"PUT\",\n        f\"/v1/annotation-criteria/{criteria_id}\",\n        body=request,\n        response_cls=api_types.UpdateAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.update_annotation_criteria_sync","title":"update_annotation_criteria_sync","text":"<pre><code>update_annotation_criteria_sync(criteria_id: str, request: UpdateAnnotationCriteriaRequest) -&gt; api_types.UpdateAnnotationCriteriaResponse\n</code></pre> <p>Creates annotation criteria based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def update_annotation_criteria_sync(\n    self, criteria_id: str, request: api_types.UpdateAnnotationCriteriaRequest\n) -&gt; api_types.UpdateAnnotationCriteriaResponse:\n    \"\"\"Creates annotation criteria based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"PUT\",\n        f\"/v1/annotation-criteria/{criteria_id}\",\n        body=request,\n        response_cls=api_types.UpdateAnnotationCriteriaResponse,\n    )\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.update_experiment","title":"update_experiment  <code>async</code>","text":"<pre><code>update_experiment(experiment_id: str, request: UpdateExperimentRequest) -&gt; api_types.Experiment\n</code></pre> <p>Updates an existing experiment based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def update_experiment(\n    self, experiment_id: str, request: api_types.UpdateExperimentRequest\n) -&gt; api_types.Experiment:\n    \"\"\"Updates an existing experiment based on the given request.\"\"\"\n    resp = await self.call(\n        \"POST\",\n        f\"/v1/experiments/{experiment_id}\",\n        body=request,\n        response_cls=api_types.UpdateExperimentResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.update_experiment_sync","title":"update_experiment_sync","text":"<pre><code>update_experiment_sync(experiment_id: str, request: UpdateExperimentRequest) -&gt; api_types.Experiment\n</code></pre> <p>Updates an existing experiment based on the given request.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def update_experiment_sync(\n    self, experiment_id: str, request: api_types.UpdateExperimentRequest\n) -&gt; api_types.Experiment:\n    \"\"\"Updates an existing experiment based on the given request.\"\"\"\n    resp = self.call_sync(\n        \"POST\",\n        f\"/v1/experiments{experiment_id}\",\n        body=request,\n        response_cls=api_types.UpdateExperimentResponse,\n    )\n    resp.raise_for_status()\n    return resp.data.experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.upload_dataset","title":"upload_dataset  <code>async</code>","text":"<pre><code>upload_dataset(file_path: str, dataset_name: str, dataset_description: Optional[str] = None, custom_field_mapping: Optional[dict[str, Union[str, list[str]]]] = None) -&gt; api_types.Dataset\n</code></pre> <p>Upload a dataset file to create a new dataset in Patronus.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the dataset file (CSV or JSONL format)</p> required <code>dataset_name</code> <code>str</code> <p>Name for the created dataset</p> required <code>dataset_description</code> <code>Optional[str]</code> <p>Optional description for the dataset</p> <code>None</code> <code>custom_field_mapping</code> <code>Optional[dict[str, Union[str, list[str]]]]</code> <p>Optional mapping of standard field names to custom field names in the dataset</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset object representing the created dataset</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def upload_dataset(\n    self,\n    file_path: str,\n    dataset_name: str,\n    dataset_description: Optional[str] = None,\n    custom_field_mapping: Optional[dict[str, Union[str, list[str]]]] = None,\n) -&gt; api_types.Dataset:\n    \"\"\"\n    Upload a dataset file to create a new dataset in Patronus.\n\n    Args:\n        file_path: Path to the dataset file (CSV or JSONL format)\n        dataset_name: Name for the created dataset\n        dataset_description: Optional description for the dataset\n        custom_field_mapping: Optional mapping of standard field names to custom field names in the dataset\n\n    Returns:\n        Dataset object representing the created dataset\n    \"\"\"\n    with open(file_path, \"rb\") as f:\n        return await self.upload_dataset_from_buffer(f, dataset_name, dataset_description, custom_field_mapping)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.upload_dataset_from_buffer","title":"upload_dataset_from_buffer  <code>async</code>","text":"<pre><code>upload_dataset_from_buffer(file_obj: BinaryIO, dataset_name: str, dataset_description: Optional[str] = None, custom_field_mapping: Optional[dict[str, Union[str, list[str]]]] = None) -&gt; api_types.Dataset\n</code></pre> <p>Upload a dataset file to create a new dataset in Patronus AI Platform.</p> <p>Parameters:</p> Name Type Description Default <code>file_obj</code> <code>BinaryIO</code> <p>File-like object containing dataset content (CSV or JSONL format)</p> required <code>dataset_name</code> <code>str</code> <p>Name for the created dataset</p> required <code>dataset_description</code> <code>Optional[str]</code> <p>Optional description for the dataset</p> <code>None</code> <code>custom_field_mapping</code> <code>Optional[dict[str, Union[str, list[str]]]]</code> <p>Optional mapping of standard field names to custom field names in the dataset</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset object representing the created dataset</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def upload_dataset_from_buffer(\n    self,\n    file_obj: typing.BinaryIO,\n    dataset_name: str,\n    dataset_description: Optional[str] = None,\n    custom_field_mapping: Optional[dict[str, Union[str, list[str]]]] = None,\n) -&gt; api_types.Dataset:\n    \"\"\"\n    Upload a dataset file to create a new dataset in Patronus AI Platform.\n\n    Args:\n        file_obj: File-like object containing dataset content (CSV or JSONL format)\n        dataset_name: Name for the created dataset\n        dataset_description: Optional description for the dataset\n        custom_field_mapping: Optional mapping of standard field names to custom field names in the dataset\n\n    Returns:\n        Dataset object representing the created dataset\n    \"\"\"\n    data = {\n        \"dataset_name\": dataset_name,\n    }\n\n    if dataset_description is not None:\n        data[\"dataset_description\"] = dataset_description\n\n    if custom_field_mapping is not None:\n        data[\"custom_field_mapping\"] = json.dumps(custom_field_mapping)\n\n    files = {\"file\": (dataset_name, file_obj)}\n\n    resp = await self.call_multipart(\n        \"POST\",\n        \"/v1/datasets\",\n        files=files,\n        data=data,\n        response_cls=api_types.CreateDatasetResponse,\n    )\n\n    resp.raise_for_status()\n    return resp.data.dataset\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.upload_dataset_from_buffer_sync","title":"upload_dataset_from_buffer_sync","text":"<pre><code>upload_dataset_from_buffer_sync(file_obj: BinaryIO, dataset_name: str, dataset_description: Optional[str] = None, custom_field_mapping: Optional[dict[str, Union[str, list[str]]]] = None) -&gt; api_types.Dataset\n</code></pre> <p>Upload a dataset file to create a new dataset in Patronus AI Platform.</p> <p>Parameters:</p> Name Type Description Default <code>file_obj</code> <code>BinaryIO</code> <p>File-like object containing dataset content (CSV or JSONL format)</p> required <code>dataset_name</code> <code>str</code> <p>Name for the created dataset</p> required <code>dataset_description</code> <code>Optional[str]</code> <p>Optional description for the dataset</p> <code>None</code> <code>custom_field_mapping</code> <code>Optional[dict[str, Union[str, list[str]]]]</code> <p>Optional mapping of standard field names to custom field names in the dataset</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset object representing the created dataset</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def upload_dataset_from_buffer_sync(\n    self,\n    file_obj: typing.BinaryIO,\n    dataset_name: str,\n    dataset_description: Optional[str] = None,\n    custom_field_mapping: Optional[dict[str, Union[str, list[str]]]] = None,\n) -&gt; api_types.Dataset:\n    \"\"\"\n    Upload a dataset file to create a new dataset in Patronus AI Platform.\n\n    Args:\n        file_obj: File-like object containing dataset content (CSV or JSONL format)\n        dataset_name: Name for the created dataset\n        dataset_description: Optional description for the dataset\n        custom_field_mapping: Optional mapping of standard field names to custom field names in the dataset\n\n    Returns:\n        Dataset object representing the created dataset\n    \"\"\"\n    data = {\n        \"dataset_name\": dataset_name,\n    }\n\n    if dataset_description is not None:\n        data[\"dataset_description\"] = dataset_description\n\n    if custom_field_mapping is not None:\n        data[\"custom_field_mapping\"] = json.dumps(custom_field_mapping)\n\n    files = {\"file\": (dataset_name, file_obj)}\n\n    resp = self.call_multipart_sync(\n        \"POST\",\n        \"/v1/datasets\",\n        files=files,\n        data=data,\n        response_cls=api_types.CreateDatasetResponse,\n    )\n\n    resp.raise_for_status()\n    return resp.data.dataset\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.upload_dataset_sync","title":"upload_dataset_sync","text":"<pre><code>upload_dataset_sync(file_path: str, dataset_name: str, dataset_description: Optional[str] = None, custom_field_mapping: Optional[dict[str, Union[str, list[str]]]] = None) -&gt; api_types.Dataset\n</code></pre> <p>Upload a dataset file to create a new dataset in Patronus AI Platform.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the dataset file (CSV or JSONL format)</p> required <code>dataset_name</code> <code>str</code> <p>Name for the created dataset</p> required <code>dataset_description</code> <code>Optional[str]</code> <p>Optional description for the dataset</p> <code>None</code> <code>custom_field_mapping</code> <code>Optional[dict[str, Union[str, list[str]]]]</code> <p>Optional mapping of standard field names to custom field names in the dataset</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset object representing the created dataset</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def upload_dataset_sync(\n    self,\n    file_path: str,\n    dataset_name: str,\n    dataset_description: Optional[str] = None,\n    custom_field_mapping: Optional[dict[str, Union[str, list[str]]]] = None,\n) -&gt; api_types.Dataset:\n    \"\"\"\n    Upload a dataset file to create a new dataset in Patronus AI Platform.\n\n    Args:\n        file_path: Path to the dataset file (CSV or JSONL format)\n        dataset_name: Name for the created dataset\n        dataset_description: Optional description for the dataset\n        custom_field_mapping: Optional mapping of standard field names to custom field names in the dataset\n\n    Returns:\n        Dataset object representing the created dataset\n    \"\"\"\n    with open(file_path, \"rb\") as f:\n        return self.upload_dataset_from_buffer_sync(f, dataset_name, dataset_description, custom_field_mapping)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.whoami","title":"whoami  <code>async</code>","text":"<pre><code>whoami() -&gt; api_types.WhoAmIResponse\n</code></pre> <p>Fetches information about the authenticated user.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>async def whoami(self) -&gt; api_types.WhoAmIResponse:\n    \"\"\"Fetches information about the authenticated user.\"\"\"\n    resp = await self.call(\"GET\", \"/v1/whoami\", response_cls=api_types.WhoAmIResponse)\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_client.PatronusAPIClient.whoami_sync","title":"whoami_sync","text":"<pre><code>whoami_sync() -&gt; api_types.WhoAmIResponse\n</code></pre> <p>Fetches information about the authenticated user.</p> Source code in <code>src/patronus/api/api_client.py</code> <pre><code>def whoami_sync(self) -&gt; api_types.WhoAmIResponse:\n    \"\"\"Fetches information about the authenticated user.\"\"\"\n    resp = self.call_sync(\"GET\", \"/v1/whoami\", response_cls=api_types.WhoAmIResponse)\n    resp.raise_for_status()\n    return resp.data\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types","title":"patronus.api.api_types","text":""},{"location":"api_ref/api_client/#patronus.api.api_types.SanitizedApp","title":"SanitizedApp  <code>module-attribute</code>","text":"<pre><code>SanitizedApp = Annotated[str, _create_field_sanitizer('[^a-zA-Z0-9-_./ -]', max_len=50, replace_with='_')]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SanitizedLocalEvaluatorID","title":"SanitizedLocalEvaluatorID  <code>module-attribute</code>","text":"<pre><code>SanitizedLocalEvaluatorID = Annotated[Optional[str], _create_field_sanitizer('[^a-zA-Z0-9\\\\-_./]', max_len=50, replace_with='-')]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SanitizedProjectName","title":"SanitizedProjectName  <code>module-attribute</code>","text":"<pre><code>SanitizedProjectName = Annotated[str, project_name_sanitizer]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.project_name_sanitizer","title":"project_name_sanitizer  <code>module-attribute</code>","text":"<pre><code>project_name_sanitizer = (_create_field_sanitizer('[^a-zA-Z0-9_ -]', max_len=50, replace_with='_'),)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Account","title":"Account","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.Account.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Account.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AddEvaluatorCriteriaRevisionRequest","title":"AddEvaluatorCriteriaRevisionRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.AddEvaluatorCriteriaRevisionRequest.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: dict[str, Any]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AddEvaluatorCriteriaRevisionResponse","title":"AddEvaluatorCriteriaRevisionResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.AddEvaluatorCriteriaRevisionResponse.evaluator_criteria","title":"evaluator_criteria  <code>instance-attribute</code>","text":"<pre><code>evaluator_criteria: EvaluatorCriteria\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateRequest","title":"AnnotateRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateRequest.annotation_criteria_id","title":"annotation_criteria_id  <code>instance-attribute</code>","text":"<pre><code>annotation_criteria_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateRequest.explanation","title":"explanation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateRequest.log_id","title":"log_id  <code>instance-attribute</code>","text":"<pre><code>log_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateRequest.value_pass","title":"value_pass  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value_pass: Optional[bool] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateRequest.value_score","title":"value_score  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value_score: Optional[float] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateRequest.value_text","title":"value_text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value_text: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateResponse","title":"AnnotateResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotateResponse.evaluation","title":"evaluation  <code>instance-attribute</code>","text":"<pre><code>evaluation: Evaluation\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCategory","title":"AnnotationCategory","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCategory.label","title":"label  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>label: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCategory.score","title":"score  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>score: Optional[float] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria","title":"AnnotationCriteria","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria.annotation_type","title":"annotation_type  <code>instance-attribute</code>","text":"<pre><code>annotation_type: AnnotationType\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria.categories","title":"categories  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>categories: Optional[list[AnnotationCategory]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: datetime\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria.project_id","title":"project_id  <code>instance-attribute</code>","text":"<pre><code>project_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationCriteria.updated_at","title":"updated_at  <code>instance-attribute</code>","text":"<pre><code>updated_at: datetime\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationType","title":"AnnotationType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationType.binary","title":"binary  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binary = 'binary'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationType.categorical","title":"categorical  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>categorical = 'categorical'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationType.continuous","title":"continuous  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>continuous = 'continuous'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationType.discrete","title":"discrete  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>discrete = 'discrete'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.AnnotationType.text_annotation","title":"text_annotation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_annotation = 'text_annotation'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.BatchCreateEvaluationsRequest","title":"BatchCreateEvaluationsRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.BatchCreateEvaluationsRequest.evaluations","title":"evaluations  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluations: list[ClientEvaluation] = Field(min_length=1, max_length=1000)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.BatchCreateEvaluationsResponse","title":"BatchCreateEvaluationsResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.BatchCreateEvaluationsResponse.evaluations","title":"evaluations  <code>instance-attribute</code>","text":"<pre><code>evaluations: list[Evaluation]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation","title":"ClientEvaluation","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.app","title":"app  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>app: Optional[SanitizedApp] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.created_at","title":"created_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created_at: Optional[datetime] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.criteria","title":"criteria  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>criteria: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.dataset_id","title":"dataset_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.dataset_sample_id","title":"dataset_sample_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_sample_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.evaluation_duration","title":"evaluation_duration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_duration: Optional[timedelta] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.evaluator_id","title":"evaluator_id  <code>instance-attribute</code>","text":"<pre><code>evaluator_id: SanitizedLocalEvaluatorID\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.experiment_id","title":"experiment_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experiment_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.explanation","title":"explanation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.explanation_duration","title":"explanation_duration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation_duration: Optional[timedelta] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.log_id","title":"log_id  <code>instance-attribute</code>","text":"<pre><code>log_id: UUID\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.metric_description","title":"metric_description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metric_description: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.metric_name","title":"metric_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metric_name: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.pass_","title":"pass_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pass_: Optional[bool] = Field(default=None, serialization_alias='pass')\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.project_id","title":"project_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.project_name","title":"project_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project_name: Optional[SanitizedProjectName] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.score","title":"score  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>score: Optional[float] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.span_id","title":"span_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>span_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.text_output","title":"text_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_output: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ClientEvaluation.trace_id","title":"trace_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateAnnotationCriteriaRequest","title":"CreateAnnotationCriteriaRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateAnnotationCriteriaRequest.annotation_type","title":"annotation_type  <code>instance-attribute</code>","text":"<pre><code>annotation_type: AnnotationType\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateAnnotationCriteriaRequest.categories","title":"categories  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>categories: Optional[list[AnnotationCategory]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateAnnotationCriteriaRequest.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateAnnotationCriteriaRequest.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = Field(min_length=1, max_length=100)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateAnnotationCriteriaRequest.project_id","title":"project_id  <code>instance-attribute</code>","text":"<pre><code>project_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateAnnotationCriteriaResponse","title":"CreateAnnotationCriteriaResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateAnnotationCriteriaResponse.annotation_criteria","title":"annotation_criteria  <code>instance-attribute</code>","text":"<pre><code>annotation_criteria: AnnotationCriteria\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateCriteriaRequest","title":"CreateCriteriaRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateCriteriaRequest.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: dict[str, Any]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateCriteriaRequest.evaluator_family","title":"evaluator_family  <code>instance-attribute</code>","text":"<pre><code>evaluator_family: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateCriteriaRequest.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateCriteriaResponse","title":"CreateCriteriaResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateCriteriaResponse.evaluator_criteria","title":"evaluator_criteria  <code>instance-attribute</code>","text":"<pre><code>evaluator_criteria: EvaluatorCriteria\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateDatasetResponse","title":"CreateDatasetResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateDatasetResponse.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset: Dataset\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateDatasetResponse.dataset_id","title":"dataset_id  <code>instance-attribute</code>","text":"<pre><code>dataset_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateExperimentRequest","title":"CreateExperimentRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateExperimentRequest.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateExperimentRequest.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateExperimentRequest.project_id","title":"project_id  <code>instance-attribute</code>","text":"<pre><code>project_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateExperimentRequest.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: dict[str, str] = Field(default_factory=dict)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateExperimentResponse","title":"CreateExperimentResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateExperimentResponse.experiment","title":"experiment  <code>instance-attribute</code>","text":"<pre><code>experiment: Experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateProjectRequest","title":"CreateProjectRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.CreateProjectRequest.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: SanitizedProjectName\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Dataset","title":"Dataset","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.Dataset.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: datetime\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Dataset.creation_at","title":"creation_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>creation_at: Optional[datetime] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Dataset.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Dataset.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Dataset.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Dataset.samples","title":"samples  <code>instance-attribute</code>","text":"<pre><code>samples: int\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Dataset.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum","title":"DatasetDatum","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.dataset_id","title":"dataset_id  <code>instance-attribute</code>","text":"<pre><code>dataset_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.evaluated_model_gold_answer","title":"evaluated_model_gold_answer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_gold_answer: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.evaluated_model_input","title":"evaluated_model_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_input: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.evaluated_model_output","title":"evaluated_model_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_output: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.evaluated_model_retrieved_context","title":"evaluated_model_retrieved_context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_retrieved_context: Optional[list[str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.evaluated_model_system_prompt","title":"evaluated_model_system_prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_system_prompt: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.meta_evaluated_model_name","title":"meta_evaluated_model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_evaluated_model_name: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.meta_evaluated_model_params","title":"meta_evaluated_model_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_evaluated_model_params: Optional[dict[str, Union[str, int, float]]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.meta_evaluated_model_provider","title":"meta_evaluated_model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_evaluated_model_provider: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.meta_evaluated_model_selected_model","title":"meta_evaluated_model_selected_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_evaluated_model_selected_model: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.DatasetDatum.sid","title":"sid  <code>instance-attribute</code>","text":"<pre><code>sid: int\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateEvaluator","title":"EvaluateEvaluator","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateEvaluator.criteria","title":"criteria  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>criteria: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateEvaluator.evaluator","title":"evaluator  <code>instance-attribute</code>","text":"<pre><code>evaluator: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateEvaluator.explain_strategy","title":"explain_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explain_strategy: str = 'always'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest","title":"EvaluateRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.app","title":"app  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>app: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.capture","title":"capture  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>capture: str = 'all'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.dataset_id","title":"dataset_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.dataset_sample_id","title":"dataset_sample_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_sample_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.evaluated_model_attachments","title":"evaluated_model_attachments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_attachments: Optional[list[EvaluatedModelAttachment]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.evaluated_model_gold_answer","title":"evaluated_model_gold_answer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_gold_answer: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.evaluated_model_input","title":"evaluated_model_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_input: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.evaluated_model_output","title":"evaluated_model_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_output: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.evaluated_model_retrieved_context","title":"evaluated_model_retrieved_context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_retrieved_context: Optional[Union[list[str], str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.evaluated_model_system_prompt","title":"evaluated_model_system_prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_system_prompt: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.evaluators","title":"evaluators  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluators: list[EvaluateEvaluator] = Field(min_length=1)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.experiment_id","title":"experiment_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experiment_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.log_id","title":"log_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.project_id","title":"project_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.project_name","title":"project_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project_name: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.span_id","title":"span_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>span_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateRequest.trace_id","title":"trace_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateResponse","title":"EvaluateResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateResponse.results","title":"results  <code>instance-attribute</code>","text":"<pre><code>results: list[EvaluateResult]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateResult","title":"EvaluateResult","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateResult.criteria","title":"criteria  <code>instance-attribute</code>","text":"<pre><code>criteria: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateResult.error_message","title":"error_message  <code>instance-attribute</code>","text":"<pre><code>error_message: Optional[str]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateResult.evaluation_result","title":"evaluation_result  <code>instance-attribute</code>","text":"<pre><code>evaluation_result: Optional[EvaluationResult]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateResult.evaluator_id","title":"evaluator_id  <code>instance-attribute</code>","text":"<pre><code>evaluator_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluateResult.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatedModelAttachment","title":"EvaluatedModelAttachment","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatedModelAttachment.media_type","title":"media_type  <code>instance-attribute</code>","text":"<pre><code>media_type: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatedModelAttachment.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatedModelAttachment.usage_type","title":"usage_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage_type: Optional[str] = 'evaluated_model_input'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation","title":"Evaluation","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.annotation_criteria_id","title":"annotation_criteria_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>annotation_criteria_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.app","title":"app  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>app: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.created_at","title":"created_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created_at: datetime\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.criteria","title":"criteria  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>criteria: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.criteria_id","title":"criteria_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>criteria_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.dataset_id","title":"dataset_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.dataset_sample_id","title":"dataset_sample_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_sample_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.evaluation_duration","title":"evaluation_duration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_duration: Optional[timedelta] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.evaluation_type","title":"evaluation_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_type: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.evaluator_family","title":"evaluator_family  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluator_family: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.evaluator_id","title":"evaluator_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluator_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.experiment_id","title":"experiment_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experiment_id: Optional[int] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.explain_strategy","title":"explain_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explain_strategy: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.explanation","title":"explanation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.explanation_duration","title":"explanation_duration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation_duration: Optional[timedelta] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: int\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.log_id","title":"log_id  <code>instance-attribute</code>","text":"<pre><code>log_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.metric_description","title":"metric_description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metric_description: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.metric_name","title":"metric_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metric_name: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.pass_","title":"pass_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pass_: Optional[bool] = Field(default=None, alias='pass')\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.project_id","title":"project_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.score","title":"score  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>score: Optional[float] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.span_id","title":"span_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>span_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.text_output","title":"text_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_output: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.trace_id","title":"trace_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluation.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult","title":"EvaluationResult","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.additional_info","title":"additional_info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_info: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.app","title":"app  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>app: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.created_at","title":"created_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created_at: Optional[AwareDatetime] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.criteria","title":"criteria  <code>instance-attribute</code>","text":"<pre><code>criteria: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.dataset_id","title":"dataset_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.dataset_sample_id","title":"dataset_sample_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_sample_id: Optional[int] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluated_model_gold_answer","title":"evaluated_model_gold_answer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_gold_answer: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluated_model_input","title":"evaluated_model_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_input: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluated_model_output","title":"evaluated_model_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_output: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluated_model_retrieved_context","title":"evaluated_model_retrieved_context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_retrieved_context: Optional[list[str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluated_model_system_prompt","title":"evaluated_model_system_prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_system_prompt: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluation_duration","title":"evaluation_duration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_duration: Optional[timedelta] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluation_metadata","title":"evaluation_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_metadata: Optional[dict] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluator_family","title":"evaluator_family  <code>instance-attribute</code>","text":"<pre><code>evaluator_family: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluator_id","title":"evaluator_id  <code>instance-attribute</code>","text":"<pre><code>evaluator_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.evaluator_profile_public_id","title":"evaluator_profile_public_id  <code>instance-attribute</code>","text":"<pre><code>evaluator_profile_public_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.experiment_id","title":"experiment_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experiment_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.explanation","title":"explanation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.explanation_duration","title":"explanation_duration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation_duration: Optional[timedelta] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.pass_","title":"pass_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pass_: Optional[bool] = Field(default=None, alias='pass')\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.project_id","title":"project_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>project_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.score_raw","title":"score_raw  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>score_raw: Optional[float] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluationResult.text_output","title":"text_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_output: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluator","title":"Evaluator","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluator.aliases","title":"aliases  <code>instance-attribute</code>","text":"<pre><code>aliases: Optional[list[str]]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluator.default_criteria","title":"default_criteria  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_criteria: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluator.evaluator_family","title":"evaluator_family  <code>instance-attribute</code>","text":"<pre><code>evaluator_family: Optional[str]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluator.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Evaluator.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria","title":"EvaluatorCriteria","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: Optional[dict[str, Any]]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: datetime\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: Optional[str]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria.evaluator_family","title":"evaluator_family  <code>instance-attribute</code>","text":"<pre><code>evaluator_family: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria.is_patronus_managed","title":"is_patronus_managed  <code>instance-attribute</code>","text":"<pre><code>is_patronus_managed: bool\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria.public_id","title":"public_id  <code>instance-attribute</code>","text":"<pre><code>public_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.EvaluatorCriteria.revision","title":"revision  <code>instance-attribute</code>","text":"<pre><code>revision: int\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Experiment","title":"Experiment","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.Experiment.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Experiment.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Experiment.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Experiment.project_id","title":"project_id  <code>instance-attribute</code>","text":"<pre><code>project_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Experiment.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationRequest","title":"ExportEvaluationRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationRequest.evaluation_results","title":"evaluation_results  <code>instance-attribute</code>","text":"<pre><code>evaluation_results: list[ExportEvaluationResult]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResponse","title":"ExportEvaluationResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResponse.evaluation_results","title":"evaluation_results  <code>instance-attribute</code>","text":"<pre><code>evaluation_results: list[ExportEvaluationResultPartial]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult","title":"ExportEvaluationResult","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.app","title":"app  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>app: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.criteria","title":"criteria  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>criteria: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.dataset_id","title":"dataset_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.dataset_sample_id","title":"dataset_sample_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_sample_id: Optional[int] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_attachments","title":"evaluated_model_attachments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_attachments: Optional[list[EvaluatedModelAttachment]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_gold_answer","title":"evaluated_model_gold_answer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_gold_answer: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_input","title":"evaluated_model_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_input: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_name","title":"evaluated_model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_name: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_output","title":"evaluated_model_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_output: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_params","title":"evaluated_model_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_params: Optional[dict[str, Union[str, int, float]]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_provider","title":"evaluated_model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_provider: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_retrieved_context","title":"evaluated_model_retrieved_context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_retrieved_context: Optional[list[str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_selected_model","title":"evaluated_model_selected_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_selected_model: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluated_model_system_prompt","title":"evaluated_model_system_prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluated_model_system_prompt: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluation_duration","title":"evaluation_duration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_duration: Optional[timedelta] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluation_metadata","title":"evaluation_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_metadata: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.evaluator_id","title":"evaluator_id  <code>instance-attribute</code>","text":"<pre><code>evaluator_id: SanitizedLocalEvaluatorID\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.experiment_id","title":"experiment_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experiment_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.explanation","title":"explanation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.explanation_duration","title":"explanation_duration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>explanation_duration: Optional[timedelta] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.pass_","title":"pass_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pass_: Optional[bool] = Field(default=None, serialization_alias='pass')\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.score_raw","title":"score_raw  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>score_raw: Optional[float] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResult.text_output","title":"text_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_output: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResultPartial","title":"ExportEvaluationResultPartial","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResultPartial.app","title":"app  <code>instance-attribute</code>","text":"<pre><code>app: Optional[str]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResultPartial.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: AwareDatetime\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResultPartial.evaluator_id","title":"evaluator_id  <code>instance-attribute</code>","text":"<pre><code>evaluator_id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ExportEvaluationResultPartial.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.GetAnnotationCriteriaResponse","title":"GetAnnotationCriteriaResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.GetAnnotationCriteriaResponse.annotation_criteria","title":"annotation_criteria  <code>instance-attribute</code>","text":"<pre><code>annotation_criteria: AnnotationCriteria\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.GetEvaluationResponse","title":"GetEvaluationResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.GetEvaluationResponse.evaluation","title":"evaluation  <code>instance-attribute</code>","text":"<pre><code>evaluation: Evaluation\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.GetExperimentResponse","title":"GetExperimentResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.GetExperimentResponse.experiment","title":"experiment  <code>instance-attribute</code>","text":"<pre><code>experiment: Experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.GetProjectResponse","title":"GetProjectResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.GetProjectResponse.project","title":"project  <code>instance-attribute</code>","text":"<pre><code>project: Project\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListAnnotationCriteriaResponse","title":"ListAnnotationCriteriaResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListAnnotationCriteriaResponse.annotation_criteria","title":"annotation_criteria  <code>instance-attribute</code>","text":"<pre><code>annotation_criteria: list[AnnotationCriteria]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest","title":"ListCriteriaRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.evaluator_family","title":"evaluator_family  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluator_family: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.evaluator_id","title":"evaluator_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluator_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.get_last_revision","title":"get_last_revision  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>get_last_revision: bool = False\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.is_patronus_managed","title":"is_patronus_managed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_patronus_managed: Optional[bool] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.limit","title":"limit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>limit: int = 1000\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.offset","title":"offset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset: int = 0\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.public_id","title":"public_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>public_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaRequest.revision","title":"revision  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>revision: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaResponse","title":"ListCriteriaResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListCriteriaResponse.evaluator_criteria","title":"evaluator_criteria  <code>instance-attribute</code>","text":"<pre><code>evaluator_criteria: list[EvaluatorCriteria]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListDatasetData","title":"ListDatasetData","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListDatasetData.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: list[DatasetDatum]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListDatasetsResponse","title":"ListDatasetsResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListDatasetsResponse.datasets","title":"datasets  <code>instance-attribute</code>","text":"<pre><code>datasets: list[Dataset]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListEvaluatorsResponse","title":"ListEvaluatorsResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.ListEvaluatorsResponse.evaluators","title":"evaluators  <code>instance-attribute</code>","text":"<pre><code>evaluators: list[Evaluator]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log","title":"Log","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.body","title":"body  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>body: Any = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.log_attributes","title":"log_attributes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_attributes: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.resource_attributes","title":"resource_attributes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resource_attributes: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.resource_schema_url","title":"resource_schema_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resource_schema_url: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.scope_attributes","title":"scope_attributes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_attributes: Optional[dict[str, str]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.scope_name","title":"scope_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_name: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.scope_schema_url","title":"scope_schema_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_schema_url: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.scope_version","title":"scope_version  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scope_version: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.service_name","title":"service_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>service_name: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.severity_number","title":"severity_number  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>severity_number: Optional[int] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.severity_test","title":"severity_test  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>severity_test: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.span_id","title":"span_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>span_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: Optional[datetime] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.trace_flags","title":"trace_flags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_flags: Optional[int] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Log.trace_id","title":"trace_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_id: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Project","title":"Project","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.Project.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.Project.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsFilter","title":"SearchEvaluationsFilter","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsFilter.and_","title":"and_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>and_: Optional[list[SearchEvaluationsFilter]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsFilter.field","title":"field  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>field: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsFilter.operation","title":"operation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>operation: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsFilter.or_","title":"or_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>or_: Optional[list[SearchEvaluationsFilter]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsFilter.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value: Optional[Any] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsRequest","title":"SearchEvaluationsRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsRequest.filters","title":"filters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filters: Optional[list[SearchEvaluationsFilter]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsResponse","title":"SearchEvaluationsResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchEvaluationsResponse.evaluations","title":"evaluations  <code>instance-attribute</code>","text":"<pre><code>evaluations: list[Evaluation]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsFilter","title":"SearchLogsFilter","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsFilter.and_","title":"and_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>and_: Optional[list[SearchLogsFilter]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsFilter.field","title":"field  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>field: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsFilter.op","title":"op  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>op: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsFilter.or_","title":"or_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>or_: Optional[list[SearchLogsFilter]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsFilter.value","title":"value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value: Optional[Any] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsRequest","title":"SearchLogsRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsRequest.filters","title":"filters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filters: Optional[list[SearchLogsFilter]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsRequest.limit","title":"limit  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>limit: int = 1000\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsRequest.order","title":"order  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>order: str = 'timestamp desc'\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsResponse","title":"SearchLogsResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.SearchLogsResponse.logs","title":"logs  <code>instance-attribute</code>","text":"<pre><code>logs: list[Log]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateAnnotationCriteriaRequest","title":"UpdateAnnotationCriteriaRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateAnnotationCriteriaRequest.annotation_type","title":"annotation_type  <code>instance-attribute</code>","text":"<pre><code>annotation_type: AnnotationType\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateAnnotationCriteriaRequest.categories","title":"categories  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>categories: Optional[list[AnnotationCategory]] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateAnnotationCriteriaRequest.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: Optional[str] = None\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateAnnotationCriteriaRequest.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = Field(min_length=1, max_length=100)\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateAnnotationCriteriaResponse","title":"UpdateAnnotationCriteriaResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateAnnotationCriteriaResponse.annotation_criteria","title":"annotation_criteria  <code>instance-attribute</code>","text":"<pre><code>annotation_criteria: AnnotationCriteria\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateExperimentRequest","title":"UpdateExperimentRequest","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateExperimentRequest.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: dict[str, Any]\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateExperimentResponse","title":"UpdateExperimentResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.UpdateExperimentResponse.experiment","title":"experiment  <code>instance-attribute</code>","text":"<pre><code>experiment: Experiment\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.WhoAmIAPIKey","title":"WhoAmIAPIKey","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.WhoAmIAPIKey.account","title":"account  <code>instance-attribute</code>","text":"<pre><code>account: Account\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.WhoAmIAPIKey.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.WhoAmICaller","title":"WhoAmICaller","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.WhoAmICaller.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key: WhoAmIAPIKey\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.WhoAmIResponse","title":"WhoAmIResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api_ref/api_client/#patronus.api.api_types.WhoAmIResponse.caller","title":"caller  <code>instance-attribute</code>","text":"<pre><code>caller: WhoAmICaller\n</code></pre>"},{"location":"api_ref/api_client/#patronus.api.api_types.sanitize_field","title":"sanitize_field","text":"<pre><code>sanitize_field(max_length: int, sub_pattern: str)\n</code></pre> Source code in <code>src/patronus/api/api_types.py</code> <pre><code>def sanitize_field(max_length: int, sub_pattern: str):\n    def wrapper(value: str) -&gt; str:\n        if not value:\n            return value\n        value = value[:max_length]\n        return re.sub(sub_pattern, \"_\", value).strip()\n\n    return wrapper\n</code></pre>"},{"location":"api_ref/config/","title":"Config","text":""},{"location":"api_ref/config/#patronus.config","title":"patronus.config","text":""},{"location":"api_ref/config/#patronus.config.Config","title":"Config","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for the Patronus SDK.</p> <p>This class defines all available configuration options with their default values and handles loading configuration from environment variables and YAML files.</p> <p>Configuration sources are checked in this order:</p> <ol> <li>Code-specified values</li> <li>Environment variables (with prefix PATRONUS_)</li> <li>YAML configuration file (patronus.yaml)</li> <li>Default values</li> </ol> <p>Attributes:</p> Name Type Description <code>service</code> <code>str</code> <p>The name of the service or application component. Defaults to OTEL_SERVICE_NAME env var or platform.node().</p> <code>api_key</code> <code>Optional[str]</code> <p>Authentication key for Patronus services.</p> <code>api_url</code> <code>str</code> <p>URL for the Patronus API service. Default: https://api.patronus.ai</p> <code>otel_endpoint</code> <code>str</code> <p>Endpoint for OpenTelemetry data collection. Default: https://otel.patronus.ai:4317</p> <code>otel_exporter_otlp_protocol</code> <code>Optional[Literal['grpc', 'http/protobuf']]</code> <p>OpenTelemetry exporter protocol. Values: grpc, http/protobuf. Falls back to standard OTEL environment variables if not set.</p> <code>ui_url</code> <code>str</code> <p>URL for the Patronus UI. Default: https://app.patronus.ai</p> <code>timeout_s</code> <code>int</code> <p>Timeout in seconds for HTTP requests. Default: 300</p> <code>project_name</code> <code>str</code> <p>Name of the project for organizing evaluations and experiments. Default: Global</p> <code>app</code> <code>str</code> <p>Name of the application within the project. Default: default</p>"},{"location":"api_ref/config/#patronus.config.config","title":"config  <code>cached</code>","text":"<pre><code>config() -&gt; Config\n</code></pre> <p>Returns the Patronus SDK configuration singleton.</p> <p>Configuration is loaded from environment variables and the patronus.yaml file (if present) when this function is first called.</p> <p>Returns:</p> Name Type Description <code>Config</code> <code>Config</code> <p>A singleton Config object containing all Patronus configuration settings.</p> Example <pre><code>from patronus.config import config\n\n# Get the configuration\ncfg = config()\n\n# Access configuration values\napi_key = cfg.api_key\nproject_name = cfg.project_name\n</code></pre> Source code in <code>src/patronus/config.py</code> <pre><code>@functools.lru_cache()\ndef config() -&gt; Config:\n    \"\"\"\n    Returns the Patronus SDK configuration singleton.\n\n    Configuration is loaded from environment variables and the patronus.yaml file\n    (if present) when this function is first called.\n\n    Returns:\n        Config: A singleton Config object containing all Patronus configuration settings.\n\n    Example:\n        ```python\n        from patronus.config import config\n\n        # Get the configuration\n        cfg = config()\n\n        # Access configuration values\n        api_key = cfg.api_key\n        project_name = cfg.project_name\n        ```\n    \"\"\"\n    cfg = Config()\n    return cfg\n</code></pre>"},{"location":"api_ref/context/","title":"Context","text":""},{"location":"api_ref/context/#patronus.context","title":"patronus.context","text":"<p>Context management for Patronus SDK.</p> <p>This module provides classes and utility functions for managing the global Patronus context and accessing different components of the SDK like logging, tracing, and API clients.</p>"},{"location":"api_ref/context/#patronus.context.PatronusScope","title":"PatronusScope  <code>dataclass</code>","text":"<pre><code>PatronusScope(service: Optional[str], project_name: Optional[str], app: Optional[str], experiment_id: Optional[str], experiment_name: Optional[str])\n</code></pre> <p>Scope information for Patronus context.</p> <p>Defines the scope of the current Patronus application or experiment.</p> <p>Attributes:</p> Name Type Description <code>service</code> <code>Optional[str]</code> <p>The service name as defined in OTeL.</p> <code>project_name</code> <code>Optional[str]</code> <p>The project name.</p> <code>app</code> <code>Optional[str]</code> <p>The application name.</p> <code>experiment_id</code> <code>Optional[str]</code> <p>The unique identifier for the experiment.</p> <code>experiment_name</code> <code>Optional[str]</code> <p>The name of the experiment.</p>"},{"location":"api_ref/context/#patronus.context.PromptsConfig","title":"PromptsConfig  <code>dataclass</code>","text":"<pre><code>PromptsConfig(directory: Path, providers: list[str], templating_engine: str)\n</code></pre>"},{"location":"api_ref/context/#patronus.context.PromptsConfig.directory","title":"directory  <code>instance-attribute</code>","text":"<pre><code>directory: Path\n</code></pre> <p>The absolute path to a directory where prompts are stored locally.</p>"},{"location":"api_ref/context/#patronus.context.PromptsConfig.providers","title":"providers  <code>instance-attribute</code>","text":"<pre><code>providers: list[str]\n</code></pre> <p>List of default prompt providers.</p>"},{"location":"api_ref/context/#patronus.context.PromptsConfig.templating_engine","title":"templating_engine  <code>instance-attribute</code>","text":"<pre><code>templating_engine: str\n</code></pre> <p>Default prompt templating engine.</p>"},{"location":"api_ref/context/#patronus.context.PatronusContext","title":"PatronusContext  <code>dataclass</code>","text":"<pre><code>PatronusContext(scope: PatronusScope, tracer_provider: TracerProvider, logger_provider: LoggerProvider, api_client_deprecated: PatronusAPIClient, api_client: Client, async_api_client: AsyncClient, exporter: BatchEvaluationExporter, prompts: PromptsConfig)\n</code></pre> <p>Context object for Patronus SDK.</p> <p>Contains all the necessary components for the SDK to function properly.</p> <p>Attributes:</p> Name Type Description <code>scope</code> <code>PatronusScope</code> <p>Scope information for this context.</p> <code>tracer_provider</code> <code>TracerProvider</code> <p>The OpenTelemetry tracer provider.</p> <code>logger_provider</code> <code>LoggerProvider</code> <p>The OpenTelemetry logger provider.</p> <code>api_client_deprecated</code> <code>PatronusAPIClient</code> <p>Client for Patronus API communication (deprecated).</p> <code>api_client</code> <code>Client</code> <p>Client for Patronus API communication using the modern client.</p> <code>async_api_client</code> <code>AsyncClient</code> <p>Asynchronous client for Patronus API communication.</p> <code>exporter</code> <code>BatchEvaluationExporter</code> <p>Exporter for batch evaluation results.</p> <code>prompts</code> <code>PromptsConfig</code> <p>Configuration for prompt management.</p>"},{"location":"api_ref/context/#patronus.context.set_global_patronus_context","title":"set_global_patronus_context","text":"<pre><code>set_global_patronus_context(ctx: PatronusContext)\n</code></pre> <p>Set the global Patronus context.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>PatronusContext</code> <p>The Patronus context to set globally.</p> required Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def set_global_patronus_context(ctx: PatronusContext):\n    \"\"\"\n    Set the global Patronus context.\n\n    Args:\n        ctx: The Patronus context to set globally.\n    \"\"\"\n    _CTX_PAT.set_global(ctx)\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_current_context_or_none","title":"get_current_context_or_none","text":"<pre><code>get_current_context_or_none() -&gt; Optional[PatronusContext]\n</code></pre> <p>Get the current Patronus context or None if not initialized.</p> <p>Returns:</p> Type Description <code>Optional[PatronusContext]</code> <p>The current PatronusContext if set, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_current_context_or_none() -&gt; Optional[PatronusContext]:\n    \"\"\"\n    Get the current Patronus context or None if not initialized.\n\n    Returns:\n        The current PatronusContext if set, otherwise None.\n    \"\"\"\n    return _CTX_PAT.get()\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_current_context","title":"get_current_context","text":"<pre><code>get_current_context() -&gt; PatronusContext\n</code></pre> <p>Get the current Patronus context.</p> <p>Returns:</p> Type Description <code>PatronusContext</code> <p>The current PatronusContext.</p> <p>Raises:</p> Type Description <code>UninitializedError</code> <p>If no active Patronus context is found.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_current_context() -&gt; PatronusContext:\n    \"\"\"\n    Get the current Patronus context.\n\n    Returns:\n        The current PatronusContext.\n\n    Raises:\n        UninitializedError: If no active Patronus context is found.\n    \"\"\"\n    ctx = get_current_context_or_none()\n    if ctx is None:\n        raise UninitializedError(\n            \"No active Patronus context found. Please initialize the library by calling patronus.init().\"\n        )\n    return ctx\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_logger","title":"get_logger","text":"<pre><code>get_logger(ctx: Optional[PatronusContext] = None, level: int = logging.INFO) -&gt; logging.Logger\n</code></pre> <p>Get a standard Python logger configured with the Patronus context.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <code>level</code> <code>int</code> <p>The logging level to set. Defaults to INFO.</p> <code>INFO</code> <p>Returns:</p> Type Description <code>Logger</code> <p>A configured Python logger.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_logger(ctx: Optional[PatronusContext] = None, level: int = logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Get a standard Python logger configured with the Patronus context.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n        level: The logging level to set. Defaults to INFO.\n\n    Returns:\n        A configured Python logger.\n    \"\"\"\n    from patronus.tracing.logger import set_logger_handler\n\n    ctx = ctx or get_current_context()\n\n    logger = logging.getLogger(\"patronus.sdk\")\n    set_logger_handler(logger, ctx.scope, ctx.logger_provider)\n    logger.setLevel(level)\n    return logger\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_logger_or_none","title":"get_logger_or_none","text":"<pre><code>get_logger_or_none(level: int = logging.INFO) -&gt; Optional[logging.Logger]\n</code></pre> <p>Get a standard Python logger or None if context is not initialized.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>The logging level to set. Defaults to INFO.</p> <code>INFO</code> <p>Returns:</p> Type Description <code>Optional[Logger]</code> <p>A configured Python logger if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_logger_or_none(level: int = logging.INFO) -&gt; Optional[logging.Logger]:\n    \"\"\"\n    Get a standard Python logger or None if context is not initialized.\n\n    Args:\n        level: The logging level to set. Defaults to INFO.\n\n    Returns:\n        A configured Python logger if context is available, otherwise None.\n    \"\"\"\n    ctx = get_current_context()\n    if ctx is None:\n        return None\n    return get_logger(ctx, level=level)\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_pat_logger","title":"get_pat_logger","text":"<pre><code>get_pat_logger(ctx: Optional[PatronusContext] = None) -&gt; PatLogger\n</code></pre> <p>Get a Patronus logger.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>A Patronus logger.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_pat_logger(ctx: Optional[PatronusContext] = None) -&gt; \"PatLogger\":\n    \"\"\"\n    Get a Patronus logger.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n\n    Returns:\n        A Patronus logger.\n    \"\"\"\n    ctx = ctx or get_current_context()\n    return ctx.logger_provider.get_logger(\"patronus.sdk\")\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_pat_logger_or_none","title":"get_pat_logger_or_none","text":"<pre><code>get_pat_logger_or_none() -&gt; Optional[PatLogger]\n</code></pre> <p>Get a Patronus logger or None if context is not initialized.</p> <p>Returns:</p> Type Description <code>Optional[Logger]</code> <p>A Patronus logger if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_pat_logger_or_none() -&gt; Optional[\"PatLogger\"]:\n    \"\"\"\n    Get a Patronus logger or None if context is not initialized.\n\n    Returns:\n        A Patronus logger if context is available, otherwise None.\n    \"\"\"\n    ctx = get_current_context_or_none()\n    if ctx is None:\n        return None\n\n    return ctx.logger_provider.get_logger(\"patronus.sdk\")\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_tracer","title":"get_tracer","text":"<pre><code>get_tracer(ctx: Optional[PatronusContext] = None) -&gt; trace.Tracer\n</code></pre> <p>Get an OpenTelemetry tracer.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tracer</code> <p>An OpenTelemetry tracer.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_tracer(ctx: Optional[PatronusContext] = None) -&gt; trace.Tracer:\n    \"\"\"\n    Get an OpenTelemetry tracer.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n\n    Returns:\n        An OpenTelemetry tracer.\n    \"\"\"\n    ctx = ctx or get_current_context()\n    return ctx.tracer_provider.get_tracer(\"patronus.sdk\")\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_tracer_or_none","title":"get_tracer_or_none","text":"<pre><code>get_tracer_or_none() -&gt; Optional[trace.Tracer]\n</code></pre> <p>Get an OpenTelemetry tracer or None if context is not initialized.</p> <p>Returns:</p> Type Description <code>Optional[Tracer]</code> <p>An OpenTelemetry tracer if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_tracer_or_none() -&gt; Optional[trace.Tracer]:\n    \"\"\"\n    Get an OpenTelemetry tracer or None if context is not initialized.\n\n    Returns:\n        An OpenTelemetry tracer if context is available, otherwise None.\n    \"\"\"\n    ctx = get_current_context_or_none()\n    if ctx is None:\n        return None\n    return ctx.tracer_provider.get_tracer(\"patronus.sdk\")\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_api_client_deprecated","title":"get_api_client_deprecated","text":"<pre><code>get_api_client_deprecated(ctx: Optional[PatronusContext] = None) -&gt; PatronusAPIClient\n</code></pre> <p>Get the Patronus API client.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <p>Returns:</p> Type Description <code>PatronusAPIClient</code> <p>The Patronus API client.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_api_client_deprecated(ctx: Optional[PatronusContext] = None) -&gt; \"PatronusAPIClient\":\n    \"\"\"\n    Get the Patronus API client.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n\n    Returns:\n        The Patronus API client.\n    \"\"\"\n    ctx = ctx or get_current_context()\n    return ctx.api_client_deprecated\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_api_client_deprecated_or_none","title":"get_api_client_deprecated_or_none","text":"<pre><code>get_api_client_deprecated_or_none() -&gt; Optional[PatronusAPIClient]\n</code></pre> <p>Get the Patronus API client or None if context is not initialized.</p> <p>Returns:</p> Type Description <code>Optional[PatronusAPIClient]</code> <p>The Patronus API client if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_api_client_deprecated_or_none() -&gt; Optional[\"PatronusAPIClient\"]:\n    \"\"\"\n    Get the Patronus API client or None if context is not initialized.\n\n    Returns:\n        The Patronus API client if context is available, otherwise None.\n    \"\"\"\n    return (ctx := get_current_context_or_none()) and ctx.api_client_deprecated\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_api_client","title":"get_api_client","text":"<pre><code>get_api_client(ctx: Optional[PatronusContext] = None) -&gt; patronus_api.Client\n</code></pre> <p>Get the Patronus API client.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>The Patronus API client.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_api_client(ctx: Optional[PatronusContext] = None) -&gt; patronus_api.Client:\n    \"\"\"\n    Get the Patronus API client.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n\n    Returns:\n        The Patronus API client.\n    \"\"\"\n    ctx = ctx or get_current_context()\n    return ctx.api_client\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_api_client_or_none","title":"get_api_client_or_none","text":"<pre><code>get_api_client_or_none() -&gt; Optional[patronus_api.Client]\n</code></pre> <p>Get the Patronus API client or None if context is not initialized.</p> <p>Returns:</p> Type Description <code>Optional[Client]</code> <p>The Patronus API client if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_api_client_or_none() -&gt; Optional[patronus_api.Client]:\n    \"\"\"\n    Get the Patronus API client or None if context is not initialized.\n\n    Returns:\n        The Patronus API client if context is available, otherwise None.\n    \"\"\"\n    return (ctx := get_current_context_or_none()) and ctx.api_client\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_async_api_client","title":"get_async_api_client","text":"<pre><code>get_async_api_client(ctx: Optional[PatronusContext] = None) -&gt; patronus_api.AsyncClient\n</code></pre> <p>Get the asynchronous Patronus API client.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <p>Returns:</p> Type Description <code>AsyncClient</code> <p>The asynchronous Patronus API client.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_async_api_client(ctx: Optional[PatronusContext] = None) -&gt; patronus_api.AsyncClient:\n    \"\"\"\n    Get the asynchronous Patronus API client.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n\n    Returns:\n        The asynchronous Patronus API client.\n    \"\"\"\n    ctx = ctx or get_current_context()\n    return ctx.async_api_client\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_async_api_client_or_none","title":"get_async_api_client_or_none","text":"<pre><code>get_async_api_client_or_none() -&gt; Optional[patronus_api.AsyncClient]\n</code></pre> <p>Get the asynchronous Patronus API client or None if context is not initialized.</p> <p>Returns:</p> Type Description <code>Optional[AsyncClient]</code> <p>The asynchronous Patronus API client if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_async_api_client_or_none() -&gt; Optional[patronus_api.AsyncClient]:\n    \"\"\"\n    Get the asynchronous Patronus API client or None if context is not initialized.\n\n    Returns:\n        The asynchronous Patronus API client if context is available, otherwise None.\n    \"\"\"\n    return (ctx := get_current_context_or_none()) and ctx.async_api_client\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_exporter","title":"get_exporter","text":"<pre><code>get_exporter(ctx: Optional[PatronusContext] = None) -&gt; BatchEvaluationExporter\n</code></pre> <p>Get the batch evaluation exporter.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <p>Returns:</p> Type Description <code>BatchEvaluationExporter</code> <p>The batch evaluation exporter.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_exporter(ctx: Optional[PatronusContext] = None) -&gt; \"BatchEvaluationExporter\":\n    \"\"\"\n    Get the batch evaluation exporter.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n\n    Returns:\n        The batch evaluation exporter.\n    \"\"\"\n    ctx = ctx or get_current_context()\n    return ctx.exporter\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_exporter_or_none","title":"get_exporter_or_none","text":"<pre><code>get_exporter_or_none() -&gt; Optional[BatchEvaluationExporter]\n</code></pre> <p>Get the batch evaluation exporter or None if context is not initialized.</p> <p>Returns:</p> Type Description <code>Optional[BatchEvaluationExporter]</code> <p>The batch evaluation exporter if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_exporter_or_none() -&gt; Optional[\"BatchEvaluationExporter\"]:\n    \"\"\"\n    Get the batch evaluation exporter or None if context is not initialized.\n\n    Returns:\n        The batch evaluation exporter if context is available, otherwise None.\n    \"\"\"\n    return (ctx := get_current_context_or_none()) and ctx.exporter\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_scope","title":"get_scope","text":"<pre><code>get_scope(ctx: Optional[PatronusContext] = None) -&gt; PatronusScope\n</code></pre> <p>Get the Patronus scope.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <p>Returns:</p> Type Description <code>PatronusScope</code> <p>The Patronus scope.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_scope(ctx: Optional[PatronusContext] = None) -&gt; PatronusScope:\n    \"\"\"\n    Get the Patronus scope.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n\n    Returns:\n        The Patronus scope.\n    \"\"\"\n    ctx = ctx or get_current_context()\n    return ctx.scope\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_scope_or_none","title":"get_scope_or_none","text":"<pre><code>get_scope_or_none() -&gt; Optional[PatronusScope]\n</code></pre> <p>Get the Patronus scope or None if context is not initialized.</p> <p>Returns:</p> Type Description <code>Optional[PatronusScope]</code> <p>The Patronus scope if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_scope_or_none() -&gt; Optional[PatronusScope]:\n    \"\"\"\n    Get the Patronus scope or None if context is not initialized.\n\n    Returns:\n        The Patronus scope if context is available, otherwise None.\n    \"\"\"\n    return (ctx := get_current_context_or_none()) and ctx.scope\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_prompts_config","title":"get_prompts_config","text":"<pre><code>get_prompts_config(ctx: Optional[PatronusContext] = None) -&gt; PromptsConfig\n</code></pre> <p>Get the Patronus prompts configuration.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Optional[PatronusContext]</code> <p>The Patronus context to use. If None, uses the current context.</p> <code>None</code> <p>Returns:</p> Type Description <code>PromptsConfig</code> <p>The Patronus prompts configuration.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_prompts_config(ctx: Optional[PatronusContext] = None) -&gt; PromptsConfig:\n    \"\"\"\n    Get the Patronus prompts configuration.\n\n    Args:\n        ctx: The Patronus context to use. If None, uses the current context.\n\n    Returns:\n        The Patronus prompts configuration.\n    \"\"\"\n    ctx = ctx or get_current_context()\n    return ctx.prompts\n</code></pre>"},{"location":"api_ref/context/#patronus.context.get_prompts_config_or_none","title":"get_prompts_config_or_none","text":"<pre><code>get_prompts_config_or_none() -&gt; Optional[PromptsConfig]\n</code></pre> <p>Get the Patronus prompts configuration or None if context is not initialized.</p> <p>Returns:</p> Type Description <code>Optional[PromptsConfig]</code> <p>The Patronus prompts configuration if context is available, otherwise None.</p> Source code in <code>src/patronus/context/__init__.py</code> <pre><code>def get_prompts_config_or_none() -&gt; Optional[PromptsConfig]:\n    \"\"\"\n    Get the Patronus prompts configuration or None if context is not initialized.\n\n    Returns:\n        The Patronus prompts configuration if context is available, otherwise None.\n    \"\"\"\n    return (ctx := get_current_context_or_none()) and ctx.prompts\n</code></pre>"},{"location":"api_ref/datasets/","title":"Datasets","text":""},{"location":"api_ref/datasets/#patronus.datasets","title":"patronus.datasets","text":""},{"location":"api_ref/datasets/#patronus.datasets.datasets","title":"datasets","text":""},{"location":"api_ref/datasets/#patronus.datasets.datasets.Attachment","title":"Attachment","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represent an attachment entry. Usually used in context of multimodal evaluation.</p>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.Fields","title":"Fields","text":"<p>               Bases: <code>TypedDict</code></p> <p>A TypedDict class representing fields for a structured data entity.</p> <p>Attributes:</p> Name Type Description <code>sid</code> <code>NotRequired[Optional[str]]</code> <p>An optional identifier for the system or session.</p> <code>system_prompt</code> <code>NotRequired[Optional[str]]</code> <p>An optional string representing the system prompt associated with the task.</p> <code>task_context</code> <code>NotRequired[Union[str, list[str], None]]</code> <p>Optional contextual information for the task in the form of a string or a list of strings.</p> <code>task_attachments</code> <code>NotRequired[Optional[list[Attachment]]]</code> <p>Optional list of attachments associated with the task.</p> <code>task_input</code> <code>NotRequired[Optional[str]]</code> <p>An optional string representing the input data for the task. Usually a user input sent to an LLM.</p> <code>task_output</code> <code>NotRequired[Optional[str]]</code> <p>An optional string representing the output result of the task. Usually a response from an LLM.</p> <code>gold_answer</code> <code>NotRequired[Optional[str]]</code> <p>An optional string representing the correct or expected answer for evaluation purposes.</p> <code>task_metadata</code> <code>NotRequired[Optional[dict[str, Any]]]</code> <p>Optional dictionary containing metadata associated with the task.</p> <code>tags</code> <code>NotRequired[Optional[dict[str, str]]]</code> <p>Optional dictionary holding additional key-value pair tags relevant to the task.</p>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.Row","title":"Row  <code>dataclass</code>","text":"<pre><code>Row(_row: Series)\n</code></pre> <p>               Bases: <code>LogSerializer</code></p> <p>Represents a data row encapsulating access to properties in a pandas Series.</p> <p>Provides attribute-based access to underlying pandas Series data with properties that ensure compatibility with structured evaluators through consistent field naming and type handling.</p>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.Row.dump_as_log","title":"dump_as_log","text":"<pre><code>dump_as_log() -&gt; dict[str, Any]\n</code></pre> <p>Serialize the Row into a dictionary format suitable for logging.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing all available row fields for logging, excluding None values.</p> Source code in <code>src/patronus/datasets/datasets.py</code> <pre><code>def dump_as_log(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize the Row into a dictionary format suitable for logging.\n\n    Returns:\n        A dictionary containing all available row fields for logging, excluding None values.\n    \"\"\"\n    return self._row.to_dict()\n</code></pre>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.Dataset","title":"Dataset  <code>dataclass</code>","text":"<pre><code>Dataset(dataset_id: Optional[str], df: DataFrame)\n</code></pre> <p>Represents a dataset.</p>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.Dataset.from_records","title":"from_records  <code>classmethod</code>","text":"<pre><code>from_records(records: Union[Iterable[Fields], Iterable[dict[str, Any]]], dataset_id: Optional[str] = None) -&gt; te.Self\n</code></pre> <p>Creates an instance of the class by processing and sanitizing provided records and optionally associating them with a specific dataset ID.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>Union[Iterable[Fields], Iterable[dict[str, Any]]]</code> <p>A collection of records to initialize the instance. Each record can either be an instance of <code>Fields</code> or a dictionary containing corresponding data.</p> required <code>dataset_id</code> <code>Optional[str]</code> <p>An optional identifier for associating the data with a specific dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>te.Self: A new instance of the class with the processed and sanitized data.</p> Source code in <code>src/patronus/datasets/datasets.py</code> <pre><code>@classmethod\ndef from_records(\n    cls,\n    records: Union[typing.Iterable[Fields], typing.Iterable[dict[str, typing.Any]]],\n    dataset_id: Optional[str] = None,\n) -&gt; te.Self:\n    \"\"\"\n    Creates an instance of the class by processing and sanitizing provided records\n    and optionally associating them with a specific dataset ID.\n\n    Args:\n        records:\n            A collection of records to initialize the instance. Each record can either\n            be an instance of `Fields` or a dictionary containing corresponding data.\n        dataset_id:\n            An optional identifier for associating the data with a specific dataset.\n\n    Returns:\n        te.Self: A new instance of the class with the processed and sanitized data.\n    \"\"\"\n    df = pd.DataFrame.from_records(records)\n    df = cls.__sanitize_df(df, dataset_id)\n    return cls(df=df, dataset_id=dataset_id)\n</code></pre>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.Dataset.to_csv","title":"to_csv","text":"<pre><code>to_csv(path_or_buf: Union[str, Path, IO[AnyStr]], **kwargs: Any) -&gt; Optional[str]\n</code></pre> <p>Saves dataset to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_buf</code> <code>Union[str, Path, IO[AnyStr]]</code> <p>String path or file-like object where the CSV will be saved.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to pandas.DataFrame.to_csv().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>String path if a path was specified and return_path is True, otherwise None.</p> Source code in <code>src/patronus/datasets/datasets.py</code> <pre><code>def to_csv(\n    self, path_or_buf: Union[str, pathlib.Path, typing.IO[typing.AnyStr]], **kwargs: typing.Any\n) -&gt; Optional[str]:\n    \"\"\"\n    Saves dataset to a CSV file.\n\n    Args:\n        path_or_buf: String path or file-like object where the CSV will be saved.\n        **kwargs: Additional arguments passed to pandas.DataFrame.to_csv().\n\n    Returns:\n        String path if a path was specified and return_path is True, otherwise None.\n    \"\"\"\n    return self.df.to_csv(path_or_buf, **kwargs)\n</code></pre>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.DatasetLoader","title":"DatasetLoader","text":"<pre><code>DatasetLoader(loader: Union[Awaitable[Dataset], Callable[[], Awaitable[Dataset]]])\n</code></pre> <p>Encapsulates asynchronous loading of a dataset.</p> <p>This class provides a mechanism to lazily load a dataset asynchronously only once, using a provided dataset loader function.</p> Source code in <code>src/patronus/datasets/datasets.py</code> <pre><code>def __init__(self, loader: Union[typing.Awaitable[Dataset], typing.Callable[[], typing.Awaitable[Dataset]]]):\n    self.__lock = asyncio.Lock()\n    self.__loader = loader\n    self.dataset: Optional[Dataset] = None\n</code></pre>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.DatasetLoader.load","title":"load  <code>async</code>","text":"<pre><code>load() -&gt; Dataset\n</code></pre> <p>Load dataset. Repeated calls will return already loaded dataset.</p> Source code in <code>src/patronus/datasets/datasets.py</code> <pre><code>async def load(self) -&gt; Dataset:\n    \"\"\"\n    Load dataset. Repeated calls will return already loaded dataset.\n    \"\"\"\n    async with self.__lock:\n        if self.dataset is not None:\n            return self.dataset\n        if inspect.iscoroutinefunction(self.__loader):\n            self.dataset = await self.__loader()\n        else:\n            self.dataset = await self.__loader\n        return self.dataset\n</code></pre>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.read_csv","title":"read_csv","text":"<pre><code>read_csv(filename_or_buffer: Union[str, Path, IO[AnyStr]], *, dataset_id: Optional[str] = None, sid_field: str = 'sid', system_prompt_field: str = 'system_prompt', task_input_field: str = 'task_input', task_context_field: str = 'task_context', task_attachments_field: str = 'task_attachments', task_output_field: str = 'task_output', gold_answer_field: str = 'gold_answer', task_metadata_field: str = 'task_metadata', tags_field: str = 'tags', **kwargs: Any) -&gt; Dataset\n</code></pre> <p>Reads a CSV file and converts it into a Dataset object. The CSV file is transformed into a structured dataset where each field maps to a specific aspect of the dataset schema provided via function arguments. You may specify custom field mappings as per your dataset structure, while additional keyword arguments are passed directly to the underlying 'pd.read_csv' function.</p> <p>Parameters:</p> Name Type Description Default <code>filename_or_buffer</code> <code>Union[str, Path, IO[AnyStr]]</code> <p>Path to the CSV file or a file-like object containing the dataset to be read.</p> required <code>dataset_id</code> <code>Optional[str]</code> <p>Optional identifier for the dataset being read. Default is None.</p> <code>None</code> <code>sid_field</code> <code>str</code> <p>Name of the column containing unique sample identifiers.</p> <code>'sid'</code> <code>system_prompt_field</code> <code>str</code> <p>Name of the column representing the system prompts.</p> <code>'system_prompt'</code> <code>task_input_field</code> <code>str</code> <p>Name of the column containing the main input for the task.</p> <code>'task_input'</code> <code>task_context_field</code> <code>str</code> <p>Name of the column describing the broader task context.</p> <code>'task_context'</code> <code>task_attachments_field</code> <code>str</code> <p>Name of the column with supplementary attachments related to the task.</p> <code>'task_attachments'</code> <code>task_output_field</code> <code>str</code> <p>Name of the column containing responses or outputs for the task.</p> <code>'task_output'</code> <code>gold_answer_field</code> <code>str</code> <p>Name of the column detailing the expected or correct answer to the task.</p> <code>'gold_answer'</code> <code>task_metadata_field</code> <code>str</code> <p>Name of the column storing metadata attributes associated with the task.</p> <code>'task_metadata'</code> <code>tags_field</code> <code>str</code> <p>Name of the column containing tags or annotations related to each sample.</p> <code>'tags'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to 'pandas.read_csv' for fine-tuning the CSV parsing behavior, such as delimiters, encoding, etc.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>The parsed dataset object containing structured data from the input CSV file.</p> Source code in <code>src/patronus/datasets/datasets.py</code> <pre><code>def read_csv(\n    filename_or_buffer: Union[str, pathlib.Path, typing.IO[typing.AnyStr]],\n    *,\n    dataset_id: Optional[str] = None,\n    sid_field: str = \"sid\",\n    system_prompt_field: str = \"system_prompt\",\n    task_input_field: str = \"task_input\",\n    task_context_field: str = \"task_context\",\n    task_attachments_field: str = \"task_attachments\",\n    task_output_field: str = \"task_output\",\n    gold_answer_field: str = \"gold_answer\",\n    task_metadata_field: str = \"task_metadata\",\n    tags_field: str = \"tags\",\n    **kwargs: typing.Any,\n) -&gt; Dataset:\n    \"\"\"\n    Reads a CSV file and converts it into a Dataset object. The CSV file is transformed\n    into a structured dataset where each field maps to a specific aspect of the dataset\n    schema provided via function arguments. You may specify custom field mappings as per\n    your dataset structure, while additional keyword arguments are passed directly to the\n    underlying 'pd.read_csv' function.\n\n    Args:\n        filename_or_buffer: Path to the CSV file or a file-like object containing the\n            dataset to be read.\n        dataset_id: Optional identifier for the dataset being read. Default is None.\n        sid_field: Name of the column containing unique sample identifiers.\n        system_prompt_field: Name of the column representing the system prompts.\n        task_input_field: Name of the column containing the main input for the task.\n        task_context_field: Name of the column describing the broader task context.\n        task_attachments_field: Name of the column with supplementary attachments\n            related to the task.\n        task_output_field: Name of the column containing responses or outputs for the\n            task.\n        gold_answer_field: Name of the column detailing the expected or correct\n            answer to the task.\n        task_metadata_field: Name of the column storing metadata attributes\n            associated with the task.\n        tags_field: Name of the column containing tags or annotations related to each\n            sample.\n        **kwargs: Additional keyword arguments passed to 'pandas.read_csv' for fine-tuning\n            the CSV parsing behavior, such as delimiters, encoding, etc.\n\n    Returns:\n        Dataset: The parsed dataset object containing structured data from the input\n            CSV file.\n    \"\"\"\n    return _read_dataframe(\n        pd.read_csv,\n        filename_or_buffer,\n        dataset_id=dataset_id,\n        sid_field=sid_field,\n        system_prompt_field=system_prompt_field,\n        task_context_field=task_context_field,\n        task_attachments_field=task_attachments_field,\n        task_input_field=task_input_field,\n        task_output_field=task_output_field,\n        gold_answer_field=gold_answer_field,\n        task_metadata_field=task_metadata_field,\n        tags_field=tags_field,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_ref/datasets/#patronus.datasets.datasets.read_jsonl","title":"read_jsonl","text":"<pre><code>read_jsonl(filename_or_buffer: Union[str, Path, IO[AnyStr]], *, dataset_id: Optional[str] = None, sid_field: str = 'sid', system_prompt_field: str = 'system_prompt', task_input_field: str = 'task_input', task_context_field: str = 'task_context', task_attachments_field: str = 'task_attachments', task_output_field: str = 'task_output', gold_answer_field: str = 'gold_answer', task_metadata_field: str = 'task_metadata', tags_field: str = 'tags', **kwargs: Any) -&gt; Dataset\n</code></pre> <p>Reads a JSONL (JSON Lines) file and transforms it into a Dataset object. This function parses the input data file or buffer in JSON Lines format into a structured format, extracting specified fields and additional metadata for usage in downstream tasks. The field mappings and additional keyword arguments can be customized to accommodate application-specific requirements.</p> <p>Parameters:</p> Name Type Description Default <code>filename_or_buffer</code> <code>Union[str, Path, IO[AnyStr]]</code> <p>The path to the file or a file-like object containing the JSONL data to be read.</p> required <code>dataset_id</code> <code>Optional[str]</code> <p>An optional identifier for the dataset being read. Defaults to None.</p> <code>None</code> <code>sid_field</code> <code>str</code> <p>The field name in the JSON lines representing the unique identifier for a sample. Defaults to \"sid\".</p> <code>'sid'</code> <code>system_prompt_field</code> <code>str</code> <p>The field name for the system prompt in the JSON lines file. Defaults to \"system_prompt\".</p> <code>'system_prompt'</code> <code>task_input_field</code> <code>str</code> <p>The field name for the task input data in the JSON lines file. Defaults to \"task_input\".</p> <code>'task_input'</code> <code>task_context_field</code> <code>str</code> <p>The field name for the task context data in the JSON lines file. Defaults to \"task_context\".</p> <code>'task_context'</code> <code>task_attachments_field</code> <code>str</code> <p>The field name for any task attachments in the JSON lines file. Defaults to \"task_attachments\".</p> <code>'task_attachments'</code> <code>task_output_field</code> <code>str</code> <p>The field name for task output data in the JSON lines file. Defaults to \"task_output\".</p> <code>'task_output'</code> <code>gold_answer_field</code> <code>str</code> <p>The field name for the gold (ground truth) answer in the JSON lines file. Defaults to \"gold_answer\".</p> <code>'gold_answer'</code> <code>task_metadata_field</code> <code>str</code> <p>The field name for metadata associated with the task in the JSON lines file. Defaults to \"task_metadata\".</p> <code>'task_metadata'</code> <code>tags_field</code> <code>str</code> <p>The field name for tags in the parsed JSON lines file. Defaults to \"tags\".</p> <code>'tags'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to <code>pd.read_json</code> for customization. The parameter \"lines\" will be forcibly set to True if not provided.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>A Dataset object containing the parsed and structured data.</p> Source code in <code>src/patronus/datasets/datasets.py</code> <pre><code>def read_jsonl(\n    filename_or_buffer: Union[str, pathlib.Path, typing.IO[typing.AnyStr]],\n    *,\n    dataset_id: Optional[str] = None,\n    sid_field: str = \"sid\",\n    system_prompt_field: str = \"system_prompt\",\n    task_input_field: str = \"task_input\",\n    task_context_field: str = \"task_context\",\n    task_attachments_field: str = \"task_attachments\",\n    task_output_field: str = \"task_output\",\n    gold_answer_field: str = \"gold_answer\",\n    task_metadata_field: str = \"task_metadata\",\n    tags_field: str = \"tags\",\n    **kwargs: typing.Any,\n) -&gt; Dataset:\n    \"\"\"\n    Reads a JSONL (JSON Lines) file and transforms it into a Dataset object. This function\n    parses the input data file or buffer in JSON Lines format into a structured format,\n    extracting specified fields and additional metadata for usage in downstream tasks. The\n    field mappings and additional keyword arguments can be customized to accommodate\n    application-specific requirements.\n\n    Args:\n        filename_or_buffer: The path to the file or a file-like object containing the JSONL\n            data to be read.\n        dataset_id: An optional identifier for the dataset being read. Defaults to None.\n        sid_field: The field name in the JSON lines representing the unique identifier for\n            a sample. Defaults to \"sid\".\n        system_prompt_field: The field name for the system prompt in the JSON lines file.\n            Defaults to \"system_prompt\".\n        task_input_field: The field name for the task input data in the JSON lines file.\n            Defaults to \"task_input\".\n        task_context_field: The field name for the task context data in the JSON lines file.\n            Defaults to \"task_context\".\n        task_attachments_field: The field name for any task attachments in the JSON lines\n            file. Defaults to \"task_attachments\".\n        task_output_field: The field name for task output data in the JSON lines file.\n            Defaults to \"task_output\".\n        gold_answer_field: The field name for the gold (ground truth) answer in the JSON\n            lines file. Defaults to \"gold_answer\".\n        task_metadata_field: The field name for metadata associated with the task in the\n            JSON lines file. Defaults to \"task_metadata\".\n        tags_field: The field name for tags in the parsed JSON lines file. Defaults to\n            \"tags\".\n        **kwargs: Additional keyword arguments to be passed to `pd.read_json` for\n            customization. The parameter \"lines\" will be forcibly set to True if not\n            provided.\n\n    Returns:\n        Dataset: A Dataset object containing the parsed and structured data.\n\n    \"\"\"\n    kwargs.setdefault(\"lines\", True)\n    return _read_dataframe(\n        pd.read_json,\n        filename_or_buffer,\n        dataset_id=dataset_id,\n        sid_field=sid_field,\n        system_prompt_field=system_prompt_field,\n        task_context_field=task_context_field,\n        task_attachments_field=task_attachments_field,\n        task_input_field=task_input_field,\n        task_output_field=task_output_field,\n        gold_answer_field=gold_answer_field,\n        task_metadata_field=task_metadata_field,\n        tags_field=tags_field,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_ref/datasets/#patronus.datasets.remote","title":"remote","text":""},{"location":"api_ref/datasets/#patronus.datasets.remote.DatasetNotFoundError","title":"DatasetNotFoundError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a dataset with the specified ID or name is not found</p>"},{"location":"api_ref/datasets/#patronus.datasets.remote.RemoteDatasetLoader","title":"RemoteDatasetLoader","text":"<pre><code>RemoteDatasetLoader(by_name: Optional[str] = None, *, by_id: Optional[str] = None)\n</code></pre> <p>               Bases: <code>DatasetLoader</code></p> <p>A loader for datasets stored remotely on the Patronus platform.</p> <p>This class provides functionality to asynchronously load a dataset from the remote API by its name or identifier, handling the fetch operation lazily and ensuring it's only performed once. You can specify either the dataset name or ID, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>by_name</code> <code>Optional[str]</code> <p>The name of the dataset to load.</p> <code>None</code> <code>by_id</code> <code>Optional[str]</code> <p>The ID of the dataset to load.</p> <code>None</code> Source code in <code>src/patronus/datasets/remote.py</code> <pre><code>def __init__(self, by_name: Optional[str] = None, *, by_id: Optional[str] = None):\n    \"\"\"\n    Initializes a new RemoteDatasetLoader instance.\n\n    Args:\n        by_name: The name of the dataset to load.\n        by_id: The ID of the dataset to load.\n    \"\"\"\n    if not (bool(by_name) ^ bool(by_id)):\n        raise ValueError(\"Either by_name or by_id must be provided, but not both.\")\n\n    self._dataset_name = by_name\n    self._dataset_id = by_id\n    super().__init__(self._load)\n</code></pre>"},{"location":"api_ref/evals/","title":"evals","text":""},{"location":"api_ref/evals/#patronus.evals","title":"patronus.evals","text":""},{"location":"api_ref/evals/#patronus.evals.evaluators","title":"evaluators","text":""},{"location":"api_ref/evals/#patronus.evals.evaluators.Evaluator","title":"Evaluator","text":"<pre><code>Evaluator(weight: Optional[Union[str, float]] = None)\n</code></pre> <p>Base Evaluator Class</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(self, weight: Optional[Union[str, float]] = None):\n    if weight is not None:\n        try:\n            decimal.Decimal(str(weight))\n        except (decimal.InvalidOperation, ValueError, TypeError):\n            raise TypeError(\n                f\"{weight} is not a valid weight. Weight must be a valid decimal number (string or float).\"\n            )\n    self.weight = weight\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.Evaluator.evaluate","title":"evaluate  <code>abstractmethod</code>","text":"<pre><code>evaluate(*args, **kwargs) -&gt; Optional[EvaluationResult]\n</code></pre> <p>Synchronous version of evaluate method. When inheriting directly from Evaluator class it's permitted to change parameters signature. Return type should stay unchanged.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>@abc.abstractmethod\ndef evaluate(self, *args, **kwargs) -&gt; Optional[EvaluationResult]:\n    \"\"\"\n    Synchronous version of evaluate method.\n    When inheriting directly from Evaluator class it's permitted to change parameters signature.\n    Return type should stay unchanged.\n    \"\"\"\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncEvaluator","title":"AsyncEvaluator","text":"<pre><code>AsyncEvaluator(weight: Optional[Union[str, float]] = None)\n</code></pre> <p>               Bases: <code>Evaluator</code></p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(self, weight: Optional[Union[str, float]] = None):\n    if weight is not None:\n        try:\n            decimal.Decimal(str(weight))\n        except (decimal.InvalidOperation, ValueError, TypeError):\n            raise TypeError(\n                f\"{weight} is not a valid weight. Weight must be a valid decimal number (string or float).\"\n            )\n    self.weight = weight\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncEvaluator.evaluate","title":"evaluate  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>evaluate(*args, **kwargs) -&gt; Optional[EvaluationResult]\n</code></pre> <p>Asynchronous version of evaluate method. When inheriting directly from Evaluator class it's permitted to change parameters signature. Return type should stay unchanged.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>@abc.abstractmethod\nasync def evaluate(self, *args, **kwargs) -&gt; Optional[EvaluationResult]:\n    \"\"\"\n    Asynchronous version of evaluate method.\n    When inheriting directly from Evaluator class it's permitted to change parameters signature.\n    Return type should stay unchanged.\n    \"\"\"\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.StructuredEvaluator","title":"StructuredEvaluator","text":"<pre><code>StructuredEvaluator(weight: Optional[Union[str, float]] = None)\n</code></pre> <p>               Bases: <code>Evaluator</code></p> <p>Base for structured evaluators</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(self, weight: Optional[Union[str, float]] = None):\n    if weight is not None:\n        try:\n            decimal.Decimal(str(weight))\n        except (decimal.InvalidOperation, ValueError, TypeError):\n            raise TypeError(\n                f\"{weight} is not a valid weight. Weight must be a valid decimal number (string or float).\"\n            )\n    self.weight = weight\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncStructuredEvaluator","title":"AsyncStructuredEvaluator","text":"<pre><code>AsyncStructuredEvaluator(weight: Optional[Union[str, float]] = None)\n</code></pre> <p>               Bases: <code>AsyncEvaluator</code></p> <p>Base for async structured evaluators</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(self, weight: Optional[Union[str, float]] = None):\n    if weight is not None:\n        try:\n            decimal.Decimal(str(weight))\n        except (decimal.InvalidOperation, ValueError, TypeError):\n            raise TypeError(\n                f\"{weight} is not a valid weight. Weight must be a valid decimal number (string or float).\"\n            )\n    self.weight = weight\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.RemoteEvaluatorMixin","title":"RemoteEvaluatorMixin","text":"<pre><code>RemoteEvaluatorMixin(evaluator_id_or_alias: str, criteria: Optional[str] = None, *, tags: Optional[dict[str, str]] = None, explain_strategy: Literal['never', 'on-fail', 'on-success', 'always'] = 'always', criteria_config: Optional[dict[str, Any]] = None, allow_update: bool = False, max_attempts: int = 3, api_: Optional[PatronusAPIClient] = None, weight: Optional[Union[str, float]] = None, retry_max_attempts: Optional[int] = 3, retry_initial_delay: Optional[int] = 1, retry_backoff_factor: Optional[int] = 2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>evaluator_id_or_alias</code> <code>str</code> <p>The ID or alias of the evaluator to use.</p> required <code>criteria</code> <code>Optional[str]</code> <p>The criteria name to use for evaluation. If not provided, the evaluator's default criteria will be used.</p> <code>None</code> <code>tags</code> <code>Optional[dict[str, str]]</code> <p>Optional tags to attach to evaluations.</p> <code>None</code> <code>explain_strategy</code> <code>Literal['never', 'on-fail', 'on-success', 'always']</code> <p>When to generate explanations for evaluations. Options are \"never\", \"on-fail\", \"on-success\", or \"always\".</p> <code>'always'</code> <code>criteria_config</code> <code>Optional[dict[str, Any]]</code> <p>Configuration for the criteria. (Currently unused)</p> <code>None</code> <code>allow_update</code> <code>bool</code> <p>Whether to allow updates. (Currently unused)</p> <code>False</code> <code>max_attempts</code> <code>int</code> <p>Maximum number of retry attempts. (Currently unused)</p> <code>3</code> <code>api_</code> <code>Optional[PatronusAPIClient]</code> <p>Optional API client instance. If not provided, will use the default client from context.</p> <code>None</code> <code>weight</code> <code>Optional[Union[str, float]]</code> <p>Optional weight for the evaluator. This is only used within the Patronus Experimentation Framework to indicate the relative importance of evaluators. Must be a valid decimal number (string or float). Weights are stored as experiment metadata and do not affect standalone evaluator usage.</p> <code>None</code> <code>retry_max_attempts</code> <code>Optional[int]</code> <p>Maximum number of retry attempts.</p> <code>3</code> <code>retry_initial_delay</code> <code>Optional[int]</code> <p>Initial delay before next retry.</p> <code>1</code> <code>retry_backoff_factor</code> <code>Optional[int]</code> <p>Delay factor between retry attempts.</p> <code>2</code> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(\n    self,\n    evaluator_id_or_alias: str,\n    criteria: Optional[str] = None,\n    *,\n    tags: Optional[dict[str, str]] = None,\n    explain_strategy: typing.Literal[\"never\", \"on-fail\", \"on-success\", \"always\"] = \"always\",\n    criteria_config: Optional[dict[str, typing.Any]] = None,\n    allow_update: bool = False,\n    max_attempts: int = 3,\n    api_: Optional[PatronusAPIClient] = None,\n    weight: Optional[Union[str, float]] = None,\n    retry_max_attempts: Optional[int] = 3,\n    retry_initial_delay: Optional[int] = 1,\n    retry_backoff_factor: Optional[int] = 2,\n):\n    \"\"\"Initialize a remote evaluator.\n\n    Args:\n        evaluator_id_or_alias: The ID or alias of the evaluator to use.\n        criteria: The criteria name to use for evaluation. If not provided,\n            the evaluator's default criteria will be used.\n        tags: Optional tags to attach to evaluations.\n        explain_strategy: When to generate explanations for evaluations.\n            Options are \"never\", \"on-fail\", \"on-success\", or \"always\".\n        criteria_config: Configuration for the criteria. (Currently unused)\n        allow_update: Whether to allow updates. (Currently unused)\n        max_attempts: Maximum number of retry attempts. (Currently unused)\n        api_: Optional API client instance. If not provided, will use the\n            default client from context.\n        weight: Optional weight for the evaluator. This is only used within\n            the Patronus Experimentation Framework to indicate the relative\n            importance of evaluators. Must be a valid decimal number (string\n            or float). Weights are stored as experiment metadata and do not\n            affect standalone evaluator usage.\n        retry_max_attempts: Maximum number of retry attempts.\n        retry_initial_delay: Initial delay before next retry.\n        retry_backoff_factor: Delay factor between retry attempts.\n    \"\"\"\n    self.evaluator_id_or_alias = evaluator_id_or_alias\n    self.evaluator_id = None\n    self.criteria = criteria\n    self.tags = tags or {}\n    self.explain_strategy = explain_strategy\n    self.criteria_config = criteria_config\n    self.allow_update = allow_update\n    self.max_attempts = max_attempts\n    self._api = api_\n    self._resolved = False\n    self.weight = weight\n    self._load_lock = threading.Lock()\n    self._async_load_lock = asyncio.Lock()\n    self.retry_max_attempts = retry_max_attempts\n    self.retry_initial_delay = retry_initial_delay\n    self.retry_backoff_factor = retry_backoff_factor\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.RemoteEvaluator","title":"RemoteEvaluator","text":"<pre><code>RemoteEvaluator(evaluator_id_or_alias: str, criteria: Optional[str] = None, *, tags: Optional[dict[str, str]] = None, explain_strategy: Literal['never', 'on-fail', 'on-success', 'always'] = 'always', criteria_config: Optional[dict[str, Any]] = None, allow_update: bool = False, max_attempts: int = 3, api_: Optional[PatronusAPIClient] = None, weight: Optional[Union[str, float]] = None, retry_max_attempts: Optional[int] = 3, retry_initial_delay: Optional[int] = 1, retry_backoff_factor: Optional[int] = 2)\n</code></pre> <p>               Bases: <code>RemoteEvaluatorMixin</code>, <code>StructuredEvaluator</code></p> <p>Synchronous remote evaluator</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(\n    self,\n    evaluator_id_or_alias: str,\n    criteria: Optional[str] = None,\n    *,\n    tags: Optional[dict[str, str]] = None,\n    explain_strategy: typing.Literal[\"never\", \"on-fail\", \"on-success\", \"always\"] = \"always\",\n    criteria_config: Optional[dict[str, typing.Any]] = None,\n    allow_update: bool = False,\n    max_attempts: int = 3,\n    api_: Optional[PatronusAPIClient] = None,\n    weight: Optional[Union[str, float]] = None,\n    retry_max_attempts: Optional[int] = 3,\n    retry_initial_delay: Optional[int] = 1,\n    retry_backoff_factor: Optional[int] = 2,\n):\n    \"\"\"Initialize a remote evaluator.\n\n    Args:\n        evaluator_id_or_alias: The ID or alias of the evaluator to use.\n        criteria: The criteria name to use for evaluation. If not provided,\n            the evaluator's default criteria will be used.\n        tags: Optional tags to attach to evaluations.\n        explain_strategy: When to generate explanations for evaluations.\n            Options are \"never\", \"on-fail\", \"on-success\", or \"always\".\n        criteria_config: Configuration for the criteria. (Currently unused)\n        allow_update: Whether to allow updates. (Currently unused)\n        max_attempts: Maximum number of retry attempts. (Currently unused)\n        api_: Optional API client instance. If not provided, will use the\n            default client from context.\n        weight: Optional weight for the evaluator. This is only used within\n            the Patronus Experimentation Framework to indicate the relative\n            importance of evaluators. Must be a valid decimal number (string\n            or float). Weights are stored as experiment metadata and do not\n            affect standalone evaluator usage.\n        retry_max_attempts: Maximum number of retry attempts.\n        retry_initial_delay: Initial delay before next retry.\n        retry_backoff_factor: Delay factor between retry attempts.\n    \"\"\"\n    self.evaluator_id_or_alias = evaluator_id_or_alias\n    self.evaluator_id = None\n    self.criteria = criteria\n    self.tags = tags or {}\n    self.explain_strategy = explain_strategy\n    self.criteria_config = criteria_config\n    self.allow_update = allow_update\n    self.max_attempts = max_attempts\n    self._api = api_\n    self._resolved = False\n    self.weight = weight\n    self._load_lock = threading.Lock()\n    self._async_load_lock = asyncio.Lock()\n    self.retry_max_attempts = retry_max_attempts\n    self.retry_initial_delay = retry_initial_delay\n    self.retry_backoff_factor = retry_backoff_factor\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.RemoteEvaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(*, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_attachments: Union[list[Any], None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -&gt; EvaluationResult\n</code></pre> <p>Evaluates data using remote Patronus Evaluator</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def evaluate(\n    self,\n    *,\n    system_prompt: Optional[str] = None,\n    task_context: Union[list[str], str, None] = None,\n    task_attachments: Union[list[Any], None] = None,\n    task_input: Optional[str] = None,\n    task_output: Optional[str] = None,\n    gold_answer: Optional[str] = None,\n    task_metadata: Optional[typing.Dict[str, typing.Any]] = None,\n    **kwargs: Any,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluates data using remote Patronus Evaluator\"\"\"\n    kws = {\n        \"system_prompt\": system_prompt,\n        \"task_context\": task_context,\n        \"task_attachments\": task_attachments,\n        \"task_input\": task_input,\n        \"task_output\": task_output,\n        \"gold_answer\": gold_answer,\n        \"task_metadata\": task_metadata,\n        **kwargs,\n    }\n    log_id = get_current_log_id(bound_arguments=kws)\n\n    attrs = get_context_evaluation_attributes()\n    tags = {**self.tags}\n    if t := attrs[\"tags\"]:\n        tags.update(t)\n    tags = merge_tags(tags, kwargs.get(\"tags\"), attrs[\"experiment_tags\"])\n    if tags:\n        kws[\"tags\"] = tags\n    if did := attrs[\"dataset_id\"]:\n        kws[\"dataset_id\"] = did\n    if sid := attrs[\"dataset_sample_id\"]:\n        kws[\"dataset_sample_id\"] = sid\n\n    resp = retry(\n        self.retry_max_attempts,\n        self.retry_initial_delay,\n        self.retry_backoff_factor,\n    )(self._evaluate)(log_id=log_id, **kws)\n    return self._translate_response(resp)\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncRemoteEvaluator","title":"AsyncRemoteEvaluator","text":"<pre><code>AsyncRemoteEvaluator(evaluator_id_or_alias: str, criteria: Optional[str] = None, *, tags: Optional[dict[str, str]] = None, explain_strategy: Literal['never', 'on-fail', 'on-success', 'always'] = 'always', criteria_config: Optional[dict[str, Any]] = None, allow_update: bool = False, max_attempts: int = 3, api_: Optional[PatronusAPIClient] = None, weight: Optional[Union[str, float]] = None, retry_max_attempts: Optional[int] = 3, retry_initial_delay: Optional[int] = 1, retry_backoff_factor: Optional[int] = 2)\n</code></pre> <p>               Bases: <code>RemoteEvaluatorMixin</code>, <code>AsyncStructuredEvaluator</code></p> <p>Asynchronous remote evaluator</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def __init__(\n    self,\n    evaluator_id_or_alias: str,\n    criteria: Optional[str] = None,\n    *,\n    tags: Optional[dict[str, str]] = None,\n    explain_strategy: typing.Literal[\"never\", \"on-fail\", \"on-success\", \"always\"] = \"always\",\n    criteria_config: Optional[dict[str, typing.Any]] = None,\n    allow_update: bool = False,\n    max_attempts: int = 3,\n    api_: Optional[PatronusAPIClient] = None,\n    weight: Optional[Union[str, float]] = None,\n    retry_max_attempts: Optional[int] = 3,\n    retry_initial_delay: Optional[int] = 1,\n    retry_backoff_factor: Optional[int] = 2,\n):\n    \"\"\"Initialize a remote evaluator.\n\n    Args:\n        evaluator_id_or_alias: The ID or alias of the evaluator to use.\n        criteria: The criteria name to use for evaluation. If not provided,\n            the evaluator's default criteria will be used.\n        tags: Optional tags to attach to evaluations.\n        explain_strategy: When to generate explanations for evaluations.\n            Options are \"never\", \"on-fail\", \"on-success\", or \"always\".\n        criteria_config: Configuration for the criteria. (Currently unused)\n        allow_update: Whether to allow updates. (Currently unused)\n        max_attempts: Maximum number of retry attempts. (Currently unused)\n        api_: Optional API client instance. If not provided, will use the\n            default client from context.\n        weight: Optional weight for the evaluator. This is only used within\n            the Patronus Experimentation Framework to indicate the relative\n            importance of evaluators. Must be a valid decimal number (string\n            or float). Weights are stored as experiment metadata and do not\n            affect standalone evaluator usage.\n        retry_max_attempts: Maximum number of retry attempts.\n        retry_initial_delay: Initial delay before next retry.\n        retry_backoff_factor: Delay factor between retry attempts.\n    \"\"\"\n    self.evaluator_id_or_alias = evaluator_id_or_alias\n    self.evaluator_id = None\n    self.criteria = criteria\n    self.tags = tags or {}\n    self.explain_strategy = explain_strategy\n    self.criteria_config = criteria_config\n    self.allow_update = allow_update\n    self.max_attempts = max_attempts\n    self._api = api_\n    self._resolved = False\n    self.weight = weight\n    self._load_lock = threading.Lock()\n    self._async_load_lock = asyncio.Lock()\n    self.retry_max_attempts = retry_max_attempts\n    self.retry_initial_delay = retry_initial_delay\n    self.retry_backoff_factor = retry_backoff_factor\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.AsyncRemoteEvaluator.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(*, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_attachments: Union[list[Any], None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -&gt; EvaluationResult\n</code></pre> <p>Evaluates data using remote Patronus Evaluator</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>async def evaluate(\n    self,\n    *,\n    system_prompt: Optional[str] = None,\n    task_context: Union[list[str], str, None] = None,\n    task_attachments: Union[list[Any], None] = None,\n    task_input: Optional[str] = None,\n    task_output: Optional[str] = None,\n    gold_answer: Optional[str] = None,\n    task_metadata: Optional[typing.Dict[str, typing.Any]] = None,\n    **kwargs: Any,\n) -&gt; EvaluationResult:\n    \"\"\"Evaluates data using remote Patronus Evaluator\"\"\"\n    kws = {\n        \"system_prompt\": system_prompt,\n        \"task_context\": task_context,\n        \"task_attachments\": task_attachments,\n        \"task_input\": task_input,\n        \"task_output\": task_output,\n        \"gold_answer\": gold_answer,\n        \"task_metadata\": task_metadata,\n        **kwargs,\n    }\n    log_id = get_current_log_id(bound_arguments=kws)\n\n    attrs = get_context_evaluation_attributes()\n    tags = {**self.tags}\n    if t := attrs[\"tags\"]:\n        tags.update(t)\n    tags = merge_tags(tags, kwargs.get(\"tags\"), attrs[\"experiment_tags\"])\n    if tags:\n        kws[\"tags\"] = tags\n    if did := attrs[\"dataset_id\"]:\n        kws[\"dataset_id\"] = did\n    if sid := attrs[\"dataset_sample_id\"]:\n        kws[\"dataset_sample_id\"] = sid\n\n    resp = await retry(\n        self.retry_max_attempts,\n        self.retry_initial_delay,\n        self.retry_backoff_factor,\n    )(self._evaluate)(log_id=log_id, **kws)\n    return self._translate_response(resp)\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.ensure_loading","title":"ensure_loading","text":"<pre><code>ensure_loading(func: Optional[Callable[..., Any]] = None)\n</code></pre> <p>Decorator that calls .load() on the decorated entity if the .load method exists. This ensures that remote evaluators are properly loaded before evaluation.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def ensure_loading(\n    func: Optional[typing.Callable[..., typing.Any]] = None,\n):\n    \"\"\"\n    Decorator that calls .load() on the decorated entity if the .load method exists.\n    This ensures that remote evaluators are properly loaded before evaluation.\n    \"\"\"\n\n    if func is None:\n        return ensure_loading()(func)\n\n    @functools.wraps(func)\n    def wrapper(self, *args, **kwargs):\n        if hasattr(self, 'load') and callable(getattr(self, 'load')) and not getattr(self, '_loaded', False):\n            self.load()\n        return func(self, *args, **kwargs)\n\n    @functools.wraps(func)\n    async def async_wrapper(self, *args, **kwargs):\n        if hasattr(self, 'load') and callable(getattr(self, 'load')) and not getattr(self, '_loaded', False):\n            await self.load()\n        return await func(self, *args, **kwargs)\n\n    if inspect.iscoroutinefunction(func):\n        return async_wrapper\n    else:\n        return wrapper\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.get_current_log_id","title":"get_current_log_id","text":"<pre><code>get_current_log_id(bound_arguments: dict[str, Any]) -&gt; Optional[LogID]\n</code></pre> <p>Return log_id for given arguments in current context. Returns None if there is no context - most likely SDK is not initialized.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def get_current_log_id(bound_arguments: dict[str, Any]) -&gt; Optional[LogID]:\n    \"\"\"\n    Return log_id for given arguments in current context.\n    Returns None if there is no context - most likely SDK is not initialized.\n    \"\"\"\n    eval_group = _ctx_evaluation_log_group.get(None)\n    if eval_group is None:\n        return None\n    log_id = eval_group.find_log(bound_arguments)\n    if log_id is None:\n        raise ValueError(\"Log not found for provided arguments\")\n    return log_id\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.bundled_eval","title":"bundled_eval","text":"<pre><code>bundled_eval(span_name: str = 'Evaluation bundle', attributes: Optional[dict[str, str]] = None)\n</code></pre> <p>Start a span that would automatically bundle evaluations.</p> <p>Evaluations are passed by arguments passed to the evaluators called inside the context manager.</p> <p>The following example would create two bundles:</p> <ul> <li>fist with arguments <code>x=10, y=20</code></li> <li>second with arguments <code>spam=\"abc123\"</code></li> </ul> <pre><code>with bundled_eval():\n    foo_evaluator(x=10, y=20)\n    bar_evaluator(x=10, y=20)\n    tar_evaluator(spam=\"abc123\")\n</code></pre> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>@contextlib.contextmanager\ndef bundled_eval(span_name: str = \"Evaluation bundle\", attributes: Optional[dict[str, str]] = None):\n    \"\"\"\n    Start a span that would automatically bundle evaluations.\n\n    Evaluations are passed by arguments passed to the evaluators called inside the context manager.\n\n    The following example would create two bundles:\n\n    - fist with arguments `x=10, y=20`\n    - second with arguments `spam=\"abc123\"`\n\n    ```python\n    with bundled_eval():\n        foo_evaluator(x=10, y=20)\n        bar_evaluator(x=10, y=20)\n        tar_evaluator(spam=\"abc123\")\n    ```\n\n    \"\"\"\n    tracer = context.get_tracer_or_none()\n    if tracer is None:\n        yield\n        return\n\n    attributes = {\n        **(attributes or {}),\n        Attributes.span_type.value: SpanTypes.eval.value,\n    }\n    with tracer.start_as_current_span(span_name, attributes=attributes):\n        with _start_evaluation_log_group():\n            yield\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.evaluators.evaluator","title":"evaluator","text":"<pre><code>evaluator(_fn: Optional[Callable[..., Any]] = None, *, evaluator_id: Union[str, Callable[[], str], None] = None, criteria: Union[str, Callable[[], str], None] = None, metric_name: Optional[str] = None, metric_description: Optional[str] = None, is_method: bool = False, span_name: Optional[str] = None, log_none_arguments: bool = False, **kwargs: Any) -&gt; typing.Callable[..., typing.Any]\n</code></pre> <p>Decorator for creating functional-style evaluators that log execution and results.</p> <p>This decorator works with both synchronous and asynchronous functions. The decorator doesn't modify the function's return value, but records it after converting to an EvaluationResult.</p> <p>Evaluators can return different types which are automatically converted to <code>EvaluationResult</code> objects:</p> <ul> <li><code>bool</code>: <code>True</code>/<code>False</code> indicating pass/fail.</li> <li><code>float</code>/<code>int</code>: Numerical scores (typically between 0-1).</li> <li><code>str</code>: Text output categorizing the result.</li> <li>EvaluationResult: Complete evaluation with scores, explanations, etc.</li> <li><code>None</code>: Indicates evaluation was skipped and no result will be recorded.</li> </ul> <p>Evaluation results are exported in the background without blocking execution. The SDK must be initialized with <code>patronus.init()</code> for evaluations to be recorded, though decorated functions will still execute even without initialization.</p> <p>The evaluator integrates with a context-based system to identify and handle shared evaluation logging and tracing spans.</p> <p>Example:</p> <pre><code>from patronus import init, evaluator\nfrom patronus.evals import EvaluationResult\n\n# Initialize the SDK to record evaluations\ninit()\n\n# Simple evaluator function\n@evaluator()\ndef exact_match(actual: str, expected: str) -&gt; bool:\n    return actual.strip() == expected.strip()\n\n# More complex evaluator with detailed result\n@evaluator()\ndef semantic_match(actual: str, expected: str) -&gt; EvaluationResult:\n    similarity = calculate_similarity(actual, expected)  # Your similarity function\n    return EvaluationResult(\n        score=similarity,\n        pass_=similarity &gt; 0.8,\n        text_output=\"High similarity\" if similarity &gt; 0.8 else \"Low similarity\",\n        explanation=f\"Calculated similarity: {similarity}\"\n    )\n\n# Use the evaluators\nresult = exact_match(\"Hello world\", \"Hello world\")\nprint(f\"Match: {result}\")  # Output: Match: True\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>_fn</code> <code>Optional[Callable[..., Any]]</code> <p>The function to be decorated.</p> <code>None</code> <code>evaluator_id</code> <code>Union[str, Callable[[], str], None]</code> <p>Name for the evaluator. Defaults to function name (or class name in case of class based evaluators).</p> <code>None</code> <code>criteria</code> <code>Union[str, Callable[[], str], None]</code> <p>Name of the criteria used by the evaluator. The use of the criteria is only recommended in more complex evaluator setups where evaluation algorithm changes depending on a criteria (think strategy pattern).</p> <code>None</code> <code>metric_name</code> <code>Optional[str]</code> <p>Name for the evaluation metric. Defaults to evaluator_id value.</p> <code>None</code> <code>metric_description</code> <code>Optional[str]</code> <p>The description of the metric used for evaluation. If not provided then the docstring of the wrapped function is used for this value.</p> <code>None</code> <code>is_method</code> <code>bool</code> <p>Whether the wrapped function is a method. This value is used to determine whether to remove \"self\" argument from the log. It also allows for dynamic evaluator_id and criteria discovery based on <code>get_evaluator_id()</code> and <code>get_criteria_id()</code> methods. User-code usually shouldn't use it as long as user defined class-based evaluators inherit from the library provided Evaluator base classes.</p> <code>False</code> <code>span_name</code> <code>Optional[str]</code> <p>Name of the span to represent this evaluation in the tracing system. Defaults to None, in which case a default name is generated based on the evaluator.</p> <code>None</code> <code>log_none_arguments</code> <code>bool</code> <p>Controls whether arguments with None values are included in log output. This setting affects only logging behavior and has no impact on function execution. Note: Only applies to top-level arguments. For nested structures like dictionaries, None values will always be logged regardless of this setting.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that may be passed to the decorator or its internal methods.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[..., Any]</code> <p>Returns the decorated function with additional evaluation behavior, suitable for synchronous or asynchronous usage.</p> Note <p>For evaluations that need to be compatible with experiments, consider using StructuredEvaluator or AsyncStructuredEvaluator classes instead.</p> Source code in <code>src/patronus/evals/evaluators.py</code> <pre><code>def evaluator(\n    _fn: Optional[typing.Callable[..., typing.Any]] = None,\n    *,\n    evaluator_id: Union[str, typing.Callable[[], str], None] = None,\n    criteria: Union[str, typing.Callable[[], str], None] = None,\n    metric_name: Optional[str] = None,\n    metric_description: Optional[str] = None,\n    is_method: bool = False,\n    span_name: Optional[str] = None,\n    log_none_arguments: bool = False,\n    **kwargs: typing.Any,\n) -&gt; typing.Callable[..., typing.Any]:\n    \"\"\"\n    Decorator for creating functional-style evaluators that log execution and results.\n\n    This decorator works with both synchronous and asynchronous functions. The decorator doesn't\n    modify the function's return value, but records it after converting to an EvaluationResult.\n\n    Evaluators can return different types which are automatically converted to `EvaluationResult` objects:\n\n    * `bool`: `True`/`False` indicating pass/fail.\n    * `float`/`int`: Numerical scores (typically between 0-1).\n    * `str`: Text output categorizing the result.\n    * [EvaluationResult][patronus.evals.types.EvaluationResult]: Complete evaluation with scores, explanations, etc.\n    * `None`: Indicates evaluation was skipped and no result will be recorded.\n\n    Evaluation results are exported in the background without blocking execution. The SDK must be\n    initialized with `patronus.init()` for evaluations to be recorded, though decorated functions\n    will still execute even without initialization.\n\n    The evaluator integrates with a context-based system to identify and handle shared evaluation\n    logging and tracing spans.\n\n    **Example:**\n\n    ```python\n    from patronus import init, evaluator\n    from patronus.evals import EvaluationResult\n\n    # Initialize the SDK to record evaluations\n    init()\n\n    # Simple evaluator function\n    @evaluator()\n    def exact_match(actual: str, expected: str) -&gt; bool:\n        return actual.strip() == expected.strip()\n\n    # More complex evaluator with detailed result\n    @evaluator()\n    def semantic_match(actual: str, expected: str) -&gt; EvaluationResult:\n        similarity = calculate_similarity(actual, expected)  # Your similarity function\n        return EvaluationResult(\n            score=similarity,\n            pass_=similarity &gt; 0.8,\n            text_output=\"High similarity\" if similarity &gt; 0.8 else \"Low similarity\",\n            explanation=f\"Calculated similarity: {similarity}\"\n        )\n\n    # Use the evaluators\n    result = exact_match(\"Hello world\", \"Hello world\")\n    print(f\"Match: {result}\")  # Output: Match: True\n    ```\n\n    Args:\n        _fn: The function to be decorated.\n        evaluator_id: Name for the evaluator.\n            Defaults to function name (or class name in case of class based evaluators).\n        criteria: Name of the criteria used by the evaluator.\n            The use of the criteria is only recommended in more complex evaluator setups\n            where evaluation algorithm changes depending on a criteria (think strategy pattern).\n        metric_name: Name for the evaluation metric. Defaults to evaluator_id value.\n        metric_description: The description of the metric used for evaluation.\n            If not provided then the docstring of the wrapped function is used for this value.\n        is_method: Whether the wrapped function is a method.\n            This value is used to determine whether to remove \"self\" argument from the log.\n            It also allows for dynamic evaluator_id and criteria discovery\n            based on `get_evaluator_id()` and `get_criteria_id()` methods.\n            User-code usually shouldn't use it as long as user defined class-based evaluators inherit from\n            the library provided Evaluator base classes.\n        span_name: Name of the span to represent this evaluation in the tracing system.\n            Defaults to None, in which case a default name is generated based on the evaluator.\n        log_none_arguments: Controls whether arguments with None values are included in log output.\n            This setting affects only logging behavior and has no impact on function execution.\n            Note: Only applies to top-level arguments. For nested structures like dictionaries,\n            None values will always be logged regardless of this setting.\n        **kwargs: Additional keyword arguments that may be passed to the decorator or its internal methods.\n\n    Returns:\n        Callable: Returns the decorated function with additional evaluation behavior, suitable for\n            synchronous or asynchronous usage.\n\n    Note:\n        For evaluations that need to be compatible with experiments, consider using\n        [StructuredEvaluator][patronus.evals.evaluators.StructuredEvaluator] or\n        [AsyncStructuredEvaluator][patronus.evals.evaluators.AsyncStructuredEvaluator] classes instead.\n\n    \"\"\"\n    if _fn is not None:\n        return evaluator()(_fn)\n\n    def decorator(fn):\n        fn_sign = inspect.signature(fn)\n\n        def _get_eval_id():\n            return (callable(evaluator_id) and evaluator_id()) or evaluator_id or fn.__name__\n\n        def _get_criteria():\n            return (callable(criteria) and criteria()) or criteria or None\n\n        def _prep(*fn_args, **fn_kwargs):\n            bound_args = fn_sign.bind(*fn_args, **fn_kwargs)\n            arguments_to_log = _as_applied_argument(fn_sign, bound_args)\n            bound_args.apply_defaults()\n            self_key_name = None\n            instance = None\n            if is_method:\n                self_key_name = next(iter(fn_sign.parameters.keys()))\n                instance = bound_args.arguments[self_key_name]\n\n            eval_id = None\n            eval_criteria = None\n            if isinstance(instance, Evaluator):\n                eval_id = instance.get_evaluator_id()\n                eval_criteria = instance.get_criteria()\n\n            if eval_id is None:\n                eval_id = _get_eval_id()\n            if eval_criteria is None:\n                eval_criteria = _get_criteria()\n\n            met_name = metric_name or eval_id\n            met_description = metric_description or inspect.getdoc(fn) or None\n\n            disable_export = isinstance(instance, RemoteEvaluatorMixin) and instance._disable_export\n\n            return PrepEval(\n                span_name=span_name,\n                evaluator_id=eval_id,\n                criteria=eval_criteria,\n                metric_name=met_name,\n                metric_description=met_description,\n                self_key_name=self_key_name,\n                arguments=arguments_to_log,\n                disable_export=disable_export,\n            )\n\n        attributes = {\n            Attributes.span_type.value: SpanTypes.eval.value,\n            GenAIAttributes.operation_name.value: OperationNames.eval.value,\n        }\n\n        @functools.wraps(fn)\n        async def wrapper_async(*fn_args, **fn_kwargs):\n            ctx = context.get_current_context_or_none()\n            if ctx is None:\n                return await fn(*fn_args, **fn_kwargs)\n\n            prep = _prep(*fn_args, **fn_kwargs)\n\n            start = time.perf_counter()\n            try:\n                with start_span(prep.display_name(), attributes=attributes):\n                    with _get_or_start_evaluation_log_group() as log_group:\n                        log_id = log_group.log(\n                            logger=context.get_pat_logger(ctx),\n                            is_method=is_method,\n                            self_key_name=prep.self_key_name,\n                            bound_arguments=prep.arguments,\n                            log_none_arguments=log_none_arguments,\n                        )\n                        ret = await fn(*fn_args, **fn_kwargs)\n            except Exception as e:\n                context.get_logger(ctx).exception(f\"Evaluator raised an exception: {e}\")\n                raise e\n            if prep.disable_export:\n                return ret\n            elapsed = time.perf_counter() - start\n            handle_eval_output(\n                ctx=ctx,\n                log_id=log_id,\n                evaluator_id=prep.evaluator_id,\n                criteria=prep.criteria,\n                metric_name=prep.metric_name,\n                metric_description=prep.metric_description,\n                ret_value=ret,\n                duration=datetime.timedelta(seconds=elapsed),\n                qualname=fn.__qualname__,\n            )\n            return ret\n\n        @functools.wraps(fn)\n        def wrapper_sync(*fn_args, **fn_kwargs):\n            ctx = context.get_current_context_or_none()\n            if ctx is None:\n                return fn(*fn_args, **fn_kwargs)\n\n            prep = _prep(*fn_args, **fn_kwargs)\n\n            start = time.perf_counter()\n            try:\n                with start_span(prep.display_name(), attributes=attributes):\n                    with _get_or_start_evaluation_log_group() as log_group:\n                        log_id = log_group.log(\n                            logger=context.get_pat_logger(ctx),\n                            is_method=is_method,\n                            self_key_name=prep.self_key_name,\n                            bound_arguments=prep.arguments,\n                            log_none_arguments=log_none_arguments,\n                        )\n                        ret = fn(*fn_args, **fn_kwargs)\n            except Exception as e:\n                context.get_logger(ctx).exception(f\"Evaluator raised an exception: {e}\")\n                raise e\n            if prep.disable_export:\n                return ret\n            elapsed = time.perf_counter() - start\n            handle_eval_output(\n                ctx=ctx,\n                log_id=log_id,\n                evaluator_id=prep.evaluator_id,\n                criteria=prep.criteria,\n                metric_name=prep.metric_name,\n                metric_description=prep.metric_description,\n                ret_value=ret,\n                duration=datetime.timedelta(seconds=elapsed),\n                qualname=fn.__qualname__,\n            )\n            return ret\n\n        def _set_attrs(wrapper: Any):\n            wrapper._pat_evaluator = True\n\n            # _pat_evaluator_id and _pat_criteria_id may be a bit misleading since\n            # may not be correct since actually values for evaluator_id and criteria\n            # are dynamically dispatched for class-based evaluators.\n            # These values will be correct for function evaluators though.\n            wrapper._pat_evaluator_id = _get_eval_id()\n            wrapper._pat_criteria = _get_criteria()\n\n        if inspect.iscoroutinefunction(fn):\n            _set_attrs(wrapper_async)\n            return wrapper_async\n        else:\n            _set_attrs(wrapper_sync)\n            return wrapper_sync\n\n    return decorator\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.types","title":"types","text":""},{"location":"api_ref/evals/#patronus.evals.types.EvaluationResult","title":"EvaluationResult","text":"<p>               Bases: <code>BaseModel</code>, <code>LogSerializer</code></p> <p>Container for evaluation outcomes including score, pass/fail status, explanations, and metadata.</p> <p>This class stores complete evaluation results with numeric scores, boolean pass/fail statuses, textual outputs, explanations, and arbitrary metadata. Evaluator functions can return instances of this class directly or return simpler types (bool, float, str) which will be automatically converted to EvaluationResult objects during recording.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>Optional[float]</code> <p>Score of the evaluation. Can be any numerical value, though typically ranges from 0 to 1, where 1 represents the best possible score.</p> <code>pass_</code> <code>Optional[bool]</code> <p>Whether the evaluation is considered to pass or fail.</p> <code>text_output</code> <code>Optional[str]</code> <p>Text output of the evaluation. Usually used for discrete human-readable category evaluation or as a label for score value.</p> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Arbitrary json-serializable metadata about evaluation.</p> <code>explanation</code> <code>Optional[str]</code> <p>Human-readable explanation of the evaluation.</p> <code>tags</code> <code>Optional[dict[str, str]]</code> <p>Key-value pair metadata.</p> <code>dataset_id</code> <code>Optional[str]</code> <p>ID of the dataset associated with evaluated sample.</p> <code>dataset_sample_id</code> <code>Optional[str]</code> <p>ID of the sample in a dataset associated with evaluated sample.</p> <code>evaluation_duration</code> <code>Optional[timedelta]</code> <p>Duration of the evaluation. In case value is not set, @evaluator decorator and Evaluator classes will set this value automatically.</p> <code>explanation_duration</code> <code>Optional[timedelta]</code> <p>Duration of the evaluation explanation.</p>"},{"location":"api_ref/evals/#patronus.evals.types.EvaluationResult.dump_as_log","title":"dump_as_log","text":"<pre><code>dump_as_log() -&gt; dict[str, Any]\n</code></pre> <p>Serialize the EvaluationResult into a dictionary format suitable for logging.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing all evaluation result fields, excluding None values.</p> Source code in <code>src/patronus/evals/types.py</code> <pre><code>def dump_as_log(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize the EvaluationResult into a dictionary format suitable for logging.\n\n    Returns:\n        A dictionary containing all evaluation result fields, excluding None values.\n    \"\"\"\n    return self.model_dump(mode='json')\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.types.EvaluationResult.format","title":"format","text":"<pre><code>format() -&gt; str\n</code></pre> <p>Format the evaluation result into a readable summary.</p> Source code in <code>src/patronus/evals/types.py</code> <pre><code>def format(self) -&gt; str:\n    \"\"\"\n    Format the evaluation result into a readable summary.\n    \"\"\"\n    md = self.model_dump(exclude_none=True, mode=\"json\")\n    return yaml.dump(md)\n</code></pre>"},{"location":"api_ref/evals/#patronus.evals.types.EvaluationResult.pretty_print","title":"pretty_print","text":"<pre><code>pretty_print(file=None) -&gt; None\n</code></pre> <p>Pretty prints the formatted content to the specified file or standard output.</p> Source code in <code>src/patronus/evals/types.py</code> <pre><code>def pretty_print(self, file=None) -&gt; None:\n    \"\"\"\n    Pretty prints the formatted content to the specified file or standard output.\n    \"\"\"\n    f = self.format()\n    print(f, file=file)\n</code></pre>"},{"location":"api_ref/experiments/","title":"experiments","text":""},{"location":"api_ref/experiments/#patronus.experiments","title":"patronus.experiments","text":""},{"location":"api_ref/experiments/#patronus.experiments.adapters","title":"adapters","text":""},{"location":"api_ref/experiments/#patronus.experiments.adapters.BaseEvaluatorAdapter","title":"BaseEvaluatorAdapter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all evaluator adapters.</p> <p>Evaluator adapters provide a standardized interface between the experiment framework and various types of evaluators (function-based, class-based, etc.).</p> <p>All concrete adapter implementations must inherit from this class and implement the required abstract methods.</p>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.EvaluatorAdapter","title":"EvaluatorAdapter","text":"<pre><code>EvaluatorAdapter(evaluator: Evaluator)\n</code></pre> <p>               Bases: <code>BaseEvaluatorAdapter</code></p> <p>Adapter for class-based evaluators conforming to the Evaluator or AsyncEvaluator protocol.</p> <p>This adapter enables the use of evaluator classes that implement either the Evaluator or AsyncEvaluator interface within the experiment framework.</p> <p>Attributes:</p> Name Type Description <code>evaluator</code> <code>Union[Evaluator, AsyncEvaluator]</code> <p>The evaluator instance to adapt.</p> <p>Examples:</p> <pre><code>import typing\nfrom typing import Optional\n\nfrom patronus import datasets\nfrom patronus.evals import Evaluator, EvaluationResult\nfrom patronus.experiments import run_experiment\nfrom patronus.experiments.adapters import EvaluatorAdapter\nfrom patronus.experiments.types import TaskResult, EvalParent\n\n\nclass MatchEvaluator(Evaluator):\n    def __init__(self, sanitizer=None):\n        if sanitizer is None:\n            sanitizer = lambda x: x\n        self.sanitizer = sanitizer\n\n    def evaluate(self, actual: str, expected: str) -&gt; EvaluationResult:\n        matched = self.sanitizer(actual) == self.sanitizer(expected)\n        return EvaluationResult(pass_=matched, score=int(matched))\n\n\nexact_match = MatchEvaluator()\nfuzzy_match = MatchEvaluator(lambda x: x.strip().lower())\n\n\nclass MatchAdapter(EvaluatorAdapter):\n    def __init__(self, evaluator: MatchEvaluator):\n        super().__init__(evaluator)\n\n    def transform(\n        self,\n        row: datasets.Row,\n        task_result: Optional[TaskResult],\n        parent: EvalParent,\n        **kwargs\n    ) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n        args = [row.task_output, row.gold_answer]\n        kwargs = {}\n        # Passing arguments via kwargs would also work in this case.\n        # kwargs = {\"actual\": row.task_output, \"expected\": row.gold_answer}\n        return args, kwargs\n\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string        \", \"gold_answer\": \"string\"}],\n    evaluators=[MatchAdapter(exact_match), MatchAdapter(fuzzy_match)],\n)\n</code></pre> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(self, evaluator: evals.Evaluator):\n    if not isinstance(evaluator, evals.Evaluator):\n        raise TypeError(f\"{evaluator} is not {evals.Evaluator.__name__}.\")\n    self.evaluator = evaluator\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.EvaluatorAdapter.transform","title":"transform","text":"<pre><code>transform(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs: Any) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]\n</code></pre> <p>Transform experiment framework arguments to evaluation method arguments.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>A list of positional arguments to pass to the evaluator function.</p> <code>dict[str, Any]</code> <p>A dictionary of keyword arguments to pass to the evaluator function.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def transform(\n    self,\n    row: datasets.Row,\n    task_result: Optional[TaskResult],\n    parent: EvalParent,\n    **kwargs: typing.Any,\n) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n    \"\"\"\n    Transform experiment framework arguments to evaluation method arguments.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        A list of positional arguments to pass to the evaluator function.\n        A dictionary of keyword arguments to pass to the evaluator function.\n    \"\"\"\n\n    return (\n        [],\n        {\"row\": row, \"task_result\": task_result, \"parent\": parent, **kwargs},\n    )\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.EvaluatorAdapter.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs: Any) -&gt; EvaluationResult\n</code></pre> <p>Evaluate the given row and task result using the adapted evaluator function.</p> <p>This method implements the BaseEvaluatorAdapter.evaluate() protocol.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>An EvaluationResult containing the evaluation outcome.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>async def evaluate(\n    self,\n    row: datasets.Row,\n    task_result: Optional[TaskResult],\n    parent: EvalParent,\n    **kwargs: typing.Any,\n) -&gt; EvaluationResult:\n    \"\"\"\n    Evaluate the given row and task result using the adapted evaluator function.\n\n    This method implements the BaseEvaluatorAdapter.evaluate() protocol.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        An EvaluationResult containing the evaluation outcome.\n    \"\"\"\n    ev_args, ev_kwargs = self.transform(row, task_result, parent, **kwargs)\n    return await self._evaluate(*ev_args, **ev_kwargs)\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.StructuredEvaluatorAdapter","title":"StructuredEvaluatorAdapter","text":"<pre><code>StructuredEvaluatorAdapter(evaluator: Union[StructuredEvaluator, AsyncStructuredEvaluator])\n</code></pre> <p>               Bases: <code>EvaluatorAdapter</code></p> <p>Adapter for structured evaluators.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(\n    self,\n    evaluator: Union[evals.StructuredEvaluator, evals.AsyncStructuredEvaluator],\n):\n    if not isinstance(evaluator, (evals.StructuredEvaluator, evals.AsyncStructuredEvaluator)):\n        raise TypeError(\n            f\"{type(evaluator)} is not \"\n            f\"{evals.AsyncStructuredEvaluator.__name__} nor {evals.StructuredEvaluator.__name__}.\"\n        )\n    super().__init__(evaluator)\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.FuncEvaluatorAdapter","title":"FuncEvaluatorAdapter","text":"<pre><code>FuncEvaluatorAdapter(fn: Callable[..., Any], weight: Optional[Union[str, float]] = None)\n</code></pre> <p>               Bases: <code>BaseEvaluatorAdapter</code></p> <p>Adapter class that allows using function-based evaluators with the experiment framework.</p> <p>This adapter serves as a bridge between function-based evaluators decorated with <code>@evaluator()</code> and the experiment framework's evaluation system. It handles both synchronous and asynchronous evaluator functions.</p> <p>Attributes:</p> Name Type Description <code>fn</code> <code>Callable</code> <p>The evaluator function to be adapted.</p> Notes <ul> <li>The function passed to this adapter must be decorated with <code>@evaluator()</code>.</li> <li>The adapter automatically handles the conversion between function results and proper   evaluation result objects.</li> </ul> <p>Examples:</p> <pre><code>Direct usage with a compatible evaluator function:\n\n```python\nfrom patronus import evaluator\nfrom patronus.experiments import FuncEvaluatorAdapter, run_experiment\nfrom patronus.datasets import Row\n\n\n@evaluator()\ndef exact_match(row: Row, **kwargs):\n    return row.task_output == row.gold_answer\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string\", \"gold_answer\": \"string\"}],\n    evaluators=[FuncEvaluatorAdapter(exact_match)]\n)\n```\n\nCustomized usage by overriding the `transform()` method:\n\n```python\nfrom typing import Optional\nimport typing\n\nfrom patronus import evaluator, datasets\nfrom patronus.experiments import FuncEvaluatorAdapter, run_experiment\nfrom patronus.experiments.types import TaskResult, EvalParent\n\n\n@evaluator()\ndef exact_match(actual, expected):\n    return actual == expected\n\n\nclass AdaptedExactMatch(FuncEvaluatorAdapter):\n    def __init__(self):\n        super().__init__(exact_match)\n\n    def transform(\n        self,\n        row: datasets.Row,\n        task_result: Optional[TaskResult],\n        parent: EvalParent,\n        **kwargs\n    ) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n        args = [row.task_output, row.gold_answer]\n        kwargs = {}\n\n        # Alternative: passing arguments via kwargs instead of args\n        # args = []\n        # kwargs = {\"actual\": row.task_output, \"expected\": row.gold_answer}\n\n        return args, kwargs\n\n\nrun_experiment(\n    dataset=[{\"task_output\": \"string\", \"gold_answer\": \"string\"}],\n    evaluators=[AdaptedExactMatch()],\n)\n```\n</code></pre> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def __init__(self, fn: typing.Callable[..., typing.Any], weight: Optional[Union[str, float]] = None):\n    if not hasattr(fn, \"_pat_evaluator\"):\n        raise ValueError(\n            f\"Passed function {fn.__qualname__} is not an evaluator. \"\n            \"Hint: add @evaluator decorator to the function.\"\n        )\n\n    if weight is not None:\n        try:\n            Decimal(str(weight))\n        except (decimal.InvalidOperation, ValueError, TypeError):\n            raise TypeError(\n                f\"{weight} is not a valid weight. Weight must be a valid decimal number (string or float).\"\n            )\n\n    self.fn = fn\n    self._weight = weight\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.FuncEvaluatorAdapter.transform","title":"transform","text":"<pre><code>transform(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs: Any) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]\n</code></pre> <p>Transform experiment framework parameters to evaluator function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>A list of positional arguments to pass to the evaluator function.</p> <code>dict[str, Any]</code> <p>A dictionary of keyword arguments to pass to the evaluator function.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>def transform(\n    self,\n    row: datasets.Row,\n    task_result: Optional[TaskResult],\n    parent: EvalParent,\n    **kwargs: typing.Any,\n) -&gt; tuple[list[typing.Any], dict[str, typing.Any]]:\n    \"\"\"\n    Transform experiment framework parameters to evaluator function parameters.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        A list of positional arguments to pass to the evaluator function.\n        A dictionary of keyword arguments to pass to the evaluator function.\n    \"\"\"\n\n    return (\n        [],\n        {\"row\": row, \"task_result\": task_result, \"parent\": parent, **kwargs},\n    )\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.adapters.FuncEvaluatorAdapter.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(row: Row, task_result: Optional[TaskResult], parent: EvalParent, **kwargs: Any) -&gt; EvaluationResult\n</code></pre> <p>Evaluate the given row and task result using the adapted evaluator function.</p> <p>This method implements the BaseEvaluatorAdapter.evaluate() protocol.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The data row being evaluated.</p> required <code>task_result</code> <code>Optional[TaskResult]</code> <p>The result of the task execution, if available.</p> required <code>parent</code> <code>EvalParent</code> <p>The parent evaluation context.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments from the experiment.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>An EvaluationResult containing the evaluation outcome.</p> Source code in <code>src/patronus/experiments/adapters.py</code> <pre><code>async def evaluate(\n    self,\n    row: datasets.Row,\n    task_result: Optional[TaskResult],\n    parent: EvalParent,\n    **kwargs: typing.Any,\n) -&gt; EvaluationResult:\n    \"\"\"\n    Evaluate the given row and task result using the adapted evaluator function.\n\n    This method implements the BaseEvaluatorAdapter.evaluate() protocol.\n\n    Args:\n        row: The data row being evaluated.\n        task_result: The result of the task execution, if available.\n        parent: The parent evaluation context.\n        **kwargs: Additional keyword arguments from the experiment.\n\n    Returns:\n        An EvaluationResult containing the evaluation outcome.\n    \"\"\"\n    ev_args, ev_kwargs = self.transform(row, task_result, parent, **kwargs)\n    return await self._evaluate(*ev_args, **ev_kwargs)\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment","title":"experiment","text":""},{"location":"api_ref/experiments/#patronus.experiments.experiment.Tags","title":"Tags  <code>module-attribute</code>","text":"<pre><code>Tags = dict[str, str]\n</code></pre> <p>Tags are key-value pairs applied to experiments, task results and evaluation results.</p>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.Task","title":"Task  <code>module-attribute</code>","text":"<pre><code>Task = Union[TaskProtocol[Union[TaskResult, str, None]], TaskProtocol[Awaitable[Union[TaskResult, str, None]]]]\n</code></pre> <p>A function that processes each dataset row and produces output for evaluation.</p>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.ExperimentDataset","title":"ExperimentDataset  <code>module-attribute</code>","text":"<pre><code>ExperimentDataset = Union[Dataset, DatasetLoader, list[dict[str, Any]], tuple[dict[str, Any], ...], DataFrame, Awaitable, Callable[[], Awaitable]]\n</code></pre> <p>Any object that would \"resolve\" into Dataset.</p>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.TaskProtocol","title":"TaskProtocol","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Defines an interface for a task.</p> <p>Task is a function that processes each dataset row and produces output for evaluation.</p>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.TaskProtocol.__call__","title":"__call__","text":"<pre><code>__call__(*, row: Row, parent: EvalParent, tags: Tags) -&gt; T\n</code></pre> <p>Processes a dataset row, using the provided context to produce task output.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>The dataset row to process.</p> required <code>parent</code> <code>EvalParent</code> <p>Reference to the parent task's output and evaluation results.</p> required <code>tags</code> <code>Tags</code> <p>Key-value pairs.</p> required <p>Returns:</p> Type Description <code>T</code> <p>Task output of type T or None to skip the row processing.</p> Example <pre><code>def simple_task(row: datasets.Row, parent: EvalParent, tags: Tags) -&gt; TaskResult:\n    # Process input from the dataset row\n    input_text = row.task_input\n\n    # Generate output\n    output = f\"Processed: {input_text}\"\n\n    # Return result\n    return TaskResult(\n        output=output,\n        metadata={\"processing_time_ms\": 42},\n        tags={\"model\": \"example-model\"}\n    )\n</code></pre> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>def __call__(self, *, row: datasets.Row, parent: EvalParent, tags: Tags) -&gt; T:\n    \"\"\"\n    Processes a dataset row, using the provided context to produce task output.\n\n    Args:\n        row: The dataset row to process.\n        parent: Reference to the parent task's output and evaluation results.\n        tags: Key-value pairs.\n\n    Returns:\n        Task output of type T or None to skip the row processing.\n\n    Example:\n        ```python\n        def simple_task(row: datasets.Row, parent: EvalParent, tags: Tags) -&gt; TaskResult:\n            # Process input from the dataset row\n            input_text = row.task_input\n\n            # Generate output\n            output = f\"Processed: {input_text}\"\n\n            # Return result\n            return TaskResult(\n                output=output,\n                metadata={\"processing_time_ms\": 42},\n                tags={\"model\": \"example-model\"}\n            )\n        ```\n    \"\"\"\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.ChainLink","title":"ChainLink","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single stage in an experiment's processing chain.</p> <p>Each ChainLink contains an optional task function that processes dataset rows and a list of evaluators that assess the task's output.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>Optional[Task]</code> <p>Function that processes a dataset row and produces output.</p> <code>evaluators</code> <code>list[AdaptableEvaluators]</code> <p>List of evaluators to assess the task's output.</p>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.Experiment","title":"Experiment","text":"<pre><code>Experiment(*, dataset: Any, task: Optional[Task] = None, evaluators: Optional[list[AdaptableEvaluators]] = None, chain: Optional[list[ChainLink]] = None, tags: Optional[dict[str, str]] = None, metadata: Optional[dict[str, Any]] = None, max_concurrency: int = 10, project_name: Optional[str] = None, experiment_name: Optional[str] = None, service: Optional[str] = None, api_key: Optional[str] = None, api_url: Optional[str] = None, otel_endpoint: Optional[str] = None, otel_exporter_otlp_protocol: Optional[str] = None, ui_url: Optional[str] = None, timeout_s: Optional[int] = None, integrations: Optional[list[Any]] = None, verify_ssl: bool = True, **kwargs)\n</code></pre> <p>Manages evaluation experiments across datasets using tasks and evaluators.</p> <p>An experiment represents a complete evaluation pipeline that processes a dataset using defined tasks, applies evaluators to the outputs, and collects the results. Experiments track progress, create reports, and interface with the Patronus platform.</p> <p>Create experiment instances using the <code>create()</code> class method or through the <code>run_experiment()</code> convenience function.</p> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dataset: typing.Any,\n    task: Optional[Task] = None,\n    evaluators: Optional[list[AdaptableEvaluators]] = None,\n    chain: Optional[list[ChainLink]] = None,\n    tags: Optional[dict[str, str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    max_concurrency: int = 10,\n    project_name: Optional[str] = None,\n    experiment_name: Optional[str] = None,\n    service: Optional[str] = None,\n    api_key: Optional[str] = None,\n    api_url: Optional[str] = None,\n    otel_endpoint: Optional[str] = None,\n    otel_exporter_otlp_protocol: Optional[str] = None,\n    ui_url: Optional[str] = None,\n    timeout_s: Optional[int] = None,\n    integrations: Optional[list[typing.Any]] = None,\n    verify_ssl: bool = True,\n    **kwargs,\n):\n    if chain and evaluators:\n        raise ValueError(\"Cannot specify both chain and evaluators\")\n\n    self._raw_dataset = dataset\n\n    if not chain:\n        chain = [{\"task\": task, \"evaluators\": evaluators}]\n    self._chain = [\n        {\"task\": _trace_task(link[\"task\"]), \"evaluators\": _adapt_evaluators(link[\"evaluators\"])} for link in chain\n    ]\n    self._started = False\n    self._finished = False\n\n    self._project_name = project_name\n    self.project = None\n\n    self._experiment_name = experiment_name\n    self.experiment = None\n\n    self.tags = tags or {}\n    self.metadata = metadata\n\n    self.max_concurrency = max_concurrency\n    self._verify_ssl = verify_ssl\n\n    self._service = service\n    self._api_key = api_key\n    self._api_url = api_url\n    self._otel_endpoint = otel_endpoint\n    self._otel_exporter_otlp_protocol = otel_exporter_otlp_protocol\n    self._ui_url = ui_url\n    self._timeout_s = timeout_s\n\n    self._prepared = False\n\n    self.reporter = Reporter()\n\n    self._integrations = integrations\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.Experiment.create","title":"create  <code>async</code> <code>classmethod</code>","text":"<pre><code>create(dataset: ExperimentDataset, task: Optional[Task] = None, evaluators: Optional[list[AdaptableEvaluators]] = None, chain: Optional[list[ChainLink]] = None, tags: Optional[Tags] = None, metadata: Optional[dict[str, Any]] = None, max_concurrency: int = 10, project_name: Optional[str] = None, experiment_name: Optional[str] = None, service: Optional[str] = None, api_key: Optional[str] = None, api_url: Optional[str] = None, otel_endpoint: Optional[str] = None, otel_exporter_otlp_protocol: Optional[str] = None, ui_url: Optional[str] = None, timeout_s: Optional[int] = None, integrations: Optional[list[Any]] = None, verify_ssl: bool = True, **kwargs: Any) -&gt; te.Self\n</code></pre> <p>Creates an instance of the class asynchronously with the specified parameters while performing necessary preparations. This method initializes various attributes including dataset, task, evaluators, chain, and additional configurations for managing concurrency, project details, service information, API keys, timeout settings, and integrations.</p> <p>Use run_experiment for more convenient usage.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ExperimentDataset</code> <p>The dataset to run evaluations against.</p> required <code>task</code> <code>Optional[Task]</code> <p>A function that processes each dataset row and produces output for evaluation. Mutually exclusive with the <code>chain</code> parameter.</p> <code>None</code> <code>evaluators</code> <code>Optional[list[AdaptableEvaluators]]</code> <p>A list of evaluators to assess the task output. Mutually exclusive with the <code>chain</code> parameter.</p> <code>None</code> <code>chain</code> <code>Optional[list[ChainLink]]</code> <p>A list of processing stages, each containing a task and associated evaluators. Use this for multi-stage evaluation pipelines.</p> <code>None</code> <code>tags</code> <code>Optional[Tags]</code> <p>Key-value pairs. All evaluations created by the experiment will contain these tags.</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Arbitrary dict. Metadata associated with the experiment.</p> <code>None</code> <code>max_concurrency</code> <code>int</code> <p>Maximum number of concurrent task and evaluation operations.</p> <code>10</code> <code>project_name</code> <code>Optional[str]</code> <p>Name of the project to create or use. Falls back to configuration or environment variables if not provided.</p> <code>None</code> <code>experiment_name</code> <code>Optional[str]</code> <p>Custom name for this experiment run. A timestamp will be appended.</p> <code>None</code> <code>service</code> <code>Optional[str]</code> <p>OpenTelemetry service name for tracing. Falls back to configuration or environment variables if not provided.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for Patronus services. Falls back to configuration or environment variables if not provided.</p> <code>None</code> <code>api_url</code> <code>Optional[str]</code> <p>URL for the Patronus API. Falls back to configuration or environment variables if not provided.</p> <code>None</code> <code>otel_endpoint</code> <code>Optional[str]</code> <p>OpenTelemetry collector endpoint. Falls back to configuration or environment variables if not provided.</p> <code>None</code> <code>otel_exporter_otlp_protocol</code> <code>Optional[str]</code> <p>OpenTelemetry exporter protocol (grpc or http/protobuf). Falls back to configuration or environment variables if not provided.</p> <code>None</code> <code>ui_url</code> <code>Optional[str]</code> <p>URL for the Patronus UI. Falls back to configuration or environment variables if not provided.</p> <code>None</code> <code>timeout_s</code> <code>Optional[int]</code> <p>Timeout in seconds for API operations. Falls back to configuration or environment variables if not provided.</p> <code>None</code> <code>integrations</code> <code>Optional[list[Any]]</code> <p>A list of OpenTelemetry instrumentors for additional tracing capabilities.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the experiment.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Experiment</code> <code>Self</code> <p>...</p> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>@classmethod\nasync def create(\n    cls,\n    dataset: ExperimentDataset,\n    task: Optional[Task] = None,\n    evaluators: Optional[list[AdaptableEvaluators]] = None,\n    chain: Optional[list[ChainLink]] = None,\n    tags: Optional[Tags] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    max_concurrency: int = 10,\n    project_name: Optional[str] = None,\n    experiment_name: Optional[str] = None,\n    service: Optional[str] = None,\n    api_key: Optional[str] = None,\n    api_url: Optional[str] = None,\n    otel_endpoint: Optional[str] = None,\n    otel_exporter_otlp_protocol: Optional[str] = None,\n    ui_url: Optional[str] = None,\n    timeout_s: Optional[int] = None,\n    integrations: Optional[list[typing.Any]] = None,\n    verify_ssl: bool = True,\n    **kwargs: typing.Any,\n) -&gt; te.Self:\n    \"\"\"\n    Creates an instance of the class asynchronously with the specified parameters while performing\n    necessary preparations. This method initializes various attributes including dataset, task,\n    evaluators, chain, and additional configurations for managing concurrency, project details,\n    service information, API keys, timeout settings, and integrations.\n\n    Use [run_experiment][patronus.experiments.experiment.run_experiment] for more convenient usage.\n\n    Args:\n        dataset: The dataset to run evaluations against.\n        task: A function that processes each dataset row and produces output for evaluation.\n            Mutually exclusive with the `chain` parameter.\n        evaluators: A list of evaluators to assess the task output. Mutually exclusive with\n            the `chain` parameter.\n        chain: A list of processing stages, each containing a task and associated evaluators.\n            Use this for multi-stage evaluation pipelines.\n        tags: Key-value pairs.\n            All evaluations created by the experiment will contain these tags.\n        metadata: Arbitrary dict.\n            Metadata associated with the experiment.\n        max_concurrency: Maximum number of concurrent task and evaluation operations.\n        project_name: Name of the project to create or use. Falls back to configuration or\n            environment variables if not provided.\n        experiment_name: Custom name for this experiment run. A timestamp will be appended.\n        service: OpenTelemetry service name for tracing. Falls back to configuration or\n            environment variables if not provided.\n        api_key: API key for Patronus services. Falls back to configuration or environment\n            variables if not provided.\n        api_url: URL for the Patronus API. Falls back to configuration or environment\n            variables if not provided.\n        otel_endpoint: OpenTelemetry collector endpoint. Falls back to configuration or\n            environment variables if not provided.\n        otel_exporter_otlp_protocol: OpenTelemetry exporter protocol (grpc or http/protobuf).\n            Falls back to configuration or environment variables if not provided.\n        ui_url: URL for the Patronus UI. Falls back to configuration or environment\n            variables if not provided.\n        timeout_s: Timeout in seconds for API operations. Falls back to configuration or\n            environment variables if not provided.\n        integrations: A list of OpenTelemetry instrumentors for additional tracing capabilities.\n        **kwargs: Additional keyword arguments passed to the experiment.\n\n    Returns:\n        Experiment: ...\n\n    \"\"\"\n    ex = cls(\n        dataset=dataset,\n        task=task,\n        evaluators=evaluators,\n        chain=chain,\n        tags=tags,\n        metadata=metadata,\n        max_concurrency=max_concurrency,\n        project_name=project_name,\n        experiment_name=experiment_name,\n        service=service,\n        api_key=api_key,\n        api_url=api_url,\n        otel_endpoint=otel_endpoint,\n        otel_exporter_otlp_protocol=otel_exporter_otlp_protocol,\n        ui_url=ui_url,\n        timeout_s=timeout_s,\n        integrations=integrations,\n        verify_ssl=verify_ssl,\n        **kwargs,\n    )\n    ex._ctx = await ex._prepare()\n\n    return ex\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.Experiment.run","title":"run  <code>async</code>","text":"<pre><code>run() -&gt; te.Self\n</code></pre> <p>Executes the experiment by processing all dataset items.</p> <p>Runs the experiment's task chain on each dataset row, applying evaluators to the results and collecting metrics. Progress is displayed with a progress bar and results are logged to the Patronus platform.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The experiment instance.</p> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>async def run(self) -&gt; te.Self:\n    \"\"\"\n    Executes the experiment by processing all dataset items.\n\n    Runs the experiment's task chain on each dataset row, applying evaluators\n    to the results and collecting metrics. Progress is displayed with a progress\n    bar and results are logged to the Patronus platform.\n\n    Returns:\n        The experiment instance.\n    \"\"\"\n    if self._started:\n        raise RuntimeError(\"Experiment already started\")\n    if self._prepared is False:\n        raise ValueError(\n            \"Experiment must be prepared before starting. \"\n            \"Seems that Experiment was not created using Experiment.create() classmethod.\"\n        )\n    self._started = True\n\n    with context._CTX_PAT.using(self._ctx):\n        await self._run()\n        self._finished = True\n        self.reporter.summary()\n\n    await asyncio.to_thread(self._ctx.exporter.force_flush)\n    await asyncio.to_thread(self._ctx.tracer_provider.force_flush)\n\n    return self\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.Experiment.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe() -&gt; pd.DataFrame\n</code></pre> <p>Converts experiment results to a pandas DataFrame.</p> <p>Creates a tabular representation of all evaluation results with dataset identifiers, task information, evaluation scores, and metadata.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame containing all experiment results.</p> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Converts experiment results to a pandas DataFrame.\n\n    Creates a tabular representation of all evaluation results with\n    dataset identifiers, task information, evaluation scores, and metadata.\n\n    Returns:\n        A pandas DataFrame containing all experiment results.\n    \"\"\"\n    if self._finished is not True:\n        raise RuntimeError(\"Experiment has to be in finished state\")\n    return self.reporter.to_dataframe()\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.Experiment.to_csv","title":"to_csv","text":"<pre><code>to_csv(path_or_buf: Union[str, Path, IO[AnyStr]], **kwargs: Any) -&gt; Optional[str]\n</code></pre> <p>Saves experiment results to a CSV file.</p> <p>Converts experiment results to a DataFrame and saves them as a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_buf</code> <code>Union[str, Path, IO[AnyStr]]</code> <p>String path or file-like object where the CSV will be saved.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to pandas.DataFrame.to_csv().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>String path if a path was specified and return_path is True, otherwise None.</p> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>def to_csv(\n    self, path_or_buf: Union[str, pathlib.Path, typing.IO[typing.AnyStr]], **kwargs: typing.Any\n) -&gt; Optional[str]:\n    \"\"\"\n    Saves experiment results to a CSV file.\n\n    Converts experiment results to a DataFrame and saves them as a CSV file.\n\n    Args:\n        path_or_buf: String path or file-like object where the CSV will be saved.\n        **kwargs: Additional arguments passed to pandas.DataFrame.to_csv().\n\n    Returns:\n        String path if a path was specified and return_path is True, otherwise None.\n\n    \"\"\"\n    return self.to_dataframe().to_csv(path_or_buf, **kwargs)\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.experiment.run_experiment","title":"run_experiment","text":"<pre><code>run_experiment(dataset: ExperimentDataset, task: Optional[Task] = None, evaluators: Optional[list[AdaptableEvaluators]] = None, chain: Optional[list[ChainLink]] = None, tags: Optional[Tags] = None, max_concurrency: int = 10, project_name: Optional[str] = None, experiment_name: Optional[str] = None, service: Optional[str] = None, api_key: Optional[str] = None, api_url: Optional[str] = None, otel_endpoint: Optional[str] = None, otel_exporter_otlp_protocol: Optional[str] = None, ui_url: Optional[str] = None, timeout_s: Optional[int] = None, integrations: Optional[list[Any]] = None, verify_ssl: bool = True, **kwargs) -&gt; Union[Experiment, typing.Awaitable[Experiment]]\n</code></pre> <p>Create and run an experiment.</p> <p>This function creates an experiment with the specified configuration and runs it to completion. The execution handling is context-aware:</p> <ul> <li>When called from an asynchronous context (with a running event loop), it returns an   awaitable that must be awaited.</li> <li>When called from a synchronous context (no running event loop), it blocks until the   experiment completes and returns the Experiment object.</li> </ul> <p>Examples:</p> <p>Synchronous execution:</p> <pre><code>experiment = run_experiment(dataset, task=some_task)\n# Blocks until the experiment finishes.\n</code></pre> <p>Asynchronous execution (e.g., in a Jupyter Notebook):</p> <pre><code>experiment = await run_experiment(dataset, task=some_task)\n# Must be awaited within an async function or event loop.\n</code></pre> <p>Parameters:</p> <p>See Experiment.create for list of arguments.</p> <p>Returns:</p> Name Type Description <code>Experiment</code> <code>Experiment</code> <p>In a synchronous context: the completed Experiment object.</p> <code>Experiment</code> <code>Awaitable[Experiment]</code> <p>In an asynchronous context: an awaitable that resolves to the Experiment object.</p> Notes <p>For manual control of the event loop, you can create and run the experiment as follows:</p> <pre><code>experiment = await Experiment.create(...)\nawait experiment.run()\n</code></pre> Source code in <code>src/patronus/experiments/experiment.py</code> <pre><code>def run_experiment(\n    dataset: ExperimentDataset,\n    task: Optional[Task] = None,\n    evaluators: Optional[list[AdaptableEvaluators]] = None,\n    chain: Optional[list[ChainLink]] = None,\n    tags: Optional[Tags] = None,\n    max_concurrency: int = 10,\n    project_name: Optional[str] = None,\n    experiment_name: Optional[str] = None,\n    service: Optional[str] = None,\n    api_key: Optional[str] = None,\n    api_url: Optional[str] = None,\n    otel_endpoint: Optional[str] = None,\n    otel_exporter_otlp_protocol: Optional[str] = None,\n    ui_url: Optional[str] = None,\n    timeout_s: Optional[int] = None,\n    integrations: Optional[list[typing.Any]] = None,\n    verify_ssl: bool = True,\n    **kwargs,\n) -&gt; Union[\"Experiment\", typing.Awaitable[\"Experiment\"]]:\n    \"\"\"\n    Create and run an experiment.\n\n    This function creates an experiment with the specified configuration and runs it to completion.\n    The execution handling is context-aware:\n\n    - When called from an asynchronous context (with a running event loop), it returns an\n      awaitable that must be awaited.\n    - When called from a synchronous context (no running event loop), it blocks until the\n      experiment completes and returns the Experiment object.\n\n\n    **Examples:**\n\n    Synchronous execution:\n\n    ```python\n    experiment = run_experiment(dataset, task=some_task)\n    # Blocks until the experiment finishes.\n    ```\n\n    Asynchronous execution (e.g., in a Jupyter Notebook):\n\n    ```python\n    experiment = await run_experiment(dataset, task=some_task)\n    # Must be awaited within an async function or event loop.\n    ```\n\n    **Parameters:**\n\n    See [Experiment.create][patronus.experiments.experiment.Experiment.create] for list of arguments.\n\n    Returns:\n        Experiment (Experiment): In a synchronous context: the completed Experiment object.\n        Experiment (Awaitable[Experiment]): In an asynchronous context:\n            an awaitable that resolves to the Experiment object.\n\n    Notes:\n        For manual control of the event loop, you can create and run the experiment as follows:\n\n        ```python\n        experiment = await Experiment.create(...)\n        await experiment.run()\n        ```\n\n    \"\"\"\n\n    async def _run_experiment() -&gt; Union[Experiment, typing.Awaitable[Experiment]]:\n        ex = await Experiment.create(\n            dataset=dataset,\n            task=task,\n            evaluators=evaluators,\n            chain=chain,\n            tags=tags,\n            max_concurrency=max_concurrency,\n            project_name=project_name,\n            experiment_name=experiment_name,\n            service=service,\n            api_key=api_key,\n            api_url=api_url,\n            otel_endpoint=otel_endpoint,\n            otel_exporter_otlp_protocol=otel_exporter_otlp_protocol,\n            ui_url=ui_url,\n            timeout_s=timeout_s,\n            integrations=integrations,\n            verify_ssl=verify_ssl,\n            **kwargs,\n        )\n        return await ex.run()\n\n    return run_until_complete(_run_experiment())\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.types","title":"types","text":""},{"location":"api_ref/experiments/#patronus.experiments.types.EvalParent","title":"EvalParent  <code>module-attribute</code>","text":"<pre><code>EvalParent = Optional[_EvalParent]\n</code></pre> <p>Type alias representing an optional reference to an evaluation parent, used to track the hierarchy of evaluations and their results</p>"},{"location":"api_ref/experiments/#patronus.experiments.types.TaskResult","title":"TaskResult","text":"<p>               Bases: <code>BaseModel</code>, <code>LogSerializer</code></p> <p>Represents the result of a task with optional output, metadata, context and tags.</p> <p>This class is used to encapsulate the result of a task, including optional fields for the output of the task, metadata related to the task, and any tags that can provide additional information or context about the task.</p> <p>Attributes:</p> Name Type Description <code>output</code> <code>Optional[str]</code> <p>The output of the task, if any.</p> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Additional information or metadata associated with the task.</p> <code>tags</code> <code>Optional[dict[str, str]]</code> <p>Key-value pairs used to tag and describe the task.</p> <code>context</code> <code>Optional[Union[list[str], str]]</code> <p>The context of the task, if any.</p>"},{"location":"api_ref/experiments/#patronus.experiments.types.TaskResult.dump_as_log","title":"dump_as_log","text":"<pre><code>dump_as_log() -&gt; dict[str, typing.Any]\n</code></pre> <p>Serialize the TaskResult into a dictionary format suitable for logging.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the task output, metadata, context and tags.</p> Source code in <code>src/patronus/experiments/types.py</code> <pre><code>def dump_as_log(self) -&gt; dict[str, typing.Any]:\n    \"\"\"\n    Serialize the TaskResult into a dictionary format suitable for logging.\n\n    Returns:\n        A dictionary containing the task output, metadata, context and tags.\n    \"\"\"\n    return self.model_dump(mode=\"json\")\n</code></pre>"},{"location":"api_ref/experiments/#patronus.experiments.types.EvalsMap","title":"EvalsMap","text":"<p>               Bases: <code>dict</code></p> <p>A specialized dictionary for storing evaluation results with flexible key handling.</p> <p>This class extends dict to provide automatic key normalization for evaluation results, allowing lookup by evaluator objects, strings, or any object with a canonical_name attribute.</p>"},{"location":"api_ref/init/","title":"Init","text":""},{"location":"api_ref/init/#patronus.init","title":"patronus.init","text":""},{"location":"api_ref/init/#patronus.init.init","title":"init","text":"<pre><code>init(project_name: Optional[str] = None, app: Optional[str] = None, api_url: Optional[str] = None, otel_endpoint: Optional[str] = None, otel_exporter_otlp_protocol: Optional[str] = None, api_key: Optional[str] = None, service: Optional[str] = None, resource_dir: Optional[str] = None, prompt_providers: Optional[list[str]] = None, prompt_templating_engine: Optional[str] = None, integrations: Optional[list[Any]] = None, **kwargs: Any) -&gt; context.PatronusContext\n</code></pre> <p>Initializes the Patronus SDK with the specified configuration.</p> <p>This function sets up the SDK with project details, API connections, and telemetry. It must be called before using evaluators or experiments to ensure proper recording of results and metrics.</p> Note <p><code>init()</code> should not be used for running experiments. Experiments have its own initialization process. You can configure them by passing configuration options to <code>run_experiment()</code> or using configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>Name of the project for organizing evaluations and experiments. Falls back to configuration file, then defaults to \"Global\" if not provided.</p> <code>None</code> <code>app</code> <code>Optional[str]</code> <p>Name of the application within the project. Falls back to configuration file, then defaults to \"default\" if not provided.</p> <code>None</code> <code>api_url</code> <code>Optional[str]</code> <p>URL for the Patronus API service. Falls back to configuration file or environment variables if not provided.</p> <code>None</code> <code>otel_endpoint</code> <code>Optional[str]</code> <p>Endpoint for OpenTelemetry data collection. Falls back to configuration file or environment variables if not provided.</p> <code>None</code> <code>otel_exporter_otlp_protocol</code> <code>Optional[str]</code> <p>OpenTelemetry exporter protocol (grpc or http/protobuf). Falls back to configuration file or environment variables if not provided.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>Authentication key for Patronus services. Falls back to configuration file or environment variables if not provided.</p> <code>None</code> <code>service</code> <code>Optional[str]</code> <p>Service name for OpenTelemetry traces. Falls back to configuration file or environment variables if not provided.</p> <code>None</code> <code>integrations</code> <code>Optional[list[Any]]</code> <p>List of integration to use.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional configuration options for the SDK.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PatronusContext</code> <code>PatronusContext</code> <p>The initialized context object.</p> Example <pre><code>import patronus\n\n# Load configuration from configuration file or environment variables\npatronus.init()\n\n# Custom initialization\npatronus.init(\n    project_name=\"my-project\",\n    app=\"recommendation-service\",\n    api_key=\"your-api-key\"\n)\n</code></pre> Source code in <code>src/patronus/init.py</code> <pre><code>def init(\n    project_name: Optional[str] = None,\n    app: Optional[str] = None,\n    api_url: Optional[str] = None,\n    otel_endpoint: Optional[str] = None,\n    otel_exporter_otlp_protocol: Optional[str] = None,\n    api_key: Optional[str] = None,\n    service: Optional[str] = None,\n    resource_dir: Optional[str] = None,\n    prompt_providers: Optional[list[str]] = None,\n    prompt_templating_engine: Optional[str] = None,\n    integrations: Optional[list[typing.Any]] = None,\n    **kwargs: typing.Any,\n) -&gt; context.PatronusContext:\n    \"\"\"\n    Initializes the Patronus SDK with the specified configuration.\n\n    This function sets up the SDK with project details, API connections, and telemetry.\n    It must be called before using evaluators or experiments to ensure proper recording\n    of results and metrics.\n\n    Note:\n        `init()` should not be used for running experiments.\n        Experiments have its own initialization process.\n        You can configure them by passing configuration options to [`run_experiment()`][patronus.experiments.experiment.run_experiment]\n        or using configuration file.\n\n    Args:\n        project_name: Name of the project for organizing evaluations and experiments.\n            Falls back to configuration file, then defaults to \"Global\" if not provided.\n        app: Name of the application within the project.\n            Falls back to configuration file, then defaults to \"default\" if not provided.\n        api_url: URL for the Patronus API service.\n            Falls back to configuration file or environment variables if not provided.\n        otel_endpoint: Endpoint for OpenTelemetry data collection.\n            Falls back to configuration file or environment variables if not provided.\n        otel_exporter_otlp_protocol: OpenTelemetry exporter protocol (grpc or http/protobuf).\n            Falls back to configuration file or environment variables if not provided.\n        api_key: Authentication key for Patronus services.\n            Falls back to configuration file or environment variables if not provided.\n        service: Service name for OpenTelemetry traces.\n            Falls back to configuration file or environment variables if not provided.\n        integrations: List of integration to use.\n        **kwargs: Additional configuration options for the SDK.\n\n    Returns:\n        PatronusContext: The initialized context object.\n\n    Example:\n        ```python\n        import patronus\n\n        # Load configuration from configuration file or environment variables\n        patronus.init()\n\n        # Custom initialization\n        patronus.init(\n            project_name=\"my-project\",\n            app=\"recommendation-service\",\n            api_key=\"your-api-key\"\n        )\n        ```\n    \"\"\"\n    api_url = api_url and api_url.rstrip(\"/\")\n    otel_endpoint = otel_endpoint and otel_endpoint.rstrip(\"/\")\n\n    if (api_url and api_url != config.DEFAULT_API_URL) and (otel_endpoint is None or otel_endpoint == config.DEFAULT_OTEL_ENDPOINT):\n        raise ValueError(\n            \"'api_url' is set to non-default value, \"\n            \"but 'otel_endpoint' is a default. Change 'otel_endpoint' to point to the same environment as 'api_url'\"\n        )\n\n    def build_and_set():\n        cfg = config.config()\n        ctx = build_context(\n            service=service or cfg.service,\n            project_name=project_name or cfg.project_name,\n            app=app or cfg.app,\n            experiment_id=None,\n            experiment_name=None,\n            api_url=api_url or cfg.api_url,\n            otel_endpoint=otel_endpoint or cfg.otel_endpoint,\n            otel_exporter_otlp_protocol=otel_exporter_otlp_protocol or cfg.otel_exporter_otlp_protocol,\n            api_key=api_key or cfg.api_key,\n            resource_dir=resource_dir or cfg.resource_dir,\n            prompt_providers=prompt_providers or cfg.prompt_providers,\n            prompt_templating_engine=cfg.prompt_templating_engine,\n            timeout_s=cfg.timeout_s,\n            integrations=integrations,\n            **kwargs,\n        )\n        context.set_global_patronus_context(ctx)\n\n    inited_now = _INIT_ONCE.do_once(build_and_set)\n    if not inited_now:\n        warnings.warn(\n            (\"The Patronus SDK has already been initialized. Duplicate initialization attempts are ignored.\"),\n            UserWarning,\n            stacklevel=2,\n        )\n    return context.get_current_context()\n</code></pre>"},{"location":"api_ref/init/#patronus.init.build_context","title":"build_context","text":"<pre><code>build_context(service: str, project_name: str, app: Optional[str], experiment_id: Optional[str], experiment_name: Optional[str], api_url: Optional[str], otel_endpoint: str, otel_exporter_otlp_protocol: Optional[str], api_key: str, resource_dir: Optional[str] = None, prompt_providers: Optional[list[str]] = None, prompt_templating_engine: Optional[str] = None, client_http: Optional[Client] = None, client_http_async: Optional[AsyncClient] = None, timeout_s: int = 60, verify_ssl: bool = True, integrations: Optional[list[Any]] = None, **kwargs: Any) -&gt; context.PatronusContext\n</code></pre> <p>Builds a Patronus context with the specified configuration parameters.</p> <p>This function creates the context object that contains all necessary components for the SDK operation, including loggers, tracers, and API clients. It is used internally by the <code>init()</code> function but can also be used directly for more advanced configuration scenarios.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>str</code> <p>Service name for OpenTelemetry traces.</p> required <code>project_name</code> <code>str</code> <p>Name of the project for organizing evaluations and experiments.</p> required <code>app</code> <code>Optional[str]</code> <p>Name of the application within the project.</p> required <code>experiment_id</code> <code>Optional[str]</code> <p>Unique identifier for an experiment when running in experiment mode.</p> required <code>experiment_name</code> <code>Optional[str]</code> <p>Display name for an experiment when running in experiment mode.</p> required <code>api_url</code> <code>Optional[str]</code> <p>URL for the Patronus API service.</p> required <code>otel_endpoint</code> <code>str</code> <p>Endpoint for OpenTelemetry data collection.</p> required <code>otel_exporter_otlp_protocol</code> <code>Optional[str]</code> <p>OpenTelemetry exporter protocol (grpc or http/protobuf).</p> required <code>api_key</code> <code>str</code> <p>Authentication key for Patronus services.</p> required <code>client_http</code> <code>Optional[Client]</code> <p>Custom HTTP client for synchronous API requests. If not provided, a new client will be created.</p> <code>None</code> <code>client_http_async</code> <code>Optional[AsyncClient]</code> <p>Custom HTTP client for asynchronous API requests. If not provided, a new client will be created.</p> <code>None</code> <code>timeout_s</code> <code>int</code> <p>Timeout in seconds for HTTP requests (default: 60).</p> <code>60</code> <code>integrations</code> <code>Optional[list[Any]]</code> <p>List of PatronusIntegrator instances.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional configuration options, including: - integrations: List of OpenTelemetry instrumentors to enable.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PatronusContext</code> <code>PatronusContext</code> <p>The initialized context object containing all necessary components for SDK operation.</p> Source code in <code>src/patronus/init.py</code> <pre><code>def build_context(\n    service: str,\n    project_name: str,\n    app: Optional[str],\n    experiment_id: Optional[str],\n    experiment_name: Optional[str],\n    api_url: Optional[str],\n    otel_endpoint: str,\n    otel_exporter_otlp_protocol: Optional[str],\n    api_key: str,\n    resource_dir: Optional[str] = None,\n    prompt_providers: Optional[list[str]] = None,\n    prompt_templating_engine: Optional[str] = None,\n    client_http: Optional[httpx.Client] = None,\n    client_http_async: Optional[httpx.AsyncClient] = None,\n    timeout_s: int = 60,\n    verify_ssl: bool = True,\n    integrations: Optional[list[typing.Any]] = None,\n    **kwargs: typing.Any,\n) -&gt; context.PatronusContext:\n    \"\"\"\n    Builds a Patronus context with the specified configuration parameters.\n\n    This function creates the context object that contains all necessary components\n    for the SDK operation, including loggers, tracers, and API clients. It is used\n    internally by the [`init()`][patronus.init.init] function but can also be used directly for more\n    advanced configuration scenarios.\n\n    Args:\n        service: Service name for OpenTelemetry traces.\n        project_name: Name of the project for organizing evaluations and experiments.\n        app: Name of the application within the project.\n        experiment_id: Unique identifier for an experiment when running in experiment mode.\n        experiment_name: Display name for an experiment when running in experiment mode.\n        api_url: URL for the Patronus API service.\n        otel_endpoint: Endpoint for OpenTelemetry data collection.\n        otel_exporter_otlp_protocol: OpenTelemetry exporter protocol (grpc or http/protobuf).\n        api_key: Authentication key for Patronus services.\n        client_http: Custom HTTP client for synchronous API requests.\n            If not provided, a new client will be created.\n        client_http_async: Custom HTTP client for asynchronous API requests.\n            If not provided, a new client will be created.\n        timeout_s: Timeout in seconds for HTTP requests (default: 60).\n        integrations: List of PatronusIntegrator instances.\n        **kwargs: Additional configuration options, including:\n            - integrations: List of OpenTelemetry instrumentors to enable.\n\n    Returns:\n        PatronusContext: The initialized context object containing all necessary\n            components for SDK operation.\n    \"\"\"\n    if client_http is None:\n        client_http = httpx.Client(timeout=timeout_s, verify=verify_ssl)\n    if client_http_async is None:\n        client_http_async = httpx.AsyncClient(timeout=timeout_s, verify=verify_ssl)\n\n    integrations = prepare_integrations(integrations)\n\n    scope = context.PatronusScope(\n        service=service,\n        project_name=project_name,\n        app=app,\n        experiment_id=experiment_id,\n        experiment_name=experiment_name,\n    )\n    api_deprecated = PatronusAPIClient(\n        client_http_async=client_http_async,\n        client_http=client_http,\n        base_url=api_url,\n        api_key=api_key,\n    )\n    api_client = patronus_api.Client(api_key=api_key, base_url=api_url)\n    async_api_client = patronus_api.AsyncClient(api_key=api_key, base_url=api_url)\n\n    logger_provider = create_logger_provider(\n        exporter_endpoint=otel_endpoint,\n        api_key=api_key,\n        scope=scope,\n        protocol=otel_exporter_otlp_protocol,\n    )\n\n    tracer_provider = create_tracer_provider(\n        exporter_endpoint=otel_endpoint,\n        api_key=api_key,\n        scope=scope,\n        protocol=otel_exporter_otlp_protocol,\n    )\n\n    eval_exporter = BatchEvaluationExporter(client=api_deprecated)\n    ctx = context.PatronusContext(\n        scope=scope,\n        tracer_provider=tracer_provider,\n        logger_provider=logger_provider,\n        api_client_deprecated=api_deprecated,\n        api_client=api_client,\n        async_api_client=async_api_client,\n        exporter=eval_exporter,\n        prompts=context.PromptsConfig(\n            directory=resource_dir and pathlib.Path(resource_dir, \"prompts\"),\n            providers=prompt_providers,\n            templating_engine=prompt_templating_engine,\n        ),\n    )\n    apply_integrations(ctx, integrations)\n    return ctx\n</code></pre>"},{"location":"api_ref/integrations/","title":"Integrations","text":""},{"location":"api_ref/integrations/#patronus.integrations","title":"patronus.integrations","text":"<p>This package provides integration points for connecting various third-party libraries and tools with the Patronus SDK.</p>"},{"location":"api_ref/integrations/#patronus.integrations.instrumenter","title":"instrumenter","text":""},{"location":"api_ref/integrations/#patronus.integrations.instrumenter.BasePatronusIntegrator","title":"BasePatronusIntegrator","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for Patronus integrations.</p> <p>This class defines the interface for integrating external libraries and tools with the Patronus context. All specific integrators should inherit from this class and implement the required methods.</p>"},{"location":"api_ref/integrations/#patronus.integrations.instrumenter.BasePatronusIntegrator.apply","title":"apply  <code>abstractmethod</code>","text":"<pre><code>apply(ctx: PatronusContext, **kwargs: Any)\n</code></pre> <p>Apply the integration to the given Patronus context.</p> <p>This method must be implemented by subclasses to define how the integration is applied to a Patronus context instance.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>PatronusContext</code> <p>The Patronus context to apply the integration to.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to the implementation.</p> <code>{}</code> Source code in <code>src/patronus/integrations/instrumenter.py</code> <pre><code>@abc.abstractmethod\ndef apply(self, ctx: \"context.PatronusContext\", **kwargs: typing.Any):\n    \"\"\"\n    Apply the integration to the given Patronus context.\n\n    This method must be implemented by subclasses to define how the\n    integration is applied to a Patronus context instance.\n\n    Args:\n        ctx: The Patronus context to apply the integration to.\n        **kwargs: Additional keyword arguments specific to the implementation.\n    \"\"\"\n</code></pre>"},{"location":"api_ref/integrations/#patronus.integrations.otel","title":"otel","text":""},{"location":"api_ref/integrations/#patronus.integrations.otel.OpenTelemetryIntegrator","title":"OpenTelemetryIntegrator","text":"<pre><code>OpenTelemetryIntegrator(instrumentor: BaseInstrumentor)\n</code></pre> <p>               Bases: <code>BasePatronusIntegrator</code></p> <p>Integration for OpenTelemetry instrumentors with Patronus.</p> <p>This class provides an adapter between OpenTelemetry instrumentors and the Patronus context, allowing for easy integration of OpenTelemetry instrumentation in Patronus-managed applications.</p> <p>Parameters:</p> Name Type Description Default <code>instrumentor</code> <code>BaseInstrumentor</code> <p>An OpenTelemetry instrumentor instance that will be applied to the Patronus context.</p> required Source code in <code>src/patronus/integrations/otel.py</code> <pre><code>def __init__(self, instrumentor: \"BaseInstrumentor\"):\n    \"\"\"\n    Initialize the OpenTelemetry integrator.\n\n    Args:\n        instrumentor: An OpenTelemetry instrumentor instance that will be\n            applied to the Patronus context.\n    \"\"\"\n    self.instrumentor = instrumentor\n</code></pre>"},{"location":"api_ref/integrations/#patronus.integrations.otel.OpenTelemetryIntegrator.apply","title":"apply","text":"<pre><code>apply(ctx: PatronusContext, **kwargs: Any)\n</code></pre> <p>Apply OpenTelemetry instrumentation to the Patronus context.</p> <p>This method configures the OpenTelemetry instrumentor with the tracer provider from the Patronus context.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>PatronusContext</code> <p>The Patronus context containing the tracer provider.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (unused).</p> <code>{}</code> Source code in <code>src/patronus/integrations/otel.py</code> <pre><code>def apply(self, ctx: \"context.PatronusContext\", **kwargs: typing.Any):\n    \"\"\"\n    Apply OpenTelemetry instrumentation to the Patronus context.\n\n    This method configures the OpenTelemetry instrumentor with the\n    tracer provider from the Patronus context.\n\n    Args:\n        ctx: The Patronus context containing the tracer provider.\n        **kwargs: Additional keyword arguments (unused).\n    \"\"\"\n    self.instrumentor.instrument(tracer_provider=ctx.tracer_provider)\n</code></pre>"},{"location":"api_ref/integrations/#patronus.integrations.pydantic_ai","title":"pydantic_ai","text":""},{"location":"api_ref/integrations/#patronus.integrations.pydantic_ai.PydanticAIIntegrator","title":"PydanticAIIntegrator","text":"<pre><code>PydanticAIIntegrator(event_mode: Literal['attributes', 'logs'] = 'logs')\n</code></pre> <p>               Bases: <code>BasePatronusIntegrator</code></p> <p>Integration for Pydantic-AI with Patronus.</p> <p>This class provides integration between Pydantic-AI agents and the Patronus observability stack, enabling tracing and logging of Pydantic-AI agent operations.</p> <p>Parameters:</p> Name Type Description Default <code>event_mode</code> <code>Literal['attributes', 'logs']</code> <p>The mode for capturing events, either as span attributes or as logs. Default is \"logs\".</p> <code>'logs'</code> Source code in <code>src/patronus/integrations/pydantic_ai.py</code> <pre><code>def __init__(self, event_mode: Literal[\"attributes\", \"logs\"] = \"logs\"):\n    \"\"\"\n    Initialize the Pydantic-AI integrator.\n\n    Args:\n        event_mode: The mode for capturing events, either as span attributes\n            or as logs. Default is \"logs\".\n    \"\"\"\n    self._instrumentation_settings = {\"event_mode\": event_mode}\n</code></pre>"},{"location":"api_ref/integrations/#patronus.integrations.pydantic_ai.PydanticAIIntegrator.apply","title":"apply","text":"<pre><code>apply(ctx: PatronusContext, **kwargs: Any)\n</code></pre> <p>Apply Pydantic-AI instrumentation to the Patronus context.</p> <p>This method configures all Pydantic-AI agents to use the tracer and logger providers from the Patronus context.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>PatronusContext</code> <p>The Patronus context containing the tracer and logger providers.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (unused).</p> <code>{}</code> Source code in <code>src/patronus/integrations/pydantic_ai.py</code> <pre><code>def apply(self, ctx: \"context.PatronusContext\", **kwargs: Any):\n    \"\"\"\n    Apply Pydantic-AI instrumentation to the Patronus context.\n\n    This method configures all Pydantic-AI agents to use the tracer and logger\n    providers from the Patronus context.\n\n    Args:\n        ctx: The Patronus context containing the tracer and logger providers.\n        **kwargs: Additional keyword arguments (unused).\n    \"\"\"\n    from pydantic_ai.agent import Agent, InstrumentationSettings\n\n    settings_kwargs = {\n        **self._instrumentation_settings,\n        \"tracer_provider\": ctx.tracer_provider,\n        \"event_logger_provider\": EventLoggerProvider(ctx.logger_provider),\n    }\n    settings = InstrumentationSettings(**settings_kwargs)\n    Agent.instrument_all(instrument=settings)\n</code></pre>"},{"location":"api_ref/pat_client/","title":"Patronus Objects","text":""},{"location":"api_ref/pat_client/#patronus.pat_client.client_async","title":"client_async","text":""},{"location":"api_ref/pat_client/#patronus.pat_client.client_async.AsyncPatronus","title":"AsyncPatronus","text":"<pre><code>AsyncPatronus(max_workers: int = 10)\n</code></pre> Source code in <code>src/patronus/pat_client/client_async.py</code> <pre><code>def __init__(self, max_workers: int = 10):\n    self._pending_tasks = collections.deque()\n    self._executor = ThreadPoolExecutor(max_workers=max_workers)\n    self._semaphore = asyncio.Semaphore(max_workers)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_async.AsyncPatronus.evaluate","title":"evaluate  <code>async</code>","text":"<pre><code>evaluate(evaluators: Union[List[Evaluator], Evaluator], *, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[dict] = None, return_exceptions: bool = False) -&gt; EvaluationContainer\n</code></pre> <p>Run multiple evaluators in parallel.</p> Source code in <code>src/patronus/pat_client/client_async.py</code> <pre><code>async def evaluate(\n    self,\n    evaluators: Union[List[Evaluator], Evaluator],\n    *,\n    system_prompt: Optional[str] = None,\n    task_context: Union[list[str], str, None] = None,\n    task_input: Optional[str] = None,\n    task_output: Optional[str] = None,\n    gold_answer: Optional[str] = None,\n    task_metadata: Optional[dict] = None,\n    return_exceptions: bool = False,\n) -&gt; EvaluationContainer:\n    \"\"\"\n    Run multiple evaluators in parallel.\n    \"\"\"\n    singular_eval = not isinstance(evaluators, list)\n    if singular_eval:\n        evaluators = [evaluators]\n    evaluators = self._map_evaluators(evaluators)\n\n    def into_coro(fn, **kwargs):\n        if inspect.iscoroutinefunction(fn):\n            coro = fn(**kwargs)\n        else:\n            coro = asyncio.to_thread(fn, **kwargs)\n        return with_semaphore(self._semaphore, coro)\n\n    with bundled_eval():\n        results = await asyncio.gather(\n            *(\n                into_coro(\n                    ev.evaluate,\n                    system_prompt=system_prompt,\n                    task_context=task_context,\n                    task_input=task_input,\n                    task_output=task_output,\n                    gold_answer=gold_answer,\n                    task_metadata=task_metadata,\n                )\n                for ev in evaluators\n            ),\n            return_exceptions=return_exceptions,\n        )\n    return EvaluationContainer(results)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_async.AsyncPatronus.evaluate_bg","title":"evaluate_bg","text":"<pre><code>evaluate_bg(evaluators: Union[List[Evaluator], Evaluator], *, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[dict] = None) -&gt; Task[EvaluationContainer]\n</code></pre> <p>Run multiple evaluators in parallel. The returned task will be a background task.</p> Source code in <code>src/patronus/pat_client/client_async.py</code> <pre><code>def evaluate_bg(\n    self,\n    evaluators: Union[List[Evaluator], Evaluator],\n    *,\n    system_prompt: Optional[str] = None,\n    task_context: Union[list[str], str, None] = None,\n    task_input: Optional[str] = None,\n    task_output: Optional[str] = None,\n    gold_answer: Optional[str] = None,\n    task_metadata: Optional[dict] = None,\n) -&gt; Task[EvaluationContainer]:\n    \"\"\"\n    Run multiple evaluators in parallel. The returned task will be a background task.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    task = loop.create_task(\n        self.evaluate(\n            evaluators=evaluators,\n            system_prompt=system_prompt,\n            task_context=task_context,\n            task_input=task_input,\n            task_output=task_output,\n            gold_answer=gold_answer,\n            task_metadata=task_metadata,\n            return_exceptions=True,\n        ),\n        name=\"evaluate_bg\",\n    )\n    self._pending_tasks.append(task)\n    task.add_done_callback(self._consume_tasks)\n    return task\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_async.AsyncPatronus.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> <p>Gracefully close the client. This will wait for all background tasks to finish.</p> Source code in <code>src/patronus/pat_client/client_async.py</code> <pre><code>async def close(self):\n    \"\"\"\n    Gracefully close the client. This will wait for all background tasks to finish.\n    \"\"\"\n    while len(self._pending_tasks) != 0:\n        await self._pending_tasks.popleft()\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync","title":"client_sync","text":""},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync.Patronus","title":"Patronus","text":"<pre><code>Patronus(workers: int = 10, shutdown_on_exit: bool = True)\n</code></pre> Source code in <code>src/patronus/pat_client/client_sync.py</code> <pre><code>def __init__(self, workers: int = 10, shutdown_on_exit: bool = True):\n    self._worker_pool = ThreadPool(workers)\n    self._supervisor_pool = ThreadPool(workers)\n\n    self._at_exit_handler = None\n    if shutdown_on_exit:\n        self._at_exit_handler = atexit.register(self.close)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync.Patronus.evaluate","title":"evaluate","text":"<pre><code>evaluate(evaluators: Union[list[Evaluator], Evaluator], *, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[dict[str, Any]] = None, return_exceptions: bool = False) -&gt; EvaluationContainer\n</code></pre> <p>Run multiple evaluators in parallel.</p> Source code in <code>src/patronus/pat_client/client_sync.py</code> <pre><code>def evaluate(\n    self,\n    evaluators: typing.Union[list[Evaluator], Evaluator],\n    *,\n    system_prompt: typing.Optional[str] = None,\n    task_context: typing.Union[list[str], str, None] = None,\n    task_input: typing.Optional[str] = None,\n    task_output: typing.Optional[str] = None,\n    gold_answer: typing.Optional[str] = None,\n    task_metadata: typing.Optional[dict[str, typing.Any]] = None,\n    return_exceptions: bool = False,\n) -&gt; EvaluationContainer:\n    \"\"\"\n    Run multiple evaluators in parallel.\n    \"\"\"\n    if not isinstance(evaluators, list):\n        evaluators = [evaluators]\n    evaluators = self._map_evaluators(evaluators)\n\n    with bundled_eval():\n        callables = [\n            _into_thread_run_fn(\n                ev.evaluate,\n                system_prompt=system_prompt,\n                task_context=task_context,\n                task_input=task_input,\n                task_output=task_output,\n                gold_answer=gold_answer,\n                task_metadata=task_metadata,\n            )\n            for ev in evaluators\n        ]\n        results = self._process_batch(callables, return_exceptions=return_exceptions)\n        return EvaluationContainer(results)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync.Patronus.evaluate_bg","title":"evaluate_bg","text":"<pre><code>evaluate_bg(evaluators: list[StructuredEvaluator], *, system_prompt: Optional[str] = None, task_context: Union[list[str], str, None] = None, task_input: Optional[str] = None, task_output: Optional[str] = None, gold_answer: Optional[str] = None, task_metadata: Optional[dict[str, Any]] = None) -&gt; TypedAsyncResult[EvaluationContainer]\n</code></pre> <p>Run multiple evaluators in parallel. The returned task will be a background task.</p> Source code in <code>src/patronus/pat_client/client_sync.py</code> <pre><code>def evaluate_bg(\n    self,\n    evaluators: list[StructuredEvaluator],\n    *,\n    system_prompt: typing.Optional[str] = None,\n    task_context: typing.Union[list[str], str, None] = None,\n    task_input: typing.Optional[str] = None,\n    task_output: typing.Optional[str] = None,\n    gold_answer: typing.Optional[str] = None,\n    task_metadata: typing.Optional[dict[str, typing.Any]] = None,\n) -&gt; TypedAsyncResult[EvaluationContainer]:\n    \"\"\"\n    Run multiple evaluators in parallel. The returned task will be a background task.\n    \"\"\"\n\n    def _run():\n        with bundled_eval():\n            callables = [\n                _into_thread_run_fn(\n                    ev.evaluate,\n                    system_prompt=system_prompt,\n                    task_context=task_context,\n                    task_input=task_input,\n                    task_output=task_output,\n                    gold_answer=gold_answer,\n                    task_metadata=task_metadata,\n                )\n                for ev in evaluators\n            ]\n            results = self._process_batch(callables, return_exceptions=True)\n            return EvaluationContainer(results)\n\n    return typing.cast(\n        TypedAsyncResult[EvaluationContainer], self._supervisor_pool.apply_async(_into_thread_run_fn(_run))\n    )\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.client_sync.Patronus.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Gracefully close the client. This will wait for all background tasks to finish.</p> Source code in <code>src/patronus/pat_client/client_sync.py</code> <pre><code>def close(self):\n    \"\"\"\n    Gracefully close the client. This will wait for all background tasks to finish.\n    \"\"\"\n    self._close()\n    if self._at_exit_handler:\n        atexit.unregister(self._at_exit_handler)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container","title":"container","text":""},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer","title":"EvaluationContainer  <code>dataclass</code>","text":"<pre><code>EvaluationContainer(results: list[Union[EvaluationResult, None, Exception]])\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.format","title":"format","text":"<pre><code>format() -&gt; str\n</code></pre> <p>Format the evaluation results into a readable summary.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def format(self) -&gt; str:\n    \"\"\"\n    Format the evaluation results into a readable summary.\n    \"\"\"\n    buf = StringIO()\n\n    total = len(self.results)\n    exceptions_count = sum(1 for r in self.results if isinstance(r, Exception))\n    successes_count = sum(1 for r in self.results if isinstance(r, EvaluationResult) and r.pass_ is True)\n    failures_count = sum(1 for r in self.results if isinstance(r, EvaluationResult) and r.pass_ is False)\n\n    buf.write(f\"Total evaluations: {total}\\n\")\n    buf.write(f\"Successes: {successes_count}\\n\")\n    buf.write(f\"Failures: {failures_count}\\n\")\n    buf.write(f\"Exceptions: {exceptions_count}\\n\\n\")\n    buf.write(\"Evaluation Details:\\n\")\n    buf.write(\"---\\n\")\n\n    # Add detailed evaluation results\n    for result in self.results:\n        if result is None:\n            buf.write(\"None\\n\")\n        elif isinstance(result, Exception):\n            buf.write(str(result))\n            buf.write(\"\\n\")\n        else:\n            buf.write(result.format())\n        buf.write(\"---\\n\")\n\n    return buf.getvalue()\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.pretty_print","title":"pretty_print","text":"<pre><code>pretty_print(file: Optional[IO] = None) -&gt; None\n</code></pre> <p>Formats and prints the current object in a human-readable form.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Optional[IO]</code> <code>None</code> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def pretty_print(self, file: Optional[IO] = None) -&gt; None:\n    \"\"\"\n    Formats and prints the current object in a human-readable form.\n\n    Args:\n        file:\n    \"\"\"\n    f = self.format()\n    print(f, file=file)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.has_exception","title":"has_exception","text":"<pre><code>has_exception() -&gt; bool\n</code></pre> <p>Checks if the results contain any exception.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def has_exception(self) -&gt; bool:\n    \"\"\"\n    Checks if the results contain any exception.\n    \"\"\"\n    return any(isinstance(r, Exception) for r in self.results)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.raise_on_exception","title":"raise_on_exception","text":"<pre><code>raise_on_exception() -&gt; None\n</code></pre> <p>Checks the results for any exceptions and raises them accordingly.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def raise_on_exception(self) -&gt; None:\n    \"\"\"\n    Checks the results for any exceptions and raises them accordingly.\n    \"\"\"\n    if not self.has_exception():\n        return None\n    exceptions = list(r for r in self.results if isinstance(r, Exception))\n    if len(exceptions) == 1:\n        raise exceptions[0]\n    raise MultiException(exceptions)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.all_succeeded","title":"all_succeeded","text":"<pre><code>all_succeeded(ignore_exceptions: bool = False) -&gt; bool\n</code></pre> <p>Check if all evaluations that were actually evaluated passed.</p> <p>Evaluations are only considered if they: - Have a non-None pass_ flag set - Are not None (skipped) - Are not exceptions (unless ignore_exceptions=True)</p> <p>Note: Returns True if no evaluations met the above criteria (empty case).</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def all_succeeded(self, ignore_exceptions: bool = False) -&gt; bool:\n    \"\"\"\n    Check if all evaluations that were actually evaluated passed.\n\n    Evaluations are only considered if they:\n    - Have a non-None pass_ flag set\n    - Are not None (skipped)\n    - Are not exceptions (unless ignore_exceptions=True)\n\n    Note: Returns True if no evaluations met the above criteria (empty case).\n    \"\"\"\n    for r in self.results:\n        if isinstance(r, Exception) and not ignore_exceptions:\n            self.raise_on_exception()\n        if r is not None and r.pass_ is False:\n            return False\n    return True\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.any_failed","title":"any_failed","text":"<pre><code>any_failed(ignore_exceptions: bool = False) -&gt; bool\n</code></pre> <p>Check if any evaluation that was actually evaluated failed.</p> <p>Evaluations are only considered if they: - Have a non-None pass_ flag set - Are not None (skipped) - Are not exceptions (unless ignore_exceptions=True)</p> <p>Note: Returns False if no evaluations met the above criteria (empty case).</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def any_failed(self, ignore_exceptions: bool = False) -&gt; bool:\n    \"\"\"\n    Check if any evaluation that was actually evaluated failed.\n\n    Evaluations are only considered if they:\n    - Have a non-None pass_ flag set\n    - Are not None (skipped)\n    - Are not exceptions (unless ignore_exceptions=True)\n\n    Note: Returns False if no evaluations met the above criteria (empty case).\n    \"\"\"\n    for r in self.results:\n        if isinstance(r, Exception) and not ignore_exceptions:\n            self.raise_on_exception()\n        if r is not None and r.pass_ is False:\n            return True\n    return False\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.failed_evaluations","title":"failed_evaluations","text":"<pre><code>failed_evaluations() -&gt; Generator[EvaluationResult, None, None]\n</code></pre> <p>Generates all failed evaluations from the results.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def failed_evaluations(self) -&gt; Generator[EvaluationResult, None, None]:\n    \"\"\"\n    Generates all failed evaluations from the results.\n    \"\"\"\n    return (r for r in self.results if not isinstance(r, (Exception, type(None))) and r.pass_ is False)\n</code></pre>"},{"location":"api_ref/pat_client/#patronus.pat_client.container.EvaluationContainer.succeeded_evaluations","title":"succeeded_evaluations","text":"<pre><code>succeeded_evaluations() -&gt; Generator[EvaluationResult, None, None]\n</code></pre> <p>Generates all successfully passed evaluations from the <code>results</code> attribute.</p> Source code in <code>src/patronus/pat_client/container.py</code> <pre><code>def succeeded_evaluations(self) -&gt; Generator[EvaluationResult, None, None]:\n    \"\"\"\n    Generates all successfully passed evaluations from the `results` attribute.\n    \"\"\"\n    return (r for r in self.results if not isinstance(r, (Exception, type(None))) and r.pass_ is True)\n</code></pre>"},{"location":"api_ref/prompts/","title":"Prompts","text":""},{"location":"api_ref/prompts/#patronus.prompts","title":"patronus.prompts","text":""},{"location":"api_ref/prompts/#patronus.prompts.clients","title":"clients","text":""},{"location":"api_ref/prompts/#patronus.prompts.clients.load_prompt","title":"load_prompt  <code>module-attribute</code>","text":"<pre><code>load_prompt = get\n</code></pre> <p>Alias for PromptClient.get.</p>"},{"location":"api_ref/prompts/#patronus.prompts.clients.aload_prompt","title":"aload_prompt  <code>module-attribute</code>","text":"<pre><code>aload_prompt = get\n</code></pre> <p>Alias for AsyncPromptClient.get.</p>"},{"location":"api_ref/prompts/#patronus.prompts.clients.push_prompt","title":"push_prompt  <code>module-attribute</code>","text":"<pre><code>push_prompt = push\n</code></pre> <p>Alias for PromptClient.push.</p>"},{"location":"api_ref/prompts/#patronus.prompts.clients.apush_prompt","title":"apush_prompt  <code>module-attribute</code>","text":"<pre><code>apush_prompt = push\n</code></pre> <p>Alias for AsyncPromptClient.push.</p>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptNotFoundError","title":"PromptNotFoundError","text":"<pre><code>PromptNotFoundError(name: str, project: Optional[str] = None, revision: Optional[int] = None, label: Optional[str] = None)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Raised when a prompt could not be found.</p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>def __init__(\n    self, name: str, project: Optional[str] = None, revision: Optional[int] = None, label: Optional[str] = None\n):\n    self.name = name\n    self.project = project\n    self.revision = revision\n    self.label = label\n    message = f\"Prompt not found (name={name!r}, project={project!r}, revision={revision!r}, label={label!r})\"\n    super().__init__(message)\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptProviderError","title":"PromptProviderError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for prompt provider errors.</p>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptProviderConnectionError","title":"PromptProviderConnectionError","text":"<p>               Bases: <code>PromptProviderError</code></p> <p>Raised when there's a connectivity issue with the prompt provider.</p>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptProviderAuthenticationError","title":"PromptProviderAuthenticationError","text":"<p>               Bases: <code>PromptProviderError</code></p> <p>Raised when there's an authentication issue with the prompt provider.</p>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptProvider","title":"PromptProvider","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptProvider.get_prompt","title":"get_prompt  <code>abstractmethod</code>","text":"<pre><code>get_prompt(name: str, revision: Optional[int], label: Optional[str], project: str, engine: TemplateEngine) -&gt; Optional[LoadedPrompt]\n</code></pre> <p>Get prompts, returns None if prompt was not found</p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>@abc.abstractmethod\ndef get_prompt(\n    self, name: str, revision: Optional[int], label: Optional[str], project: str, engine: TemplateEngine\n) -&gt; Optional[LoadedPrompt]:\n    \"\"\"Get prompts, returns None if prompt was not found\"\"\"\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptProvider.aget_prompt","title":"aget_prompt  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>aget_prompt(name: str, revision: Optional[int], label: Optional[str], project: str, engine: TemplateEngine) -&gt; Optional[LoadedPrompt]\n</code></pre> <p>Get prompts, returns None if prompt was not found</p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>@abc.abstractmethod\nasync def aget_prompt(\n    self, name: str, revision: Optional[int], label: Optional[str], project: str, engine: TemplateEngine\n) -&gt; Optional[LoadedPrompt]:\n    \"\"\"Get prompts, returns None if prompt was not found\"\"\"\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptClientMixin","title":"PromptClientMixin","text":""},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptClient","title":"PromptClient","text":"<pre><code>PromptClient(provider_factory: Optional[ProviderFactory] = None)\n</code></pre> <p>               Bases: <code>PromptClientMixin</code></p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>def __init__(self, provider_factory: Optional[ProviderFactory] = None) -&gt; None:\n    self._cache: PromptCache = PromptCache()\n    self._provider_factory: ProviderFactory = provider_factory or {\n        \"local\": lambda: LocalPromptProvider(),\n        \"api\": lambda: APIPromptProvider(),\n    }\n    self._api_provider = APIPromptProvider()\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptClient.get","title":"get","text":"<pre><code>get(name: str, revision: Optional[int] = None, label: Optional[str] = None, project: Union[str, Type[NOT_GIVEN]] = NOT_GIVEN, disable_cache: bool = False, provider: Union[PromptProvider, _DefaultProviders, Sequence[Union[PromptProvider, _DefaultProviders]], Type[NOT_GIVEN]] = NOT_GIVEN, engine: Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]] = NOT_GIVEN) -&gt; LoadedPrompt\n</code></pre> <p>Get the prompt. If neither revision nor label is specified then the prompt with latest revision is returned.</p> <p>Project is loaded from the config by default. You can specify the project name of the prompt if you want to override the value from the config.</p> <p>By default, once a prompt is retrieved it's cached. You can disable caching.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the prompt to retrieve.</p> required <code>revision</code> <code>Optional[int]</code> <p>Optional specific revision number to retrieve. If not specified, the latest revision is used.</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>Optional label to filter by. If specified, only prompts with this label will be returned.</p> <code>None</code> <code>project</code> <code>Union[str, Type[NOT_GIVEN]]</code> <p>Optional project name override. If not specified, the project name from config is used.</p> <code>NOT_GIVEN</code> <code>disable_cache</code> <code>bool</code> <p>If True, bypasses the cache for both reading and writing.</p> <code>False</code> <code>provider</code> <code>Union[PromptProvider, _DefaultProviders, Sequence[Union[PromptProvider, _DefaultProviders]], Type[NOT_GIVEN]]</code> <p>The provider(s) to use for retrieving prompts. Can be a string identifier ('local', 'api'),      a PromptProvider instance, or a sequence of these. If not specified, defaults to config setting.</p> <code>NOT_GIVEN</code> <code>engine</code> <code>Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]]</code> <p>The template engine to use for rendering prompts. Can be a string identifier ('f-string', 'mustache', 'jinja2')    or a TemplateEngine instance. If not specified, defaults to config setting.</p> <code>NOT_GIVEN</code> <p>Returns:</p> Name Type Description <code>LoadedPrompt</code> <code>LoadedPrompt</code> <p>The retrieved prompt object.</p> <p>Raises:</p> Type Description <code>PromptNotFoundError</code> <p>If the prompt could not be found with the specified parameters.</p> <code>ValueError</code> <p>If the provided provider or engine is invalid.</p> <code>PromptProviderError</code> <p>If there was an error communicating with the prompt provider.</p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>def get(\n    self,\n    name: str,\n    revision: Optional[int] = None,\n    label: Optional[str] = None,\n    project: Union[str, Type[NOT_GIVEN]] = NOT_GIVEN,\n    disable_cache: bool = False,\n    provider: Union[\n        PromptProvider,\n        _DefaultProviders,\n        Sequence[Union[PromptProvider, _DefaultProviders]],\n        Type[NOT_GIVEN],\n    ] = NOT_GIVEN,\n    engine: Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]] = NOT_GIVEN,\n) -&gt; LoadedPrompt:\n    \"\"\"\n    Get the prompt.\n    If neither revision nor label is specified then the prompt with latest revision is returned.\n\n    Project is loaded from the config by default.\n    You can specify the project name of the prompt if you want to override the value from the config.\n\n    By default, once a prompt is retrieved it's cached. You can disable caching.\n\n    Args:\n        name: The name of the prompt to retrieve.\n        revision: Optional specific revision number to retrieve. If not specified, the latest revision is used.\n        label: Optional label to filter by. If specified, only prompts with this label will be returned.\n        project: Optional project name override. If not specified, the project name from config is used.\n        disable_cache: If True, bypasses the cache for both reading and writing.\n        provider: The provider(s) to use for retrieving prompts. Can be a string identifier ('local', 'api'),\n                 a PromptProvider instance, or a sequence of these. If not specified, defaults to config setting.\n        engine: The template engine to use for rendering prompts. Can be a string identifier ('f-string', 'mustache', 'jinja2')\n               or a TemplateEngine instance. If not specified, defaults to config setting.\n\n    Returns:\n        LoadedPrompt: The retrieved prompt object.\n\n    Raises:\n        PromptNotFoundError: If the prompt could not be found with the specified parameters.\n        ValueError: If the provided provider or engine is invalid.\n        PromptProviderError: If there was an error communicating with the prompt provider.\n    \"\"\"\n    project_name: str = self._resolve_project(project)\n    resolved_providers: list[PromptProvider] = self._resolve_providers(provider, self._provider_factory)\n    resolved_engine: TemplateEngine = self._resolve_engine(engine)\n\n    cache_key: _CacheKey = _CacheKey(project_name=project_name, prompt_name=name, revision=revision, label=label)\n    if not disable_cache:\n        cached_prompt: Optional[LoadedPrompt] = self._cache.get(cache_key)\n        if cached_prompt is not None:\n            return cached_prompt\n\n    prompt: Optional[LoadedPrompt] = None\n    provider_errors: list[str] = []\n\n    for i, prompt_provider in enumerate(resolved_providers):\n        log.debug(\"Trying prompt provider %d (%s)\", i + 1, prompt_provider.__class__.__name__)\n        try:\n            prompt = prompt_provider.get_prompt(name, revision, label, project_name, engine=resolved_engine)\n            if prompt is not None:\n                log.debug(\"Prompt found using provider %s\", prompt_provider.__class__.__name__)\n                break\n        except PromptProviderConnectionError as e:\n            provider_errors.append(str(e))\n            continue\n        except PromptProviderAuthenticationError as e:\n            provider_errors.append(str(e))\n            continue\n        except Exception as e:\n            provider_errors.append(f\"Unexpected error from provider {prompt_provider.__class__.__name__}: {str(e)}\")\n            continue\n\n    if prompt is None:\n        if provider_errors:\n            error_msg: str = self._format_provider_errors(provider_errors)\n            raise PromptNotFoundError(\n                name=name, project=project_name, revision=revision, label=label\n            ) from Exception(error_msg)\n        else:\n            raise PromptNotFoundError(name=name, project=project_name, revision=revision, label=label)\n\n    if not disable_cache:\n        self._cache.put(cache_key, prompt)\n\n    return prompt\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.clients.PromptClient.push","title":"push","text":"<pre><code>push(prompt: Prompt, project: Union[str, Type[NOT_GIVEN]] = NOT_GIVEN, engine: Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]] = NOT_GIVEN) -&gt; LoadedPrompt\n</code></pre> <p>Push a prompt to the API, creating a new revision only if needed.</p> <p>If a prompt revision with the same normalized body and metadata already exists, the existing revision will be returned. If the metadata differs, a new revision will be created.</p> <p>The engine parameter is only used to set property on output LoadedPrompt object. It is not persisted in any way and doesn't affect how the prompt is stored in Patronus AI Platform.</p> <p>Note that when a new prompt definition is created, the description is used as provided. However, when creating a new revision for an existing prompt definition, the description parameter doesn't update the existing prompt definition's description.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Prompt</code> <p>The prompt to push</p> required <code>project</code> <code>Union[str, Type[NOT_GIVEN]]</code> <p>Optional project name override. If not specified, the project name from config is used.</p> <code>NOT_GIVEN</code> <code>engine</code> <code>Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]]</code> <p>The template engine to use for rendering the returned prompt. If not specified, defaults to config setting.</p> <code>NOT_GIVEN</code> <p>Returns:</p> Name Type Description <code>LoadedPrompt</code> <code>LoadedPrompt</code> <p>The created or existing prompt revision</p> <p>Raises:</p> Type Description <code>PromptProviderError</code> <p>If there was an error communicating with the prompt provider.</p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>def push(\n    self,\n    prompt: Prompt,\n    project: Union[str, Type[NOT_GIVEN]] = NOT_GIVEN,\n    engine: Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]] = NOT_GIVEN,\n) -&gt; LoadedPrompt:\n    \"\"\"\n    Push a prompt to the API, creating a new revision only if needed.\n\n    If a prompt revision with the same normalized body and metadata already exists,\n    the existing revision will be returned. If the metadata differs, a new revision will be created.\n\n    The engine parameter is only used to set property on output LoadedPrompt object.\n    It is not persisted in any way and doesn't affect how the prompt is stored in Patronus AI Platform.\n\n    Note that when a new prompt definition is created, the description is used as provided.\n    However, when creating a new revision for an existing prompt definition, the\n    description parameter doesn't update the existing prompt definition's description.\n\n    Args:\n        prompt: The prompt to push\n        project: Optional project name override. If not specified, the project name from config is used.\n        engine: The template engine to use for rendering the returned prompt. If not specified, defaults to config setting.\n\n    Returns:\n        LoadedPrompt: The created or existing prompt revision\n\n    Raises:\n        PromptProviderError: If there was an error communicating with the prompt provider.\n    \"\"\"\n    project_name: str = self._resolve_project(project)\n    resolved_engine: TemplateEngine = self._resolve_engine(engine)\n\n    normalized_body_sha256 = calculate_normalized_body_hash(prompt.body)\n\n    cli = context.get_api_client().prompts\n    # Try to find existing revision with same hash\n    resp = cli.list_revisions(\n        prompt_name=prompt.name,\n        project_name=project_name,\n        normalized_body_sha256=normalized_body_sha256,\n    )\n\n    # Variables for create_revision parameters\n    prompt_id = patronus_api.NOT_GIVEN\n    prompt_name = prompt.name\n    create_new_prompt = True\n    prompt_def = None\n\n    # If we found a matching revision, check if metadata is the same\n    if resp.prompt_revisions:\n        log.debug(\"Found %d revisions with matching body hash\", len(resp.prompt_revisions))\n        prompt_id = resp.prompt_revisions[0].prompt_definition_id\n        create_new_prompt = False\n\n        resp_pd = cli.list_definitions(prompt_id=prompt_id, limit=1)\n        if not resp_pd.prompt_definitions:\n            raise PromptProviderError(\n                \"Prompt revision has been found but prompt definition was not found. This should not happen\"\n            )\n        prompt_def = resp_pd.prompt_definitions[0]\n\n        # Check if the provided description is different from existing one and warn if so\n        if prompt.description is not None and prompt.description != prompt_def.description:\n            warnings.warn(\n                f\"Prompt description ({prompt.description!r}) differs from the existing one \"\n                f\"({prompt_def.description!r}). The description won't be updated.\"\n            )\n\n        new_metadata_cmp = json.dumps(prompt.metadata, sort_keys=True)\n        for rev in resp.prompt_revisions:\n            metadata_cmp = json.dumps(rev.metadata, sort_keys=True)\n            if new_metadata_cmp == metadata_cmp:\n                log.debug(\"Found existing revision with matching metadata, returning revision %d\", rev.revision)\n                return self._api_provider._create_loaded_prompt(\n                    prompt_revision=rev,\n                    prompt_def=prompt_def,\n                    engine=resolved_engine,\n                )\n\n        # For existing prompt, don't need name/project\n        prompt_name = patronus_api.NOT_GIVEN\n        project_name = patronus_api.NOT_GIVEN\n    else:\n        # No matching revisions found, will create new prompt\n        log.debug(\"No revisions with matching body hash found, creating new prompt and revision\")\n\n    # Create a new revision with appropriate parameters\n    log.debug(\n        \"Creating new revision (new_prompt=%s, prompt_id=%s, prompt_name=%s)\",\n        create_new_prompt,\n        prompt_id if prompt_id != patronus_api.NOT_GIVEN else \"NOT_GIVEN\",\n        prompt_name if prompt_name != patronus_api.NOT_GIVEN else \"NOT_GIVEN\",\n    )\n    resp = cli.create_revision(\n        body=prompt.body,\n        prompt_id=prompt_id,\n        prompt_name=prompt_name,\n        project_name=project_name if create_new_prompt else patronus_api.NOT_GIVEN,\n        prompt_description=prompt.description,\n        metadata=prompt.metadata,\n    )\n\n    prompt_revision = resp.prompt_revision\n\n    # If we created a new prompt, we need to fetch the definition\n    if create_new_prompt:\n        resp_pd = cli.list_definitions(prompt_id=prompt_revision.prompt_definition_id, limit=1)\n        if not resp_pd.prompt_definitions:\n            raise PromptProviderError(\n                \"Prompt revision has been created but prompt definition was not found. This should not happen\"\n            )\n        prompt_def = resp_pd.prompt_definitions[0]\n\n    return self._api_provider._create_loaded_prompt(prompt_revision, prompt_def, resolved_engine)\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.clients.AsyncPromptClient","title":"AsyncPromptClient","text":"<pre><code>AsyncPromptClient(provider_factory: Optional[ProviderFactory] = None)\n</code></pre> <p>               Bases: <code>PromptClientMixin</code></p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>def __init__(self, provider_factory: Optional[ProviderFactory] = None) -&gt; None:\n    self._cache: AsyncPromptCache = AsyncPromptCache()\n    self._provider_factory: ProviderFactory = provider_factory or {\n        \"local\": lambda: LocalPromptProvider(),\n        \"api\": lambda: APIPromptProvider(),\n    }\n    self._api_provider = APIPromptProvider()\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.clients.AsyncPromptClient.get","title":"get  <code>async</code>","text":"<pre><code>get(name: str, revision: Optional[int] = None, label: Optional[str] = None, project: Union[str, Type[NOT_GIVEN]] = NOT_GIVEN, disable_cache: bool = False, provider: Union[PromptProvider, _DefaultProviders, Sequence[Union[PromptProvider, _DefaultProviders]], Type[NOT_GIVEN]] = NOT_GIVEN, engine: Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]] = NOT_GIVEN) -&gt; LoadedPrompt\n</code></pre> <p>Get the prompt asynchronously. If neither revision nor label is specified then the prompt with latest revision is returned.</p> <p>Project is loaded from the config by default. You can specify the project name of the prompt if you want to override the value from the config.</p> <p>By default, once a prompt is retrieved it's cached. You can disable caching.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the prompt to retrieve.</p> required <code>revision</code> <code>Optional[int]</code> <p>Optional specific revision number to retrieve. If not specified, the latest revision is used.</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>Optional label to filter by. If specified, only prompts with this label will be returned.</p> <code>None</code> <code>project</code> <code>Union[str, Type[NOT_GIVEN]]</code> <p>Optional project name override. If not specified, the project name from config is used.</p> <code>NOT_GIVEN</code> <code>disable_cache</code> <code>bool</code> <p>If True, bypasses the cache for both reading and writing.</p> <code>False</code> <code>provider</code> <code>Union[PromptProvider, _DefaultProviders, Sequence[Union[PromptProvider, _DefaultProviders]], Type[NOT_GIVEN]]</code> <p>The provider(s) to use for retrieving prompts. Can be a string identifier ('local', 'api'),      a PromptProvider instance, or a sequence of these. If not specified, defaults to config setting.</p> <code>NOT_GIVEN</code> <code>engine</code> <code>Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]]</code> <p>The template engine to use for rendering prompts. Can be a string identifier ('f-string', 'mustache', 'jinja2')    or a TemplateEngine instance. If not specified, defaults to config setting.</p> <code>NOT_GIVEN</code> <p>Returns:</p> Name Type Description <code>LoadedPrompt</code> <code>LoadedPrompt</code> <p>The retrieved prompt object.</p> <p>Raises:</p> Type Description <code>PromptNotFoundError</code> <p>If the prompt could not be found with the specified parameters.</p> <code>ValueError</code> <p>If the provided provider or engine is invalid.</p> <code>PromptProviderError</code> <p>If there was an error communicating with the prompt provider.</p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>async def get(\n    self,\n    name: str,\n    revision: Optional[int] = None,\n    label: Optional[str] = None,\n    project: Union[str, Type[NOT_GIVEN]] = NOT_GIVEN,\n    disable_cache: bool = False,\n    provider: Union[\n        PromptProvider, _DefaultProviders, Sequence[Union[PromptProvider, _DefaultProviders]], Type[NOT_GIVEN]\n    ] = NOT_GIVEN,\n    engine: Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]] = NOT_GIVEN,\n) -&gt; LoadedPrompt:\n    \"\"\"\n    Get the prompt asynchronously.\n    If neither revision nor label is specified then the prompt with latest revision is returned.\n\n    Project is loaded from the config by default.\n    You can specify the project name of the prompt if you want to override the value from the config.\n\n    By default, once a prompt is retrieved it's cached. You can disable caching.\n\n    Args:\n        name: The name of the prompt to retrieve.\n        revision: Optional specific revision number to retrieve. If not specified, the latest revision is used.\n        label: Optional label to filter by. If specified, only prompts with this label will be returned.\n        project: Optional project name override. If not specified, the project name from config is used.\n        disable_cache: If True, bypasses the cache for both reading and writing.\n        provider: The provider(s) to use for retrieving prompts. Can be a string identifier ('local', 'api'),\n                 a PromptProvider instance, or a sequence of these. If not specified, defaults to config setting.\n        engine: The template engine to use for rendering prompts. Can be a string identifier ('f-string', 'mustache', 'jinja2')\n               or a TemplateEngine instance. If not specified, defaults to config setting.\n\n    Returns:\n        LoadedPrompt: The retrieved prompt object.\n\n    Raises:\n        PromptNotFoundError: If the prompt could not be found with the specified parameters.\n        ValueError: If the provided provider or engine is invalid.\n        PromptProviderError: If there was an error communicating with the prompt provider.\n    \"\"\"\n    project_name: str = self._resolve_project(project)\n    resolved_providers: list[PromptProvider] = self._resolve_providers(provider, self._provider_factory)\n    resolved_engine: TemplateEngine = self._resolve_engine(engine)\n\n    cache_key: _CacheKey = _CacheKey(project_name=project_name, prompt_name=name, revision=revision, label=label)\n    if not disable_cache:\n        cached_prompt: Optional[LoadedPrompt] = await self._cache.get(cache_key)\n        if cached_prompt is not None:\n            return cached_prompt\n\n    prompt: Optional[LoadedPrompt] = None\n    provider_errors: list[str] = []\n\n    for i, prompt_provider in enumerate(resolved_providers):\n        log.debug(\"Trying prompt provider %d (%s) async\", i + 1, prompt_provider.__class__.__name__)\n        try:\n            prompt = await prompt_provider.aget_prompt(name, revision, label, project_name, engine=resolved_engine)\n            if prompt is not None:\n                log.debug(\"Prompt found using async provider %s\", prompt_provider.__class__.__name__)\n                break\n        except PromptProviderConnectionError as e:\n            provider_errors.append(str(e))\n            continue\n        except PromptProviderAuthenticationError as e:\n            provider_errors.append(str(e))\n            continue\n        except Exception as e:\n            provider_errors.append(f\"Unexpected error from provider {prompt_provider.__class__.__name__}: {str(e)}\")\n            continue\n\n    if prompt is None:\n        if provider_errors:\n            error_msg: str = self._format_provider_errors(provider_errors)\n            raise PromptNotFoundError(\n                name=name, project=project_name, revision=revision, label=label\n            ) from Exception(error_msg)\n        else:\n            raise PromptNotFoundError(name=name, project=project_name, revision=revision, label=label)\n\n    if not disable_cache:\n        await self._cache.put(cache_key, prompt)\n\n    return prompt\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.clients.AsyncPromptClient.push","title":"push  <code>async</code>","text":"<pre><code>push(prompt: Prompt, project: Union[str, Type[NOT_GIVEN]] = NOT_GIVEN, engine: Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]] = NOT_GIVEN) -&gt; LoadedPrompt\n</code></pre> <p>Push a prompt to the API asynchronously, creating a new revision only if needed.</p> <p>If a prompt revision with the same normalized body and metadata already exists, the existing revision will be returned. If the metadata differs, a new revision will be created.</p> <p>The engine parameter is only used to set property on output LoadedPrompt object. It is not persisted in any way and doesn't affect how the prompt is stored in Patronus AI Platform.</p> <p>Note that when a new prompt definition is created, the description is used as provided. However, when creating a new revision for an existing prompt definition, the description parameter doesn't update the existing prompt definition's description.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Prompt</code> <p>The prompt to push</p> required <code>project</code> <code>Union[str, Type[NOT_GIVEN]]</code> <p>Optional project name override. If not specified, the project name from config is used.</p> <code>NOT_GIVEN</code> <code>engine</code> <code>Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]]</code> <p>The template engine to use for rendering the returned prompt. If not specified, defaults to config setting.</p> <code>NOT_GIVEN</code> <p>Returns:</p> Name Type Description <code>LoadedPrompt</code> <code>LoadedPrompt</code> <p>The created or existing prompt revision</p> <p>Raises:</p> Type Description <code>PromptProviderError</code> <p>If there was an error communicating with the prompt provider.</p> Source code in <code>src/patronus/prompts/clients.py</code> <pre><code>async def push(\n    self,\n    prompt: Prompt,\n    project: Union[str, Type[NOT_GIVEN]] = NOT_GIVEN,\n    engine: Union[TemplateEngine, DefaultTemplateEngines, Type[NOT_GIVEN]] = NOT_GIVEN,\n) -&gt; LoadedPrompt:\n    \"\"\"\n    Push a prompt to the API asynchronously, creating a new revision only if needed.\n\n    If a prompt revision with the same normalized body and metadata already exists,\n    the existing revision will be returned. If the metadata differs, a new revision will be created.\n\n    The engine parameter is only used to set property on output LoadedPrompt object.\n    It is not persisted in any way and doesn't affect how the prompt is stored in Patronus AI Platform.\n\n    Note that when a new prompt definition is created, the description is used as provided.\n    However, when creating a new revision for an existing prompt definition, the\n    description parameter doesn't update the existing prompt definition's description.\n\n    Args:\n        prompt: The prompt to push\n        project: Optional project name override. If not specified, the project name from config is used.\n        engine: The template engine to use for rendering the returned prompt. If not specified, defaults to config setting.\n\n    Returns:\n        LoadedPrompt: The created or existing prompt revision\n\n    Raises:\n        PromptProviderError: If there was an error communicating with the prompt provider.\n    \"\"\"\n    project_name: str = self._resolve_project(project)\n    resolved_engine: TemplateEngine = self._resolve_engine(engine)\n\n    normalized_body_sha256 = calculate_normalized_body_hash(prompt.body)\n\n    cli = context.get_async_api_client().prompts\n    # Try to find existing revision with same hash\n    resp = await cli.list_revisions(\n        prompt_name=prompt.name,\n        project_name=project_name,\n        normalized_body_sha256=normalized_body_sha256,\n    )\n\n    # Variables for create_revision parameters\n    prompt_id = patronus_api.NOT_GIVEN\n    prompt_name = prompt.name\n    create_new_prompt = True\n    prompt_def = None\n\n    # If we found a matching revision, check if metadata is the same\n    if resp.prompt_revisions:\n        log.debug(\"Found %d revisions with matching body hash\", len(resp.prompt_revisions))\n        prompt_id = resp.prompt_revisions[0].prompt_definition_id\n        create_new_prompt = False\n\n        resp_pd = await cli.list_definitions(prompt_id=prompt_id, limit=1)\n        if not resp_pd.prompt_definitions:\n            raise PromptProviderError(\n                \"Prompt revision has been found but prompt definition was not found. This should not happen\"\n            )\n        prompt_def = resp_pd.prompt_definitions[0]\n\n        # Check if the provided description is different from existing one and warn if so\n        if prompt.description is not None and prompt.description != prompt_def.description:\n            warnings.warn(\n                f\"Prompt description ({prompt.description!r}) differs from the existing one \"\n                f\"({prompt_def.description!r}). The description won't be updated.\"\n            )\n\n        new_metadata_cmp = json.dumps(prompt.metadata, sort_keys=True)\n        for rev in resp.prompt_revisions:\n            metadata_cmp = json.dumps(rev.metadata, sort_keys=True)\n            if new_metadata_cmp == metadata_cmp:\n                log.debug(\"Found existing revision with matching metadata, returning revision %d\", rev.revision)\n                return self._api_provider._create_loaded_prompt(\n                    prompt_revision=rev,\n                    prompt_def=prompt_def,\n                    engine=resolved_engine,\n                )\n\n        # For existing prompt, don't need name/project\n        prompt_name = patronus_api.NOT_GIVEN\n        project_name = patronus_api.NOT_GIVEN\n    else:\n        # No matching revisions found, will create new prompt\n        log.debug(\"No revisions with matching body hash found, creating new prompt and revision\")\n\n    # Create a new revision with appropriate parameters\n    log.debug(\n        \"Creating new revision (new_prompt=%s, prompt_id=%s, prompt_name=%s)\",\n        create_new_prompt,\n        prompt_id if prompt_id != patronus_api.NOT_GIVEN else \"NOT_GIVEN\",\n        prompt_name if prompt_name != patronus_api.NOT_GIVEN else \"NOT_GIVEN\",\n    )\n    resp = await cli.create_revision(\n        body=prompt.body,\n        prompt_id=prompt_id,\n        prompt_name=prompt_name,\n        project_name=project_name if create_new_prompt else patronus_api.NOT_GIVEN,\n        prompt_description=prompt.description,\n        metadata=prompt.metadata,\n    )\n\n    prompt_revision = resp.prompt_revision\n\n    # If we created a new prompt, we need to fetch the definition\n    if create_new_prompt:\n        resp_pd = await cli.list_definitions(prompt_id=prompt_revision.prompt_definition_id, limit=1)\n        if not resp_pd.prompt_definitions:\n            raise PromptProviderError(\n                \"Prompt revision has been created but prompt definition was not found. This should not happen\"\n            )\n        prompt_def = resp_pd.prompt_definitions[0]\n\n    return self._api_provider._create_loaded_prompt(prompt_revision, prompt_def, resolved_engine)\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.models","title":"models","text":""},{"location":"api_ref/prompts/#patronus.prompts.models.BasePrompt","title":"BasePrompt","text":""},{"location":"api_ref/prompts/#patronus.prompts.models.BasePrompt.with_engine","title":"with_engine","text":"<pre><code>with_engine(engine: Union[TemplateEngine, DefaultTemplateEngines]) -&gt; typing.Self\n</code></pre> <p>Create a new prompt with the specified template engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Union[TemplateEngine, DefaultTemplateEngines]</code> <p>Either a TemplateEngine instance or a string identifier ('f-string', 'mustache', 'jinja2')</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new prompt instance with the specified engine</p> Source code in <code>src/patronus/prompts/models.py</code> <pre><code>def with_engine(self, engine: Union[TemplateEngine, DefaultTemplateEngines]) -&gt; typing.Self:\n    \"\"\"\n    Create a new prompt with the specified template engine.\n\n    Args:\n        engine: Either a TemplateEngine instance or a string identifier ('f-string', 'mustache', 'jinja2')\n\n    Returns:\n        A new prompt instance with the specified engine\n    \"\"\"\n    resolved_engine = get_template_engine(engine)\n    return dataclasses.replace(self, _engine=resolved_engine)\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.models.BasePrompt.render","title":"render","text":"<pre><code>render(**kwargs: Any) -&gt; str\n</code></pre> <p>Render the prompt template with the provided arguments.</p> <p>If no engine is set on the prompt, the default engine from context/config will be used. If no arguments are provided, the template body is returned as-is.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Template arguments to be rendered in the prompt body</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The rendered prompt</p> Source code in <code>src/patronus/prompts/models.py</code> <pre><code>def render(self, **kwargs: Any) -&gt; str:\n    \"\"\"\n    Render the prompt template with the provided arguments.\n\n    If no engine is set on the prompt, the default engine from context/config will be used.\n    If no arguments are provided, the template body is returned as-is.\n\n    Args:\n        **kwargs: Template arguments to be rendered in the prompt body\n\n    Returns:\n        The rendered prompt\n    \"\"\"\n    if not kwargs:\n        return self.body\n\n    engine = self._engine\n    if engine is None:\n        # Get default engine from context\n        engine_name = context.get_prompts_config().templating_engine\n        engine = get_template_engine(engine_name)\n\n    return engine.render(self.body, **kwargs)\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.models.calculate_normalized_body_hash","title":"calculate_normalized_body_hash","text":"<pre><code>calculate_normalized_body_hash(body: str) -&gt; str\n</code></pre> <p>Calculate the SHA-256 hash of normalized prompt body.</p> <p>Normalization is done by stripping whitespace from the start and end of the body.</p> <p>Parameters:</p> Name Type Description Default <code>body</code> <code>str</code> <p>The prompt body</p> required <p>Returns:</p> Type Description <code>str</code> <p>SHA-256 hash of the normalized body</p> Source code in <code>src/patronus/prompts/models.py</code> <pre><code>def calculate_normalized_body_hash(body: str) -&gt; str:\n    \"\"\"Calculate the SHA-256 hash of normalized prompt body.\n\n    Normalization is done by stripping whitespace from the start and end of the body.\n\n    Args:\n        body: The prompt body\n\n    Returns:\n        SHA-256 hash of the normalized body\n    \"\"\"\n    normalized_body = body.strip()\n    return hashlib.sha256(normalized_body.encode()).hexdigest()\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.templating","title":"templating","text":""},{"location":"api_ref/prompts/#patronus.prompts.templating.TemplateEngine","title":"TemplateEngine","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api_ref/prompts/#patronus.prompts.templating.TemplateEngine.render","title":"render  <code>abstractmethod</code>","text":"<pre><code>render(template: str, **kwargs) -&gt; str\n</code></pre> <p>Render the template with the given arguments.</p> Source code in <code>src/patronus/prompts/templating.py</code> <pre><code>@abc.abstractmethod\ndef render(self, template: str, **kwargs) -&gt; str:\n    \"\"\"Render the template with the given arguments.\"\"\"\n</code></pre>"},{"location":"api_ref/prompts/#patronus.prompts.templating.get_template_engine","title":"get_template_engine","text":"<pre><code>get_template_engine(engine: Union[TemplateEngine, DefaultTemplateEngines]) -&gt; TemplateEngine\n</code></pre> <p>Convert a template engine name to an actual engine instance.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Union[TemplateEngine, DefaultTemplateEngines]</code> <p>Either a template engine instance or a string identifier ('f-string', 'mustache', 'jinja2')</p> required <p>Returns:</p> Type Description <code>TemplateEngine</code> <p>A template engine instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided engine string is not recognized</p> Source code in <code>src/patronus/prompts/templating.py</code> <pre><code>def get_template_engine(engine: Union[TemplateEngine, DefaultTemplateEngines]) -&gt; TemplateEngine:\n    \"\"\"\n    Convert a template engine name to an actual engine instance.\n\n    Args:\n        engine: Either a template engine instance or a string identifier ('f-string', 'mustache', 'jinja2')\n\n    Returns:\n        A template engine instance\n\n    Raises:\n        ValueError: If the provided engine string is not recognized\n    \"\"\"\n    if isinstance(engine, TemplateEngine):\n        return engine\n\n    if engine == \"f-string\":\n        return FStringTemplateEngine()\n    elif engine == \"mustache\":\n        return MustacheTemplateEngine()\n    elif engine == \"jinja2\":\n        return Jinja2TemplateEngine()\n\n    raise ValueError(\n        \"Provided engine must be an instance of TemplateEngine or \"\n        \"one of the default engines ('f-string', 'mustache', 'jinja2'). \"\n        f\"Instead got {engine!r}\"\n    )\n</code></pre>"},{"location":"api_ref/tracing/","title":"Tracing","text":""},{"location":"api_ref/tracing/#patronus.tracing","title":"patronus.tracing","text":""},{"location":"api_ref/tracing/#patronus.tracing.decorators","title":"decorators","text":""},{"location":"api_ref/tracing/#patronus.tracing.decorators.start_span","title":"start_span","text":"<pre><code>start_span(name: str, *, record_exception: bool = True, attributes: Optional[Attributes] = None) -&gt; Iterator[Optional[typing.Any]]\n</code></pre> <p>Context manager for creating and managing a trace span.</p> <p>This function is used to create a span within the current context using the tracer, allowing you to track execution timing or events within a specific block of code. The context is set by <code>patronus.init()</code> function. If SDK was not initialized, yielded value will be None.</p> <p>Example:</p> <pre><code>import patronus\n\npatronus.init()\n\n# Use context manager for finer-grained tracing\ndef complex_operation():\n    with patronus.start_span(\"Data preparation\"):\n        # Prepare data\n        pass\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the span.</p> required <code>record_exception</code> <code>bool</code> <p>Whether to record exceptions that occur within the span. Default is True.</p> <code>True</code> <code>attributes</code> <code>Optional[Attributes]</code> <p>Attributes to associate with the span, providing additional metadata.</p> <code>None</code> Source code in <code>src/patronus/tracing/decorators.py</code> <pre><code>@contextlib.contextmanager\ndef start_span(\n    name: str, *, record_exception: bool = True, attributes: Optional[Attributes] = None\n) -&gt; Iterator[Optional[typing.Any]]:\n    \"\"\"\n    Context manager for creating and managing a trace span.\n\n    This function is used to create a span within the current context using the tracer,\n    allowing you to track execution timing or events within a specific block of code.\n    The context is set by `patronus.init()` function. If SDK was not initialized, yielded value will be None.\n\n    Example:\n\n    ```python\n    import patronus\n\n    patronus.init()\n\n    # Use context manager for finer-grained tracing\n    def complex_operation():\n        with patronus.start_span(\"Data preparation\"):\n            # Prepare data\n            pass\n    ```\n\n\n    Args:\n        name (str): The name of the span.\n        record_exception (bool): Whether to record exceptions that occur within the span. Default is True.\n        attributes (Optional[Attributes]): Attributes to associate with the span, providing additional metadata.\n    \"\"\"\n    tracer = context.get_tracer_or_none()\n    if tracer is None:\n        yield\n        return\n    with tracer.start_as_current_span(\n        name,\n        record_exception=record_exception,\n        attributes=attributes,\n    ) as span:\n        yield span\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.decorators.traced","title":"traced","text":"<pre><code>traced(span_name: Optional[str] = None, *, log_args: bool = True, log_results: bool = True, log_exceptions: bool = True, disable_log: bool = False, attributes: Attributes = None, **kwargs: Any)\n</code></pre> <p>A decorator to trace function execution by recording a span for the traced function.</p> <p>Example:</p> <pre><code>import patronus\n\npatronus.init()\n\n# Trace a function with the @traced decorator\n@patronus.traced()\ndef process_input(user_query):\n    # Process the input\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>span_name</code> <code>Optional[str]</code> <p>The name of the traced span. Defaults to the function name if not provided.</p> <code>None</code> <code>log_args</code> <code>bool</code> <p>Whether to log the arguments passed to the function. Default is True.</p> <code>True</code> <code>log_results</code> <code>bool</code> <p>Whether to log the function's return value. Default is True.</p> <code>True</code> <code>log_exceptions</code> <code>bool</code> <p>Whether to log any exceptions raised while executing the function. Default is True.</p> <code>True</code> <code>disable_log</code> <code>bool</code> <p>Whether to disable logging the trace information. Default is False.</p> <code>False</code> <code>attributes</code> <code>Attributes</code> <p>Attributes to attach to the traced span. Default is None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for the decorator.</p> <code>{}</code> Source code in <code>src/patronus/tracing/decorators.py</code> <pre><code>def traced(\n    # Give name for the traced span. Defaults to a function name if not provided.\n    span_name: Optional[str] = None,\n    *,\n    # Whether to log function arguments.\n    log_args: bool = True,\n    # Whether to log function output.\n    log_results: bool = True,\n    # Whether to log an exception if one was raised.\n    log_exceptions: bool = True,\n    # Whether to prevent a log message to be created.\n    disable_log: bool = False,\n    attributes: Attributes = None,\n    **kwargs: typing.Any,\n):\n    \"\"\"\n    A decorator to trace function execution by recording a span for the traced function.\n\n    Example:\n\n    ```python\n    import patronus\n\n    patronus.init()\n\n    # Trace a function with the @traced decorator\n    @patronus.traced()\n    def process_input(user_query):\n        # Process the input\n    ```\n\n    Args:\n        span_name (Optional[str]): The name of the traced span. Defaults to the function name if not provided.\n        log_args (bool): Whether to log the arguments passed to the function. Default is True.\n        log_results (bool): Whether to log the function's return value. Default is True.\n        log_exceptions (bool): Whether to log any exceptions raised while executing the function. Default is True.\n        disable_log (bool): Whether to disable logging the trace information. Default is False.\n        attributes (Attributes): Attributes to attach to the traced span. Default is None.\n        **kwargs: Additional arguments for the decorator.\n    \"\"\"\n\n    def decorator(func):\n        name = span_name or func.__qualname__\n        sig = inspect.signature(func)\n        record_exception = not disable_log and log_exceptions\n\n        def log_call(fn_args: typing.Any, fn_kwargs: typing.Any, ret: typing.Any, exc: Exception):\n            if disable_log:\n                return\n\n            logger = context.get_pat_logger()\n            severity = SeverityNumber.INFO\n            body = {\"function.name\": name}\n            if log_args:\n                bound_args = sig.bind(*fn_args, **fn_kwargs)\n                body[\"function.arguments\"] = {**bound_args.arguments, **bound_args.arguments}\n            if log_results is not None and exc is None:\n                body[\"function.output\"] = ret\n            if log_exceptions and exc is not None:\n                module = type(exc).__module__\n                qualname = type(exc).__qualname__\n                exception_type = f\"{module}.{qualname}\" if module and module != \"builtins\" else qualname\n                body[\"exception.type\"] = exception_type\n                body[\"exception.message\"] = str(exc)\n                severity = SeverityNumber.ERROR\n            logger.log(body, log_type=LogTypes.trace, severity=severity)\n\n        @functools.wraps(func)\n        def wrapper_sync(*f_args, **f_kwargs):\n            tracer = context.get_tracer_or_none()\n            if tracer is None:\n                return func(*f_args, **f_kwargs)\n\n            exc = None\n            ret = None\n            with tracer.start_as_current_span(name, record_exception=record_exception, attributes=attributes):\n                try:\n                    ret = func(*f_args, **f_kwargs)\n                except Exception as e:\n                    exc = e\n                    raise exc\n                finally:\n                    log_call(f_args, f_kwargs, ret, exc)\n\n                return ret\n\n        @functools.wraps(func)\n        async def wrapper_async(*f_args, **f_kwargs):\n            tracer = context.get_tracer_or_none()\n            if tracer is None:\n                return await func(*f_args, **f_kwargs)\n\n            exc = None\n            ret = None\n            with tracer.start_as_current_span(name, record_exception=record_exception, attributes=attributes):\n                try:\n                    ret = await func(*f_args, **f_kwargs)\n                except Exception as e:\n                    exc = e\n                    raise exc\n                finally:\n                    log_call(f_args, f_kwargs, ret, exc)\n\n                return ret\n\n        if inspect.iscoroutinefunction(func):\n            wrapper_async._pat_traced = True\n            return wrapper_async\n        else:\n            wrapper_async._pat_traced = True\n            return wrapper_sync\n\n    return decorator\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.exporters","title":"exporters","text":"<p>This module provides exporter selection functionality for OpenTelemetry traces and logs. It handles protocol resolution based on Patronus configuration and standard OTEL environment variables.</p>"},{"location":"api_ref/tracing/#patronus.tracing.exporters.create_trace_exporter","title":"create_trace_exporter","text":"<pre><code>create_trace_exporter(endpoint: str, api_key: str, protocol: Optional[str] = None) -&gt; SpanExporter\n</code></pre> <p>Create a configured trace exporter instance.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The OTLP endpoint URL</p> required <code>api_key</code> <code>str</code> <p>Authentication key for Patronus services</p> required <code>protocol</code> <code>Optional[str]</code> <p>OTLP protocol override from Patronus configuration</p> <code>None</code> <p>Returns:</p> Type Description <code>SpanExporter</code> <p>Configured trace exporter instance</p> Source code in <code>src/patronus/tracing/exporters.py</code> <pre><code>def create_trace_exporter(endpoint: str, api_key: str, protocol: Optional[str] = None) -&gt; SpanExporter:\n    \"\"\"\n    Create a configured trace exporter instance.\n\n    Args:\n        endpoint: The OTLP endpoint URL\n        api_key: Authentication key for Patronus services\n        protocol: OTLP protocol override from Patronus configuration\n\n    Returns:\n        Configured trace exporter instance\n    \"\"\"\n    resolved_protocol = _resolve_otlp_protocol(protocol)\n\n    if resolved_protocol == \"http/protobuf\":\n        # For HTTP exporter, ensure endpoint has the correct path\n        if not endpoint.endswith(\"/v1/traces\"):\n            endpoint = endpoint.rstrip(\"/\") + \"/v1/traces\"\n        return OTLPSpanExporterHTTP(endpoint=endpoint, headers={\"x-api-key\": api_key})\n    else:\n        # For gRPC exporter, determine if connection should be insecure based on URL scheme\n        is_insecure = endpoint.startswith(\"http://\")\n        return OTLPSpanExporterGRPC(endpoint=endpoint, headers={\"x-api-key\": api_key}, insecure=is_insecure)\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.exporters.create_log_exporter","title":"create_log_exporter","text":"<pre><code>create_log_exporter(endpoint: str, api_key: str, protocol: Optional[str] = None) -&gt; LogExporter\n</code></pre> <p>Create a configured log exporter instance.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The OTLP endpoint URL</p> required <code>api_key</code> <code>str</code> <p>Authentication key for Patronus services</p> required <code>protocol</code> <code>Optional[str]</code> <p>OTLP protocol override from Patronus configuration</p> <code>None</code> <p>Returns:</p> Type Description <code>LogExporter</code> <p>Configured log exporter instance</p> Source code in <code>src/patronus/tracing/exporters.py</code> <pre><code>def create_log_exporter(endpoint: str, api_key: str, protocol: Optional[str] = None) -&gt; LogExporter:\n    \"\"\"\n    Create a configured log exporter instance.\n\n    Args:\n        endpoint: The OTLP endpoint URL\n        api_key: Authentication key for Patronus services\n        protocol: OTLP protocol override from Patronus configuration\n\n    Returns:\n        Configured log exporter instance\n    \"\"\"\n    resolved_protocol = _resolve_otlp_protocol(protocol)\n\n    if resolved_protocol == \"http/protobuf\":\n        # For HTTP exporter, ensure endpoint has the correct path\n        if not endpoint.endswith(\"/v1/logs\"):\n            endpoint = endpoint.rstrip(\"/\") + \"/v1/logs\"\n        return OTLPLogExporterHTTP(endpoint=endpoint, headers={\"x-api-key\": api_key})\n    else:\n        # For gRPC exporter, determine if connection should be insecure based on URL scheme\n        is_insecure = endpoint.startswith(\"http://\")\n        return OTLPLogExporterGRPC(endpoint=endpoint, headers={\"x-api-key\": api_key}, insecure=is_insecure)\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.tracer","title":"tracer","text":"<p>This module provides the implementation for tracing support using the OpenTelemetry SDK.</p>"},{"location":"api_ref/tracing/#patronus.tracing.tracer.PatronusAttributesSpanProcessor","title":"PatronusAttributesSpanProcessor","text":"<pre><code>PatronusAttributesSpanProcessor(project_name: str, app: Optional[str] = None, experiment_id: Optional[str] = None)\n</code></pre> <p>               Bases: <code>SpanProcessor</code></p> <p>Processor that adds Patronus-specific attributes to all spans.</p> <p>This processor ensures that each span includes the mandatory attributes: <code>project_name</code>, and optionally adds <code>app</code> or <code>experiment_id</code> attributes if they are provided during initialization.</p> Source code in <code>src/patronus/tracing/tracer.py</code> <pre><code>def __init__(self, project_name: str, app: Optional[str] = None, experiment_id: Optional[str] = None):\n    self.project_name = project_name\n    self.experiment_id = None\n    self.app = None\n\n    if experiment_id is not None:\n        self.experiment_id = experiment_id\n    else:\n        self.app = app\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.tracer.create_tracer_provider","title":"create_tracer_provider  <code>cached</code>","text":"<pre><code>create_tracer_provider(exporter_endpoint: str, api_key: str, scope: PatronusScope, protocol: Optional[str] = None) -&gt; TracerProvider\n</code></pre> <p>Creates and returns a cached TracerProvider configured with the specified exporter.</p> <p>The function utilizes an OpenTelemetry BatchSpanProcessor and an OTLPSpanExporter to initialize the tracer. The configuration is cached for reuse.</p> Source code in <code>src/patronus/tracing/tracer.py</code> <pre><code>@functools.lru_cache()\ndef create_tracer_provider(\n    exporter_endpoint: str,\n    api_key: str,\n    scope: context.PatronusScope,\n    protocol: Optional[str] = None,\n) -&gt; TracerProvider:\n    \"\"\"\n    Creates and returns a cached TracerProvider configured with the specified exporter.\n\n    The function utilizes an OpenTelemetry BatchSpanProcessor and an\n    OTLPSpanExporter to initialize the tracer. The configuration is cached for reuse.\n    \"\"\"\n    resource = None\n    if scope.service is not None:\n        resource = Resource.create({\"service.name\": scope.service})\n    provider = TracerProvider(resource=resource)\n    provider.add_span_processor(\n        PatronusAttributesSpanProcessor(\n            project_name=scope.project_name,\n            app=scope.app,\n            experiment_id=scope.experiment_id,\n        )\n    )\n    provider.add_span_processor(\n        BatchSpanProcessor(_create_exporter(endpoint=exporter_endpoint, api_key=api_key, protocol=protocol))\n    )\n    return provider\n</code></pre>"},{"location":"api_ref/tracing/#patronus.tracing.tracer.create_tracer","title":"create_tracer","text":"<pre><code>create_tracer(scope: PatronusScope, exporter_endpoint: str, api_key: str, protocol: Optional[str] = None) -&gt; trace.Tracer\n</code></pre> <p>Creates an OpenTelemetry (OTeL) tracer tied to the specified scope.</p> Source code in <code>src/patronus/tracing/tracer.py</code> <pre><code>def create_tracer(\n    scope: context.PatronusScope,\n    exporter_endpoint: str,\n    api_key: str,\n    protocol: Optional[str] = None,\n) -&gt; trace.Tracer:\n    \"\"\"\n    Creates an OpenTelemetry (OTeL) tracer tied to the specified scope.\n    \"\"\"\n    provider = create_tracer_provider(\n        exporter_endpoint=exporter_endpoint,\n        api_key=api_key,\n        scope=scope,\n        protocol=protocol,\n    )\n    return provider.get_tracer(\"patronus.sdk\")\n</code></pre>"},{"location":"evaluations/batching/","title":"Batch Evaluations","text":"<p>When evaluating multiple outputs or using multiple evaluators, Patronus provides efficient batch evaluation capabilities. This page covers how to perform batch evaluations and manage evaluation groups.</p>"},{"location":"evaluations/batching/#using-patronus-client","title":"Using Patronus Client","text":"<p>For more advanced batch evaluation needs, use the <code>Patronus</code> client:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nwith Patronus() as client:\n    # Run multiple evaluators in parallel\n    results = client.evaluate(\n        evaluators=[\n            RemoteEvaluator(\"judge\", \"patronus:is-helpful\"),\n            RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n        ],\n        task_input=\"What is quantum computing?\",\n        task_output=\"Quantum computing uses quantum bits or qubits to perform computations...\",\n        gold_answer=\"Computing that uses quantum phenomena like superposition and entanglement\"\n    )\n\n    # Check if all evaluations passed\n    if results.all_succeeded():\n        print(\"All evaluations passed!\")\n    else:\n        print(\"Some evaluations failed:\")\n        for failed in results.failed_evaluations():\n            print(f\"  - {failed.text_output}\")\n</code></pre> <p>The <code>Patronus</code> client provides:</p> <ul> <li>Parallel evaluation execution</li> <li>Connection pooling</li> <li>Error handling</li> <li>Result aggregation</li> </ul>"},{"location":"evaluations/batching/#asynchronous-evaluation","title":"Asynchronous Evaluation","text":"<p>For asynchronous workflows, use <code>AsyncPatronus</code>:</p> <pre><code>import asyncio\nfrom patronus import init\nfrom patronus.pat_client import AsyncPatronus\nfrom patronus.evals import AsyncRemoteEvaluator\n\ninit()\n\n\nasync def evaluate_responses():\n    async with AsyncPatronus() as client:\n        # Run evaluations asynchronously\n        results = await client.evaluate(\n            evaluators=[\n                AsyncRemoteEvaluator(\"judge\", \"patronus:is-helpful\"),\n                AsyncRemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n            ],\n            task_input=\"What is quantum computing?\",\n            task_output=\"Quantum computing uses quantum bits or qubits to perform computations...\",\n            gold_answer=\"Computing that uses quantum phenomena like superposition and entanglement\"\n        )\n\n        print(f\"Number of evaluations: {len(results.results)}\")\n        print(f\"All passed: {results.all_succeeded()}\")\n\n# Run the async function\nasyncio.run(evaluate_responses())\n</code></pre>"},{"location":"evaluations/batching/#background-evaluation","title":"Background Evaluation","text":"<p>For non-blocking evaluation, use the <code>evaluate_bg()</code> method:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nwith Patronus() as client:\n    # Start background evaluation\n    future = client.evaluate_bg(\n        evaluators=[\n            RemoteEvaluator(\"judge\", \"factual-accuracy\"),\n            RemoteEvaluator(\"judge\", \"patronus:helpfulness\")\n        ],\n        task_input=\"Explain how vaccines work.\",\n        task_output=\"Vaccines work by training the immune system to recognize and combat pathogens...\"\n    )\n\n    # Do other work while evaluation happens in background\n    print(\"Continuing with other tasks...\")\n\n    results = future.get()  # Blocks until complete\n\n    print(f\"Evaluation complete: {results.all_succeeded()}\")\n</code></pre> <p>The async version works similarly:</p> <pre><code>async with AsyncPatronus() as client:\n    # Start background evaluation\n    task = client.evaluate_bg(\n        evaluators=[...],\n        task_input=\"...\",\n        task_output=\"...\"\n    )\n\n    # Do other async work\n    await some_other_async_function()\n\n    # Get results when needed\n    results = await task\n</code></pre>"},{"location":"evaluations/batching/#working-with-evaluation-results","title":"Working with Evaluation Results","text":"<p>The <code>evaluate()</code> method returns an <code>EvaluationContainer</code> with several useful methods:</p> <pre><code>results = client.evaluate(evaluators=[...], task_input=\"...\", task_output=\"...\")\n\nif results.any_failed():\n    print(\"Some evaluations failed\")\n\nif results.all_succeeded():\n    print(\"All evaluations passed\")\n\nfor failed in results.failed_evaluations():\n    print(f\"Failed: {failed.text_output}\")\n\nfor success in results.succeeded_evaluations():\n    print(f\"Passed: {success.text_output}\")\n\nif results.has_exception():\n    results.raise_on_exception()  # Re-raise any exceptions that occurred\n</code></pre>"},{"location":"evaluations/batching/#example-comprehensive-quality-check","title":"Example: Comprehensive Quality Check","text":"<p>Here's a complete example of batch evaluation for content quality:</p> <pre><code>from patronus import init\nfrom patronus.pat_client import Patronus\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\ndef check_content_quality(question, answer):\n    with Patronus() as client:\n        results = client.evaluate(\n            evaluators=[\n                RemoteEvaluator(\"judge\", \"factual-accuracy\"),\n                RemoteEvaluator(\"judge\", \"helpfulness\"),\n                RemoteEvaluator(\"judge\", \"coherence\"),\n                RemoteEvaluator(\"judge\", \"grammar\"),\n                RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n            ],\n            task_input=question,\n            task_output=answer\n        )\n\n        if results.any_failed():\n            print(\"Content quality check failed\")\n            for failed in results.failed_evaluations():\n                print(f\"- Failed check: {failed.text_output}\")\n                print(f\"  Explanation: {failed.explanation}\")\n            return False\n\n        print(\"Content passed all quality checks\")\n        return True\n\ncheck_content_quality(\n    \"What is the capital of France?\",\n    \"The capital of France is Paris, which is located on the Seine River.\"\n)\n</code></pre>"},{"location":"evaluations/batching/#using-the-bundled_eval-context-manager","title":"Using the <code>bundled_eval()</code> Context Manager","text":"<p>The <code>bundled_eval()</code> is a lower-level context manager that groups multiple evaluations together based on their arguments. This is particularly useful when working with multiple user-defined evaluators that don't conform to the Patronus structured evaluator format.</p> <pre><code>import patronus\nfrom patronus.evals import bundled_eval, evaluator\n\npatronus.init()\n\n@evaluator()\ndef exact_match(actual, expected) -&gt; bool:\n    return actual == expected\n\n@evaluator()\ndef iexact_match(actual: str, expected: str) -&gt; bool:\n    return actual.strip().lower() == expected.strip().lower()\n\n# Group these evaluations together in a single trace and single log record\nwith bundled_eval():\n    exact_match(\"string\", \"string\")\n    iexact_match(\"string\", \"string\")\n</code></pre>"},{"location":"evaluations/evaluators/","title":"User-Defined Evaluators","text":"<p>Evaluators are the core building blocks of Patronus's evaluation system. This page covers how to create and use your own custom evaluators to assess LLM outputs according to your specific criteria.</p>"},{"location":"evaluations/evaluators/#creating-basic-evaluators","title":"Creating Basic Evaluators","text":"<p>The simplest way to create an evaluator is with the <code>@evaluator()</code> decorator:</p> <pre><code>from patronus import evaluator\n\n@evaluator()\ndef keyword_match(text: str, keywords: list[str]) -&gt; float:\n    \"\"\"\n    Evaluates whether the text contains the specified keywords.\n    Returns a score between 0.0 and 1.0 based on the percentage of matched keywords.\n    \"\"\"\n    matches = sum(keyword.lower() in text.lower() for keyword in keywords)\n    return matches / len(keywords) if keywords else 0.0\n</code></pre> <p>This decorator automatically:</p> <ul> <li>Integrates with the Patronus tracing</li> <li>Exports evaluation results to the Patronus Platform</li> </ul>"},{"location":"evaluations/evaluators/#flexible-input-and-output","title":"Flexible Input and Output","text":"<p>User-defined evaluators can accept any parameters and return several types of results:</p> <pre><code># Boolean evaluator (pass/fail)\n@evaluator()\ndef contains_answer(text: str, answer: str) -&gt; bool:\n    return answer.lower() in text.lower()\n\n\n# Numeric evaluator (score)\n@evaluator()\ndef semantic_similarity(text1: str, text2: str) -&gt; float:\n    # Simple example - in practice use proper semantic similarity\n    words1, words2 = set(text1.lower().split()), set(text2.lower().split())\n    intersection = words1.intersection(words2)\n    union = words1.union(words2)\n    return len(intersection) / len(union) if union else 0.0\n\n\n# String evaluator\n@evaluator()\ndef tone_classifier(text: str) -&gt; str:\n    positive = ['good', 'excellent', 'great', 'helpful']\n    negative = ['bad', 'poor', 'unhelpful', 'wrong']\n\n    pos_count = sum(word in text.lower() for word in positive)\n    neg_count = sum(word in text.lower() for word in negative)\n\n    if pos_count &gt; neg_count:\n        return \"positive\"\n    elif neg_count &gt; pos_count:\n        return \"negative\"\n    else:\n        return \"neutral\"\n</code></pre>"},{"location":"evaluations/evaluators/#return-types","title":"Return Types","text":"<p>Evaluators can return different types which are automatically converted to <code>EvaluationResult</code> objects:</p> <ul> <li>Boolean: <code>True</code>/<code>False</code> indicating pass/fail</li> <li>Float/Integer: Numerical scores (typically between 0-1)</li> <li>String: Text output categorizing the result</li> <li>EvaluationResult: Complete evaluation with scores, explanations, etc.</li> </ul>"},{"location":"evaluations/evaluators/#using-evaluationresult","title":"Using EvaluationResult","text":"<p>For more detailed evaluations, return an <code>EvaluationResult</code> object:</p> <pre><code>from patronus import evaluator\nfrom patronus.evals import EvaluationResult\n\n@evaluator()\ndef comprehensive_evaluation(response: str, reference: str) -&gt; EvaluationResult:\n    # Example implementation - replace with actual logic\n    has_keywords = all(word in response.lower() for word in [\"important\", \"key\", \"concept\"])\n    accuracy = 0.85  # Calculated accuracy score\n\n    return EvaluationResult(\n        score=accuracy,  # Numeric score (typically 0-1)\n        pass_=accuracy &gt;= 0.7,  # Boolean pass/fail\n        text_output=\"Satisfactory\" if accuracy &gt;= 0.7 else \"Needs improvement\",  # Category\n        explanation=f\"Response {'contains' if has_keywords else 'is missing'} key terms. Accuracy: {accuracy:.2f}\",\n        metadata={  # Additional structured data\n            \"has_required_keywords\": has_keywords,\n            \"response_length\": len(response),\n            \"accuracy\": accuracy\n        }\n    )\n</code></pre> <p>The <code>EvaluationResult</code> object can include:</p> <ul> <li>score: Numerical assessment (typically 0-1)</li> <li>pass_: Boolean pass/fail status</li> <li>text_output: Categorical or textual result</li> <li>explanation: Human-readable explanation of the result</li> <li>metadata: Additional structured data for analysis</li> <li>tags: Key-value pairs for filtering and organization</li> </ul>"},{"location":"evaluations/evaluators/#using-evaluators","title":"Using Evaluators","text":"<p>Once defined, evaluators can be used directly:</p> <pre><code># Use evaluators as normal function\nresult = keyword_match(\"The capital of France is Paris\", [\"capital\", \"France\", \"Paris\"])\nprint(f\"Score: {result}\")  # Output: Score: 1.0\n\n\n# Using class-based evaluator\nsafety_check = ContentSafetyEvaluator()\nresult = safety_check.evaluate(\n    task_output=\"This is a helpful and safe response.\"\n)\nprint(f\"Safety check passed: {result.pass_}\")  # Output: Safety check passed: True\n</code></pre>"},{"location":"evaluations/patronus-evaluators/","title":"Patronus Evaluators","text":"<p>Patronus provides a suite of evaluators that help you assess LLM outputs without writing complex evaluation logic. These managed evaluators run on Patronus infrastructure. Visit Patronus Platform console to define your own criteria.</p>"},{"location":"evaluations/patronus-evaluators/#using-patronus-evaluators","title":"Using Patronus Evaluators","text":"<p>You can use Patronus evaluators through the <code>RemoteEvaluator</code> class:</p> <pre><code>from patronus import init\nfrom patronus.evals import RemoteEvaluator\n\ninit()\n\nfactual_accuracy = RemoteEvaluator(\"judge\", \"factual-accuracy\")\n\n# Evaluate an LLM output\nresult = factual_accuracy.evaluate(\n    task_input=\"What is the capital of France?\",\n    task_output=\"The capital of France is Paris, which is located on the Seine River.\",\n    gold_answer=\"Paris\"\n)\n\nprint(f\"Passed: {result.pass_}\")\nprint(f\"Score: {result.score}\")\nprint(f\"Explanation: {result.explanation}\")\n</code></pre>"},{"location":"evaluations/patronus-evaluators/#retry-configuration","title":"Retry Configuration","text":"<p>RemoteEvaluators support automatic retry with exponential backoff for transient failures. You can configure the retry behavior using the following parameters:</p> <pre><code>from patronus.evals import RemoteEvaluator\n\n# Configure retry behavior\nevaluator = RemoteEvaluator(\n    \"judge\",\n    \"factual-accuracy\",\n    retry_max_attempts=5,       # Maximum number of retry attempts (default: 3)\n    retry_initial_delay=2,      # Initial delay in seconds before first retry (default: 1)\n    retry_backoff_factor=3      # Multiplier for delay between retries (default: 2)\n)\n\nresult = evaluator.evaluate(\n    task_input=\"What is the capital of France?\",\n    task_output=\"Paris\",\n    gold_answer=\"Paris\"\n)\n</code></pre> <p>Retry Parameters:</p> <ul> <li><code>retry_max_attempts</code> (int): Maximum number of attempts to retry the evaluation request. Default is 3.</li> <li><code>retry_initial_delay</code> (int): Initial delay in seconds before the first retry attempt. Default is 1 second.</li> <li><code>retry_backoff_factor</code> (int): Multiplier applied to the delay between each retry attempt, creating exponential backoff. Default is 2.</li> </ul> <p>Example Retry Behavior:</p> <p>With default settings (<code>retry_max_attempts=3</code>, <code>retry_initial_delay=1</code>, <code>retry_backoff_factor=2</code>): - 1st attempt: immediate - 2nd attempt: after 1 second - 3rd attempt: after 2 seconds (1 \u00d7 2) - 4th attempt: after 4 seconds (2 \u00d7 2)</p> <p>If all retry attempts fail, the original exception will be raised.</p>"},{"location":"evaluations/patronus-evaluators/#synchronous-and-asynchronous-versions","title":"Synchronous and Asynchronous Versions","text":"<p>Patronus evaluators are available in both synchronous and asynchronous versions:</p> <pre><code># Synchronous usage (as shown above)\nfactual_accuracy = RemoteEvaluator(\"judge\", \"factual-accuracy\")\nresult = factual_accuracy.evaluate(...)\n\n# Asynchronous usage\nfrom patronus.evals import AsyncRemoteEvaluator\n\nasync_factual_accuracy = AsyncRemoteEvaluator(\"judge\", \"factual-accuracy\")\nresult = await async_factual_accuracy.evaluate(...)\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Examples of how to use Patronus and what it can do.</p>"},{"location":"examples/#usage","title":"Usage","text":"<p>These examples demonstrate common use cases and integration patterns for Patronus.</p>"},{"location":"examples/#setting-required-environment-variables","title":"Setting required environment variables","text":"<p>Most examples require you to set up authentication with Patronus and other services. In most cases, you'll need to set the following environment variables:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport OPENAI_API_KEY=your-api-key\n</code></pre> <p>Some examples may require additional API keys (like <code>ANTHROPIC_API_KEY</code>).</p>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>There are three ways to run the examples:</p>"},{"location":"examples/#1-running-with-uv","title":"1. Running with <code>uv</code>","text":"<p>You can run examples with <code>uv</code>, which automatically installs the required dependencies:</p> <pre><code># Remember to export environment variables before running the example.\nuv run --no-cache --with \"patronus-examples[smolagents]\" \\\n    -m patronus_examples.tracking.smolagents_weather\n</code></pre> <p>This installs the <code>patronus-examples</code> package with the necessary optional dependencies.</p>"},{"location":"examples/#2-pulling-the-repository-and-executing-the-scripts-directly","title":"2. Pulling the repository and executing the scripts directly","text":"<p>You can clone the repository and run the scripts directly:</p> <pre><code># Clone the repository\ngit clone https://github.com/patronus-ai/patronus-py.git\ncd patronus-py\n\n# Run the example script (requires uv)\n./examples/patronus_examples/tracking/smolagents_weather.py\n</code></pre> <p>See the script files for more information. They use uv script annotations to handle dependencies.</p>"},{"location":"examples/#3-copy-and-paste-example","title":"3. Copy and paste example","text":"<p>You can copy the example code into your own project and install the dependencies with any package manager of your choice. Each example file includes a list of required dependencies at the top of the document.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":"<p>Patronus provides examples for various LLM frameworks and direct API integrations:</p>"},{"location":"examples/#direct-llm-api-integrations","title":"Direct LLM API Integrations","text":"<ul> <li>OpenAI Weather Example - Simple example of tracing OpenAI API calls</li> <li>Anthropic Weather Example - Simple example of tracing Anthropic API calls</li> </ul>"},{"location":"examples/#agent-frameworks","title":"Agent Frameworks","text":"<ul> <li>Smolagents Weather - Using Patronus with Smolagents</li> <li>PydanticAI Weather - Using Patronus with PydanticAI</li> <li>OpenAI Agents Weather - Using Patronus with OpenAI Agents</li> <li>LangChain Weather - Using Patronus with LangChain and LangGraph</li> <li>CrewAI Weather - Using Patronus with CrewAI</li> </ul> <p>Each example demonstrates: - How to set up Patronus integrations with the specific framework - How to trace LLM calls and tool usage - How to analyze the execution flow of your application</p> <p>All examples follow a similar pattern using a weather application to make it easy to compare the different frameworks.</p>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":"<ul> <li>Manual OpenTelemetry with OpenAI - An example showing how to use OpenTelemetry directly without Patronus SDK</li> </ul>"},{"location":"examples/anthropic-weather/","title":"Trace Anthropic","text":""},{"location":"examples/anthropic-weather/#running-the-example","title":"Running the example","text":"<p>To run this example, you need to add API keys to your environment:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport ANTHROPIC_API_KEY=your-api-key\n</code></pre>"},{"location":"examples/anthropic-weather/#running-with-uv","title":"Running with <code>uv</code>","text":"<p>You can run the example as a one-liner with zero setup:</p> <pre><code># Remember to export environment variables before running the example.\nuv run --no-cache --with \"patronus-examples[anthropic]\" \\\n    -m patronus_examples.tracking.anthropic_weather\n</code></pre>"},{"location":"examples/anthropic-weather/#running-the-script-directly","title":"Running the script directly","text":"<p>If you've cloned the repository, you can run the script directly:</p> <pre><code># Clone the repository\ngit clone https://github.com/patronus-ai/patronus-py.git\ncd patronus-py\n\n# Run the example script (requires uv)\n./examples/patronus_examples/tracking/anthropic_weather.py\n</code></pre>"},{"location":"examples/anthropic-weather/#manual-installation","title":"Manual installation","text":"<p>If you prefer to copy the example code to your own project, you'll need to install these dependencies:</p> <pre><code>pip install patronus\npip install anthropic\npip install openinference-instrumentation-anthropic\n</code></pre>"},{"location":"examples/anthropic-weather/#example-overview","title":"Example overview","text":"<p>This example demonstrates how to use Patronus to trace Anthropic API calls when implementing a simple weather application. The application:</p> <ol> <li>Uses the Anthropic Claude API to parse a user question about weather</li> <li>Extracts location coordinates from the LLM's output through Claude's tool calling</li> <li>Calls a weather API to get actual temperature data</li> <li>Returns the result to the user</li> </ol> <p>The example shows how Patronus can help you monitor and debug Anthropic API interactions, track tool usage, and visualize the entire application flow.</p>"},{"location":"examples/anthropic-weather/#example-code","title":"Example code","text":"<pre><code># examples/patronus_examples/tracking/anthropic_weather.py\n\nimport requests\n\nimport anthropic\nfrom openinference.instrumentation.anthropic import AnthropicInstrumentor\nimport patronus\n\n# Initialize patronus with Anthropic Instrumentor\npatronus.init(integrations=[AnthropicInstrumentor()])\n\n\ndef get_weather(latitude, longitude):\n    response = requests.get(\n        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&amp;longitude={longitude}&amp;current=temperature_2m,wind_speed_10m&amp;hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n    )\n    data = response.json()\n    return data[\"current\"][\"temperature_2m\"]\n\n\ndef get_client():\n    client = anthropic.Anthropic()\n    return client\n\n\n@patronus.traced()\ndef call_llm(client, user_prompt):\n    tools = [\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get current temperature for provided coordinates in celsius.\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"latitude\": {\"type\": \"number\"},\n                    \"longitude\": {\"type\": \"number\"},\n                },\n                \"required\": [\"latitude\", \"longitude\"],\n            },\n        }\n    ]\n\n    response = client.messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=1024,\n        tools=tools,\n        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n    )\n    return response\n\n\n@patronus.traced(\"anthropic-weather\")\ndef main():\n    user_prompt = \"What's the weather like in Paris today?\"\n\n    client = get_client()\n    response = call_llm(client, user_prompt)\n    print(\"LLM Response\")\n    print(response.model_dump_json())\n\n    weather_response = None\n    if response.content:\n        for content_block in response.content:\n            if content_block.type == \"tool_use\" and content_block.name == \"get_weather\":\n                kwargs = content_block.input\n                print(\"Weather API Response\")\n                weather_response = get_weather(**kwargs)\n                print(weather_response)\n\n    if weather_response:\n        print(user_prompt)\n        print(f\"Answer: {weather_response}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/crewai-weather/","title":"Trace CrewAI","text":""},{"location":"examples/crewai-weather/#running-the-example","title":"Running the example","text":"<p>To run this example, you need to add API keys to your environment:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"examples/crewai-weather/#running-with-uv","title":"Running with <code>uv</code>","text":"<p>You can run the example as a one-liner with zero setup:</p> <pre><code># Remember to export environment variables before running the example.\nuv run --no-cache --with \"patronus-examples[crewai]\" \\\n    -m patronus_examples.tracking.crewai_weather\n</code></pre>"},{"location":"examples/crewai-weather/#running-the-script-directly","title":"Running the script directly","text":"<p>If you've cloned the repository, you can run the script directly:</p> <pre><code># Clone the repository\ngit clone https://github.com/patronus-ai/patronus-py.git\ncd patronus-py\n\n# Run the example script (requires uv)\n./examples/patronus_examples/tracking/crewai_weather.py\n</code></pre>"},{"location":"examples/crewai-weather/#manual-installation","title":"Manual installation","text":"<p>If you prefer to copy the example code to your own project, you'll need to install these dependencies:</p> <pre><code>pip install patronus\npip install crewai\npip install openinference.instrumentation.crewai\npip install opentelemetry-instrumentation-threading\npip install opentelemetry-instrumentation-asyncio\n</code></pre>"},{"location":"examples/crewai-weather/#example-overview","title":"Example overview","text":"<p>This example demonstrates how to use Patronus to trace and monitor CrewAI agents in a weather application. The example:</p> <ol> <li>Sets up a specialized Weather Information Specialist agent with a custom weather tool</li> <li>Creates a manager agent that coordinates information requests</li> <li>Defines tasks for each agent to perform</li> <li>Configures a hierarchical workflow using the CrewAI Crew construct</li> <li>Traces the entire execution flow with Patronus</li> </ol> <p>The example shows how Patronus integrates with CrewAI to provide visibility into agent interactions, tool usage, and the hierarchical task execution process.</p>"},{"location":"examples/crewai-weather/#example-code","title":"Example code","text":"<pre><code># examples/patronus_examples/tracking/crewai_weather.py\n\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai.tools import BaseTool\nfrom opentelemetry.instrumentation.threading import ThreadingInstrumentor\nfrom opentelemetry.instrumentation.asyncio import AsyncioInstrumentor\nfrom openinference.instrumentation.crewai import CrewAIInstrumentor\nimport patronus\n\npatronus.init(\n    integrations=[CrewAIInstrumentor(), ThreadingInstrumentor(), AsyncioInstrumentor()]\n)\n\n\n# Create a custom tool for weather information\nclass WeatherTool(BaseTool):\n    name: str = \"get_weather_api\"\n    description: str = \"Returns the weather report for a specific location\"\n\n    def _run(self, location: str) -&gt; str:\n        \"\"\"\n        Returns the weather report.\n\n        Args:\n            location: the name of the place that you want the weather for. Should be a place name, followed by possibly a city name, then a country, like \"Anchor Point, Taghazout, Morocco\".\n\n        Returns:\n            The weather report.\n        \"\"\"\n        temperature_celsius, risk_of_rain, wave_height = 10, 0.5, 4  # mock outputs\n        return f\"Weather report for {location}: Temperature will be {temperature_celsius}\u00b0C, risk of rain is {risk_of_rain * 100:.0f}%, wave height is {wave_height}m.\"\n\n\n# Initialize weather tool\nweather_tool = WeatherTool()\n\n# Define agents\nweather_agent = Agent(\n    role=\"Weather Information Specialist\",\n    goal=\"Provide accurate weather information for specific locations and times\",\n    backstory=\"\"\"You are a weather information specialist that must call the available tool to get the most recent reports\"\"\",\n    verbose=False,\n    allow_delegation=False,\n    tools=[weather_tool],\n    max_iter=5,\n)\n\nmanager_agent = Agent(\n    role=\"Information Manager\",\n    goal=\"Coordinate information requests and delegate to specialized agents\",\n    backstory=\"\"\"You manage and coordinate information requests, delegating specialized\n    queries to the appropriate experts. You ensure users get the most accurate and relevant\n    information.\"\"\",\n    verbose=False,\n    allow_delegation=True,\n    max_iter=10,\n)\n\n# Create tasks\nweather_task = Task(\n    description=\"\"\"Find out the current weather at a specific location.\"\"\",\n    expected_output=\"Complete weather report with temperature, rain and wave height information\",\n    agent=weather_agent,\n)\n\nmanager_task = Task(\n    description=\"\"\"Process the user query about weather in Paris, France.\n    Ensure the weather information is complete (with temperature, rain and wave height) and properly formatted.\n    You must coordinate with the weather agent for this task.\"\"\",\n    expected_output=\"Weather report for Paris\",\n    agent=manager_agent,\n)\n\n# Instantiate crew with a sequential process\ncrew = Crew(\n    agents=[weather_agent],\n    tasks=[manager_task, weather_task],\n    verbose=False,\n    manager_agent=manager_agent,\n    process=Process.hierarchical,\n)\n\n\n@patronus.traced(\"weather-crew-ai\")\ndef main():\n    result = crew.kickoff()\n    print(result)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/langchain-weather/","title":"Trace LangChain","text":""},{"location":"examples/langchain-weather/#running-the-example","title":"Running the example","text":"<p>To run this example, you need to add API keys to your environment:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"examples/langchain-weather/#running-with-uv","title":"Running with <code>uv</code>","text":"<p>You can run the example as a one-liner with zero setup:</p> <pre><code># Remember to export environment variables before running the example.\nuv run --no-cache --with \"patronus-examples[langchain]\" \\\n    -m patronus_examples.tracking.langchain_weather\n</code></pre>"},{"location":"examples/langchain-weather/#running-the-script-directly","title":"Running the script directly","text":"<p>If you've cloned the repository, you can run the script directly:</p> <pre><code># Clone the repository\ngit clone https://github.com/patronus-ai/patronus-py.git\ncd patronus-py\n\n# Run the example script (requires uv)\n./examples/patronus_examples/tracking/langchain_weather.py\n</code></pre>"},{"location":"examples/langchain-weather/#manual-installation","title":"Manual installation","text":"<p>If you prefer to copy the example code to your own project, you'll need to install these dependencies:</p> <pre><code>pip install patronus\npip install pydantic\npip install langchain_openai\npip install langgraph\npip install langchain_core\npip install openinference-instrumentation-langchain\npip install opentelemetry-instrumentation-threading\npip install opentelemetry-instrumentation-asyncio\n</code></pre>"},{"location":"examples/langchain-weather/#example-overview","title":"Example overview","text":"<p>This example demonstrates how to use Patronus to trace a LangChain and LangGraph workflow for a weather application. The example:</p> <ol> <li>Sets up a StateGraph with manager and weather agent nodes</li> <li>Implements a router to control workflow transitions</li> <li>Uses a tool to provide mock weather data</li> <li>Traces the entire LangChain and LangGraph execution with Patronus</li> </ol> <p>The example shows how Patronus can provide visibility into complex, multi-node LangGraph workflows, including tool usage and agent transitions.</p>"},{"location":"examples/langchain-weather/#example-code","title":"Example code","text":"<pre><code># examples/patronus_examples/tracking/langchain_weather.py\n\nfrom typing import Literal, Dict, List, Any\nfrom langchain_core.messages import (\n    HumanMessage,\n    AIMessage,\n    BaseMessage,\n)\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, StateGraph\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\n\nfrom openinference.instrumentation.langchain import LangChainInstrumentor\nfrom opentelemetry.instrumentation.threading import ThreadingInstrumentor\nfrom opentelemetry.instrumentation.asyncio import AsyncioInstrumentor\n\nimport patronus\n\npatronus.init(\n    integrations=[\n        LangChainInstrumentor(),\n        ThreadingInstrumentor(),\n        AsyncioInstrumentor(),\n    ]\n)\n\n\n@tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the current weather in a given city.\n\n    Args:\n        city: The name of the city to get weather for.\n\n    Returns:\n        A string describing the current weather in the city.\n    \"\"\"\n    return f\"The weather in {city} is sunny\"\n\n\nclass MessagesState(BaseModel):\n    \"\"\"State for the manager-weather agent workflow.\"\"\"\n\n    messages: List[BaseMessage] = Field(default_factory=list)\n    current_agent: str = Field(default=\"manager\")\n\n\nmanager_model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\nweather_model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n\ntools = [get_weather]\ntools_dict = {tool.name: tool for tool in tools}\n\nweather_model_with_tools = weather_model.bind_tools(tools)\n\n\ndef manager_agent(state: MessagesState) -&gt; Dict[str, Any]:\n    messages = state.messages  # Access as attribute\n    # Get response from the manager model\n    response = manager_model.invoke(messages)\n\n    # Check if the manager wants to use the weather agent\n    manager_text = response.content.lower()\n    if \"weather\" in manager_text and \"in\" in manager_text:\n        # Delegate to weather agent\n        return {\n            \"messages\": messages\n                        + [\n                            AIMessage(\n                                content=\"I'll check the weather for you. Delegating to weather agent.\"\n                            )\n                        ],\n            \"current_agent\": \"weather\",\n        }\n\n    return {\"messages\": messages + [response], \"current_agent\": \"manager\"}\n\n\n# Define the weather agent node using a simpler approach\ndef weather_agent(state: MessagesState) -&gt; Dict[str, Any]:\n    messages = state.messages  # Access as attribute\n    human_queries = [msg for msg in messages if isinstance(msg, HumanMessage)]\n    if not human_queries:\n        return {\n            \"messages\": messages + [AIMessage(content=\"I need a query about weather.\")],\n            \"current_agent\": \"manager\",\n        }\n\n    query = human_queries[-1].content\n\n    try:\n        # weather_prompt = (\n        #     f\"Extract the city name from this query and provide the weather: '{query}'\"\n        # )\n\n        city_match = None\n\n        # Common cities that might be mentioned\n        common_cities = [\n            \"Paris\",\n            \"London\",\n            \"New York\",\n            \"Tokyo\",\n            \"Berlin\",\n            \"Rome\",\n            \"Madrid\",\n        ]\n        for city in common_cities:\n            if city.lower() in query.lower():\n                city_match = city\n                break\n\n        if city_match:\n            weather_result = get_weather.invoke(city_match)\n            weather_response = (\n                f\"I checked the weather for {city_match}. {weather_result}\"\n            )\n        else:\n            if \"weather in \" in query.lower():\n                parts = query.lower().split(\"weather in \")\n                if len(parts) &gt; 1:\n                    city_match = parts[1].strip().split()[0].capitalize()\n                    weather_result = get_weather.invoke(city_match)\n                    weather_response = (\n                        f\"I checked the weather for {city_match}. {weather_result}\"\n                    )\n                else:\n                    weather_response = (\n                        \"I couldn't identify a specific city in your query.\"\n                    )\n            else:\n                weather_response = \"I couldn't identify a specific city in your query.\"\n\n        return {\n            \"messages\": messages\n                        + [AIMessage(content=f\"Weather Agent: {weather_response}\")],\n            \"current_agent\": \"manager\",\n        }\n    except Exception as e:\n        error_message = f\"I encountered an error while checking the weather: {str(e)}\"\n        return {\n            \"messages\": messages\n                        + [AIMessage(content=f\"Weather Agent: {error_message}\")],\n            \"current_agent\": \"manager\",\n        }\n\n\ndef router(state: MessagesState) -&gt; Literal[\"manager\", \"weather\", END]:\n    if len(state.messages) &gt; 10:  # Prevent infinite loops\n        return END\n\n    # Route based on current_agent\n    if state.current_agent == \"weather\":\n        return \"weather\"\n    elif state.current_agent == \"manager\":\n        # Check if the last message is from the manager and indicates completion\n        if len(state.messages) &gt; 0 and isinstance(state.messages[-1], AIMessage):\n            if \"delegating to weather agent\" not in state.messages[-1].content.lower():\n                return END\n\n    return \"manager\"\n\n\nworkflow = StateGraph(MessagesState)\nworkflow.add_node(\"manager\", manager_agent)\nworkflow.add_node(\"weather\", weather_agent)\n\nworkflow.set_entry_point(\"manager\")\nworkflow.add_conditional_edges(\"manager\", router)\nworkflow.add_conditional_edges(\"weather\", router)\n\ncheckpointer = MemorySaver()\napp = workflow.compile(checkpointer=checkpointer)\n\n\ndef run_workflow(query: str):\n    initial_state = MessagesState(\n        messages=[HumanMessage(content=query)], current_agent=\"manager\"\n    )\n\n    config = {\"configurable\": {\"thread_id\": \"weather_demo_thread\"}}\n    final_state = app.invoke(initial_state, config=config)\n\n    for message in final_state[\"messages\"]:\n        if isinstance(message, HumanMessage):\n            print(f\"Human: {message.content}\")\n        elif isinstance(message, AIMessage):\n            print(f\"AI: {message.content}\")\n        else:\n            print(f\"Other: {message.content}\")\n\n    return final_state\n\n\n@patronus.traced(\"weather-langchain\")\ndef main():\n    final_state = run_workflow(\"What is the weather in Paris?\")\n    return final_state\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/openai-agents-weather/","title":"Trace OpenAI Agents","text":""},{"location":"examples/openai-agents-weather/#running-the-example","title":"Running the example","text":"<p>To run this example, you need to add API keys to your environment:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"examples/openai-agents-weather/#running-with-uv","title":"Running with <code>uv</code>","text":"<p>You can run the example as a one-liner with zero setup:</p> <pre><code># Remember to export environment variables before running the example.\nuv run --no-cache --with \"patronus-examples[openai-agents]\" \\\n    -m patronus_examples.tracking.openai_agents_weather\n</code></pre>"},{"location":"examples/openai-agents-weather/#running-the-script-directly","title":"Running the script directly","text":"<p>If you've cloned the repository, you can run the script directly:</p> <pre><code># Clone the repository\ngit clone https://github.com/patronus-ai/patronus-py.git\ncd patronus-py\n\n# Run the example script (requires uv)\n./examples/patronus_examples/tracking/openai_agents_weather.py\n</code></pre>"},{"location":"examples/openai-agents-weather/#manual-installation","title":"Manual installation","text":"<p>If you prefer to copy the example code to your own project, you'll need to install these dependencies:</p> <pre><code>pip install patronus\npip install openai-agents\npip install openinference-instrumentation-openai-agents\npip install opentelemetry-instrumentation-threading\npip install opentelemetry-instrumentation-asyncio\n</code></pre>"},{"location":"examples/openai-agents-weather/#example-overview","title":"Example overview","text":"<p>This example demonstrates how to use Patronus to trace and monitor OpenAI Agents in an asynchronous weather application. The example:</p> <ol> <li>Sets up a weather agent with a function tool to retrieve weather information</li> <li>Creates a manager agent that can delegate to the weather agent</li> <li>Handles the workflow using the OpenAI Agents Runner</li> <li>Traces the entire agent execution flow with Patronus</li> </ol> <p>The example shows how Patronus integrates with OpenAI Agents to provide visibility into agent hierarchies, tool usage, and asynchronous workflows.</p>"},{"location":"examples/openai-agents-weather/#example-code","title":"Example code","text":"<pre><code># examples/patronus_examples/tracking/openai_agents_weather.py\n\nfrom agents import Agent, Runner, function_tool\nfrom openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\nfrom opentelemetry.instrumentation.threading import ThreadingInstrumentor\nfrom opentelemetry.instrumentation.asyncio import AsyncioInstrumentor\nimport asyncio\nimport patronus\n\npatronus.init(\n    integrations=[\n        OpenAIAgentsInstrumentor(),\n        ThreadingInstrumentor(),\n        AsyncioInstrumentor(),\n    ]\n)\n\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is sunny\"\n\n\ndef get_agents(tools=[]):\n    weather_agent = Agent(\n        name=\"weather_agent\",\n        instructions=\"You are a helpful assistant that can call tools and return weather related information\",\n        model=\"o3-mini\",\n        tools=tools,\n    )\n\n    manager_agent = Agent(\n        name=\"manager_agent\",\n        instructions=\"You are a helpful assistant that can call other agents to accomplish different tasks\",\n        model=\"o3-mini\",\n        handoffs=[weather_agent],\n    )\n    return manager_agent\n\n\n@patronus.traced(\"weather-openai-agent\")\nasync def main():\n    manager_agent = get_agents([get_weather])\n    result = await Runner.run(manager_agent, \"How is the weather in Paris, France?\")\n    return result.final_output\n\n\nif __name__ == \"__main__\":\n    print(\"Starting agent...\")\n    result = asyncio.run(main())\n    print(result)\n</code></pre>"},{"location":"examples/openai-weather/","title":"Trace OpenAI","text":""},{"location":"examples/openai-weather/#running-the-example","title":"Running the example","text":"<p>To run this example, you need to add API keys to your environment:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"examples/openai-weather/#running-with-uv","title":"Running with <code>uv</code>","text":"<p>You can run the example as a one-liner with zero setup:</p> <pre><code># Remember to export environment variables before running the example.\nuv run --no-cache --with \"patronus-examples[openai]\" \\\n    -m patronus_examples.tracking.openai_weather\n</code></pre>"},{"location":"examples/openai-weather/#running-the-script-directly","title":"Running the script directly","text":"<p>If you've cloned the repository, you can run the script directly:</p> <pre><code># Clone the repository\ngit clone https://github.com/patronus-ai/patronus-py.git\ncd patronus-py\n\n# Run the example script (requires uv)\n./examples/patronus_examples/tracking/openai_weather.py\n</code></pre>"},{"location":"examples/openai-weather/#manual-installation","title":"Manual installation","text":"<p>If you prefer to copy the example code to your own project, you'll need to install these dependencies:</p> <pre><code>pip install patronus\npip install openai\npip install openinference-instrumentation-openai\n</code></pre>"},{"location":"examples/openai-weather/#example-overview","title":"Example overview","text":"<p>This example demonstrates how to use Patronus to trace OpenAI API calls when implementing a simple weather application. The application:</p> <ol> <li>Uses the OpenAI API to parse a user question about weather</li> <li>Extracts location coordinates from the LLM's output</li> <li>Calls a weather API to get actual temperature data</li> <li>Returns the result to the user</li> </ol> <p>The example shows how Patronus can help you monitor and debug OpenAI API interactions, track tool usage, and visualize the entire application flow.</p>"},{"location":"examples/openai-weather/#example-code","title":"Example code","text":"<pre><code># examples/patronus_examples/tracking/openai_weather.py\n\nimport json\n\nimport requests\nfrom openai import OpenAI\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nimport patronus\n\n# Initialize patronus with OpenAI Instrumentor\npatronus.init(integrations=[OpenAIInstrumentor()])\n\n\ndef get_weather(latitude, longitude):\n    response = requests.get(\n        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&amp;longitude={longitude}&amp;current=temperature_2m,wind_speed_10m&amp;hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n    )\n    data = response.json()\n    return data[\"current\"][\"temperature_2m\"]\n\n\ndef get_client():\n    client = OpenAI()\n    return client\n\n\n@patronus.traced()\ndef call_llm(client, user_prompt):\n    tools = [\n        {\n            \"type\": \"function\",\n            \"name\": \"get_weather\",\n            \"description\": \"Get current temperature for provided coordinates in celsius.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"latitude\": {\"type\": \"number\"},\n                    \"longitude\": {\"type\": \"number\"},\n                },\n                \"required\": [\"latitude\", \"longitude\"],\n                \"additionalProperties\": False,\n            },\n            \"strict\": True,\n        }\n    ]\n\n    input_messages = [{\"role\": \"user\", \"content\": user_prompt}]\n\n    response = client.responses.create(\n        model=\"gpt-4.1\",\n        input=input_messages,\n        tools=tools,\n    )\n    return response\n\n\n@patronus.traced(\"openai-weather\")\ndef main():\n    user_prompt = \"What's the weather like in Paris today?\"\n\n    client = get_client()\n    response = call_llm(client, user_prompt)\n    print(\"LLM Response\")\n    print(response.model_dump_json())\n\n    weather_response = None\n    if response.output:\n        output = response.output[0]\n        if output.type == \"function_call\" and output.name == \"get_weather\":\n            kwargs = json.loads(output.arguments)\n            print(\"Weather API Response\")\n            weather_response = get_weather(**kwargs)\n            print(weather_response)\n\n    if weather_response:\n        print(user_prompt)\n        print(f\"Answer: {weather_response}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/otel-openai-weather/","title":"Manual OTel with OpenAI","text":""},{"location":"examples/otel-openai-weather/#manual-opentelemetry-tracing-example","title":"Manual OpenTelemetry Tracing Example","text":"<p>This example demonstrates how to use OpenTelemetry (OTel) directly with OpenInference instrumenters to trace a simple OpenAI weather application without using Patronus SDK. This shows how to implement manual instrumentation combined with automatic instrumenters.</p>"},{"location":"examples/otel-openai-weather/#running-the-example","title":"Running the example","text":"<p>To run this example, you need to add your OpenAI API key to your environment:</p> <pre><code>export OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"examples/otel-openai-weather/#running-with-uv","title":"Running with <code>uv</code>","text":"<p>You can run the example as a one-liner with zero setup:</p> <pre><code># Remember to export environment variables before running the example\nuv run --no-cache --with \"patronus-examples opentelemetry-api&gt;=1.31.0 opentelemetry-sdk&gt;=1.31.0 opentelemetry-exporter-otlp&gt;=1.31.0 openinference-instrumentation-openai&gt;=0.1.28 openai httpx&gt;=0.27.0\" \\\n    -m patronus_examples.tracking.otel_openai_weather\n</code></pre>"},{"location":"examples/otel-openai-weather/#running-with-patronus-otel-collector","title":"Running with Patronus OTel collector","text":"<p>To export traces to Patronus OTel collector, set these additional environment variables:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport OTEL_EXPORTER_OTLP_ENDPOINT=\"https://otel.patronus.ai:4317\"\nexport OTEL_EXPORTER_OTLP_HEADERS=\"x-api-key=$PATRONUS_API_KEY\"\n</code></pre>"},{"location":"examples/otel-openai-weather/#manual-installation","title":"Manual installation","text":"<p>If you prefer to copy the example code to your own project, you'll need to install these dependencies:</p> <pre><code>pip install openai\npip install opentelemetry-api\npip install opentelemetry-sdk\npip install opentelemetry-exporter-otlp\npip install openinference-instrumentation-openai\npip install httpx\n</code></pre>"},{"location":"examples/otel-openai-weather/#example-overview","title":"Example overview","text":"<p>This example demonstrates how to combine manual OpenTelemetry instrumentation with OpenInference auto-instrumentation for an OpenAI-based weather application. The application:</p> <ol> <li>Sets up a complete OpenTelemetry tracing pipeline</li> <li>Initializes OpenInference instrumenter for OpenAI</li> <li>Calls the OpenAI API which is automatically traced by OpenInference</li> <li>Adds additional manual spans for non-OpenAI components</li> <li>Makes an instrumented HTTP request using httpx to a weather API</li> <li>Records all relevant attributes and events in spans</li> </ol> <p>The example shows how to:</p> <ul> <li>Configure an OpenTelemetry TracerProvider</li> <li>Set up either console or OTLP exporters</li> <li>Initialize OpenInference instrumenters with OpenTelemetry</li> <li>Create nested manual spans for tracking operations</li> <li>Use httpx for HTTP requests with proper tracing</li> <li>Add attributes to spans for better observability</li> <li>Handle errors and exceptions in spans</li> </ul>"},{"location":"examples/otel-openai-weather/#example-code","title":"Example code","text":"<pre><code># examples/patronus_examples/tracking/otel_openai_weather.py\n\nimport json\nimport os\nimport httpx\nfrom openai import OpenAI\n\n# OpenTelemetry imports\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\n# Import OpenInference instrumenter for OpenAI\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\n# Configure OpenTelemetry\nresource = Resource(attributes={\n    ResourceAttributes.SERVICE_NAME: \"openai-weather-app\",\n    ResourceAttributes.SERVICE_VERSION: \"0.1.0\",\n})\n\n# Initialize the trace provider with the resource\ntrace_provider = TracerProvider(resource=resource)\n\n# If OTEL_EXPORTER_OTLP_ENDPOINT is not set, we'll use console exporter\n# Otherwise, we'll use OTLP exporter for sending to the Patronus collector\nif os.environ.get(\"OTEL_EXPORTER_OTLP_ENDPOINT\"):\n    # Configure OTLPSpanExporter\n    # The environment variables OTEL_EXPORTER_OTLP_ENDPOINT and OTEL_EXPORTER_OTLP_HEADERS\n    # should be set before running this example\n    otlp_exporter = OTLPSpanExporter()\n    trace_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\nelse:\n    # For local development/testing we can use ConsoleSpanExporter\n    from opentelemetry.sdk.trace.export import ConsoleSpanExporter\n    trace_provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\n\n# Set the provider\ntrace.set_tracer_provider(trace_provider)\n\n# Initialize OpenInference instrumenter for OpenAI\n# This will automatically instrument all OpenAI API calls\nopenai_instrumentor = OpenAIInstrumentor()\nopenai_instrumentor.instrument()\n\n# Get a tracer for our manual spans\ntracer = trace.get_tracer(\"openai.weather.example\")\n\n\ndef get_weather(latitude, longitude):\n    \"\"\"Get weather data from the Open Meteo API using httpx\"\"\"\n    with tracer.start_as_current_span(\n        \"get_weather\",\n        attributes={\n            \"service.name\": \"weather_api\",\n            \"weather.latitude\": latitude,\n            \"weather.longitude\": longitude\n        }\n    ) as span:\n        try:\n            # Create the URL with parameters\n            url = \"https://api.open-meteo.com/v1/forecast\"\n            params = {\n                \"latitude\": latitude,\n                \"longitude\": longitude,\n                \"current\": \"temperature_2m,wind_speed_10m\",\n                \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m\"\n            }\n\n            # Trace the HTTP request using httpx\n            with tracer.start_as_current_span(\n                \"http_request\",\n                attributes={\n                    \"http.method\": \"GET\",\n                    \"http.url\": url,\n                    \"http.request.query\": str(params)\n                }\n            ):\n                # Use httpx client for the request\n                with httpx.Client() as client:\n                    response = client.get(url, params=params)\n\n                # Add response information to the span\n                span.set_attribute(\"http.status_code\", response.status_code)\n\n                if response.status_code != 200:\n                    span.record_exception(Exception(f\"Weather API returned status {response.status_code}\"))\n                    span.set_status(trace.StatusCode.ERROR)\n                    return None\n\n                data = response.json()\n                temperature = data[\"current\"][\"temperature_2m\"]\n\n                # Add weather data to the span\n                span.set_attribute(\"weather.temperature_celsius\", temperature)\n\n                return temperature\n        except Exception as e:\n            # Record the exception in the span\n            span.record_exception(e)\n            span.set_status(trace.StatusCode.ERROR, str(e))\n            raise\n\n\ndef get_client():\n    \"\"\"Create and return an OpenAI client\"\"\"\n    with tracer.start_as_current_span(\"get_openai_client\"):\n        return OpenAI()\n\n\ndef call_llm(client, user_prompt):\n    \"\"\"Call the OpenAI API to process the user prompt\n\n    Note: With OpenInference instrumenter, the OpenAI API call will be\n    automatically traced. This function adds some additional manual spans\n    for demonstration purposes.\n    \"\"\"\n    with tracer.start_as_current_span(\n        \"call_llm\",\n        attributes={\n            \"ai.prompt.text\": user_prompt,\n            \"ai.prompt.tokens\": len(user_prompt.split())\n        }\n    ) as span:\n        try:\n            # Define tools available to the model\n            tools = [\n                {\n                    \"type\": \"function\",\n                    \"name\": \"get_weather\",\n                    \"description\": \"Get current temperature for provided coordinates in celsius.\",\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"latitude\": {\"type\": \"number\"},\n                            \"longitude\": {\"type\": \"number\"},\n                        },\n                        \"required\": [\"latitude\", \"longitude\"],\n                        \"additionalProperties\": False,\n                    },\n                    \"strict\": True,\n                }\n            ]\n\n            input_messages = [{\"role\": \"user\", \"content\": user_prompt}]\n\n            # The OpenAI API call will be automatically traced by OpenInference\n            # We don't need to create a span for it, but we can add attributes to our parent span\n            response = client.responses.create(\n                model=\"gpt-4.1\",\n                input=input_messages,\n                tools=tools,\n            )\n\n            # Check if the response contains a tool call\n            has_tool_call = False\n            if response.output and len(response.output) &gt; 0:\n                output = response.output[0]\n                if output.type == \"function_call\":\n                    has_tool_call = True\n                    span.set_attribute(\"openai.response.tool_called\", output.name)\n\n            span.set_attribute(\"openai.response.has_tool_call\", has_tool_call)\n\n            return response\n        except Exception as e:\n            span.record_exception(e)\n            span.set_status(trace.StatusCode.ERROR, str(e))\n            raise\n\n\ndef main():\n    \"\"\"Main function to process the weather query\"\"\"\n    with tracer.start_as_current_span(\"openai-weather-main\") as root_span:\n        user_prompt = \"What's the weather like in Paris today?\"\n        root_span.set_attribute(\"query\", user_prompt)\n\n        try:\n            client = get_client()\n            response = call_llm(client, user_prompt)\n            print(\"LLM Response\")\n            print(response.model_dump_json())\n\n            weather_response = None\n            if response.output:\n                output = response.output[0]\n                if output.type == \"function_call\" and output.name == \"get_weather\":\n                    # Parse the arguments from the function call\n                    with tracer.start_as_current_span(\n                        \"parse_function_call\",\n                        attributes={\"function_name\": output.name}\n                    ):\n                        kwargs = json.loads(output.arguments)\n                        root_span.set_attribute(\"weather.latitude\", kwargs.get(\"latitude\"))\n                        root_span.set_attribute(\"weather.longitude\", kwargs.get(\"longitude\"))\n\n                    print(\"Weather API Response\")\n                    weather_response = get_weather(**kwargs)\n                    print(weather_response)\n\n            if weather_response:\n                with tracer.start_as_current_span(\"format_weather_response\"):\n                    print(user_prompt)\n                    formatted_answer = f\"Answer: {weather_response}\"\n                    print(formatted_answer)\n                    root_span.set_attribute(\"weather.answer\", formatted_answer)\n\n            # Mark the trace as successful\n            root_span.set_status(trace.StatusCode.OK)\n\n        except Exception as e:\n            # Record any exceptions that occurred\n            root_span.record_exception(e)\n            root_span.set_status(trace.StatusCode.ERROR, str(e))\n            print(f\"Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n    # Ensure all spans are exported before the program exits\n    trace_provider.shutdown()\n</code></pre>"},{"location":"examples/pydanticai-weather/","title":"Trace PydanticAI","text":""},{"location":"examples/pydanticai-weather/#running-the-example","title":"Running the example","text":"<p>To run this example, you need to add API keys to your environment:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"examples/pydanticai-weather/#running-with-uv","title":"Running with <code>uv</code>","text":"<p>You can run the example as a one-liner with zero setup:</p> <pre><code># Remember to export environment variables before running the example.\nuv run --no-cache --with \"patronus-examples[pydantic-ai]\" \\\n    -m patronus_examples.tracking.pydanticai_weather\n</code></pre>"},{"location":"examples/pydanticai-weather/#running-the-script-directly","title":"Running the script directly","text":"<p>If you've cloned the repository, you can run the script directly:</p> <pre><code># Clone the repository\ngit clone https://github.com/patronus-ai/patronus-py.git\ncd patronus-py\n\n# Run the example script (requires uv)\n./examples/patronus_examples/tracking/pydanticai_weather.py\n</code></pre>"},{"location":"examples/pydanticai-weather/#manual-installation","title":"Manual installation","text":"<p>If you prefer to copy the example code to your own project, you'll need to install these dependencies:</p> <pre><code>pip install patronus\npip install pydantic-ai-slim[openai]\npip install opentelemetry-instrumentation-asyncio\npip install opentelemetry-instrumentation-threading\n</code></pre>"},{"location":"examples/pydanticai-weather/#example-overview","title":"Example overview","text":"<p>This example demonstrates how to use Patronus to trace Pydantic-AI agent interactions in an asynchronous application. The example:</p> <ol> <li>Sets up two Pydantic-AI agents: a weather agent and a manager agent</li> <li>Configures the weather agent with a tool to provide mock weather data</li> <li>Configures the manager agent with a tool to call the weather agent</li> <li>Demonstrates how to handle agent-to-agent communication</li> </ol> <p>The example shows how Patronus can trace asynchronous workflows and provide visibility into multi-agent systems built with Pydantic-AI.</p>"},{"location":"examples/pydanticai-weather/#example-code","title":"Example code","text":"<pre><code># examples/patronus_examples/tracking/pydanticai_weather.py\n\nimport asyncio\nfrom pydantic_ai import Agent\nfrom opentelemetry.instrumentation.threading import ThreadingInstrumentor\nfrom opentelemetry.instrumentation.asyncio import AsyncioInstrumentor\nfrom patronus.integrations.pydantic_ai import PydanticAIIntegrator\nimport patronus\n\npatronus.init(\n    integrations=[\n        PydanticAIIntegrator(),\n        ThreadingInstrumentor(),\n        AsyncioInstrumentor(),\n    ]\n)\n\n\ndef get_agent(system_prompt=\"You are a helpful assistant\"):\n    agent = Agent(\"openai:gpt-4o\", output_type=str, system_prompt=system_prompt)\n    return agent\n\n\n@patronus.traced(\"weather-pydantic-ai\")\nasync def main():\n    # Create weather agent and attach tool to it\n    weather_agent = get_agent(\n        \"You are a helpful assistant that can help with weather information.\"\n    )\n\n    @weather_agent.tool_plain()\n    async def get_weather():\n        # Mock tool output\n        return (\n            \"Today's weather is Sunny with a forecasted high of 30\u00b0C and a low of 25\u00b0C. \"\n            \"The wind is expected at 4 km/h.\"\n        )\n\n    # Create manager agent\n    manager_agent = get_agent(\n        \"You are a helpful assistant that can coordinate with other subagents \"\n        \"and query them for more information about topics.\"\n    )\n\n    # Create a tool to execute the weather agent\n    @manager_agent.tool_plain()\n    async def call_weather_agent():\n        weather_info = await weather_agent.run(\"What is the weather in Paris, France?\")\n        return str(weather_info)\n\n    # Run the manager\n    print(\"Running the agent...\")\n    return await manager_agent.run(\"What is the weather in Paris, France?\")\n\n\nif __name__ == \"__main__\":\n    result = asyncio.run(main())\n    print(result)\n</code></pre>"},{"location":"examples/smolagents-weather/","title":"Trace Smolagents","text":""},{"location":"examples/smolagents-weather/#running-the-example","title":"Running the example","text":"<p>To run this example, you need to add API keys to your environment:</p> <pre><code>export PATRONUS_API_KEY=your-api-key\nexport OPENAI_API_KEY=your-api-key\n</code></pre>"},{"location":"examples/smolagents-weather/#running-with-uv","title":"Running with <code>uv</code>","text":"<p>You can run the example as a one-liner with zero setup:</p> <pre><code># Remember to export environment variables before running the example.\nuv run --no-cache --with \"patronus-examples[smolagents]\" \\\n    -m patronus_examples.tracking.smolagents_weather\n</code></pre>"},{"location":"examples/smolagents-weather/#running-the-script-directly","title":"Running the script directly","text":"<p>If you've cloned the repository, you can run the script directly:</p> <pre><code># Clone the repository\ngit clone https://github.com/patronus-ai/patronus-py.git\ncd patronus-py\n\n# Run the example script (requires uv)\n./examples/patronus_examples/tracking/smolagents_weather.py\n</code></pre>"},{"location":"examples/smolagents-weather/#manual-installation","title":"Manual installation","text":"<p>If you prefer to copy the example code to your own project, you'll need to install these dependencies:</p> <pre><code>pip install patronus\npip install smolagents[litellm]\npip install openinference-instrumentation-smolagents\npip install opentelemetry-instrumentation-threading\n</code></pre>"},{"location":"examples/smolagents-weather/#example-overview","title":"Example overview","text":"<p>This example demonstrates how to use Patronus to trace Smolagents tool calls and LLM interactions. The application:</p> <ol> <li>Sets up a Smolagents agent with a weather tool</li> <li>Configures a hierarchical agent structure with subagents</li> <li>Processes a user query about weather in Paris</li> <li>Handles the tool calling workflow automatically</li> </ol> <p>The example shows how Patronus provides visibility into the agent's decision-making process, tool usage, and interaction between different agent layers.</p>"},{"location":"examples/smolagents-weather/#example-code","title":"Example code","text":"<pre><code># examples/patronus_examples/tracking/smolagents_weather.py\n\nfrom datetime import datetime\n\nfrom openinference.instrumentation.smolagents import SmolagentsInstrumentor\nfrom opentelemetry.instrumentation.threading import ThreadingInstrumentor\nfrom smolagents import LiteLLMModel, ToolCallingAgent, tool\n\nimport patronus\n\npatronus.init(integrations=[SmolagentsInstrumentor(), ThreadingInstrumentor()])\n\n\n@tool\ndef get_weather_api(location: str, date_time: str) -&gt; str:\n    \"\"\"\n    Returns the weather report.\n\n    Args:\n        location: the name of the place that you want the weather for.\n            Should be a place name, followed by possibly a city name, then a country,\n            like \"Anchor Point, Taghazout, Morocco\".\n        date_time: the date and time for which you want the report, formatted as '%m/%d/%y %H:%M:%S'.\n    \"\"\"\n    try:\n        date_time = datetime.strptime(date_time, \"%m/%d/%y %H:%M:%S\")\n    except Exception as e:\n        raise ValueError(\n            \"Conversion of `date_time` to datetime format failed, \"\n            f\"make sure to provide a string in format '%m/%d/%y %H:%M:%S': {e}\"\n        )\n    temperature_celsius, risk_of_rain, wave_height = 10, 0.5, 4  # mock outputs\n    return (\n        f\"Weather report for {location}, {date_time}: \"\n        f\"Temperature will be {temperature_celsius}\u00b0C, \"\n        f\"risk of rain is {risk_of_rain * 100:.0f}%, wave height is {wave_height}m.\"\n    )\n\n\ndef create_agent(model_id):\n    # Create weather agent\n    weather_model = LiteLLMModel(model_id, temperature=0.0, top_p=1.0)\n    weather_subagent = ToolCallingAgent(\n        tools=[get_weather_api],\n        model=weather_model,\n        max_steps=10,\n        name=\"weather_agent\",\n        description=\"This agent can provide information about the weather at a certain location\",\n    )\n\n    # Create manager agent and add weather agent as subordinate\n    manager_model = LiteLLMModel(model_id, temperature=0.0, top_p=1.0)\n    agent = ToolCallingAgent(\n        model=manager_model,\n        managed_agents=[weather_subagent],\n        tools=[],\n        add_base_tools=False,\n    )\n    return agent\n\n\n@patronus.traced(\"weather-smolagents\")\ndef main():\n    agent = create_agent(\"openai/gpt-4o\")\n    agent.run(\"What is the weather in Paris, France?\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"experiments/advanced/","title":"Advanced Experiment Features","text":"<p>This page covers advanced features of the Patronus Experimentation Framework that help you build more sophisticated evaluation workflows.</p>"},{"location":"experiments/advanced/#multi-stage-processing-with-chains","title":"Multi-Stage Processing with Chains","text":"<p>For complex workflows, you can use chains to create multi-stage processing and evaluation pipelines. Chains connect multiple processing stages where the output of one stage becomes the input to the next.</p>"},{"location":"experiments/advanced/#basic-chain-structure","title":"Basic Chain Structure","text":"<pre><code>from patronus.experiments import run_experiment\nfrom patronus.evals import RemoteEvaluator\n\nexperiment = run_experiment(\n    dataset=dataset,\n    chain=[\n        # Stage 1: Generate summaries\n        {\n            \"task\": generate_summary,\n            \"evaluators\": [\n                RemoteEvaluator(\"judge\", \"conciseness\"),\n                RemoteEvaluator(\"judge\", \"coherence\")\n            ]\n        },\n        # Stage 2: Generate questions from summaries\n        {\n            \"task\": generate_questions,\n            \"evaluators\": [\n                RemoteEvaluator(\"judge\", \"relevance\"),\n                QuestionDiversityEvaluator()\n            ]\n        },\n        # Stage 3: Answer questions\n        {\n            \"task\": answer_questions,\n            \"evaluators\": [\n                RemoteEvaluator(\"judge\", \"factual-accuracy\"),\n                RemoteEvaluator(\"judge\", \"helpfulness\")\n            ]\n        }\n    ]\n)\n</code></pre> <p>Each stage in the chain can: 1. Apply its own task function (or no task if set to <code>None</code>) 2. Use its own set of evaluators 3. Access results from previous stages</p>"},{"location":"experiments/advanced/#accessing-previous-results-in-chain-tasks","title":"Accessing Previous Results in Chain Tasks","text":"<p>Tasks in later chain stages can access outputs and evaluations from earlier stages through the <code>parent</code> parameter:</p> <pre><code>def generate_questions(row, parent, **kwargs):\n    \"\"\"Generate questions based on a summary from the previous stage.\"\"\"\n    # Get the summary from the previous task\n    summary = parent.task.output if parent and parent.task else None\n\n    if not summary:\n        return None\n\n    # Check if summary evaluations are available\n    if parent and parent.evals:\n        coherence = parent.evals.get(\"judge:coherence\")\n        # Use previous evaluation results to guide question generation\n        if coherence and coherence.score &gt; 0.8:\n            return \"Here are three detailed questions based on the summary...\"\n        else:\n            return \"Here are three basic questions about the summary...\"\n\n    # Default questions if no evaluations available\n    return \"Here are some standard questions about the topic...\"\n</code></pre> <p>This example demonstrates how a task can adapt its behavior based on previous outputs and evaluations.</p>"},{"location":"experiments/advanced/#concurrency-controls","title":"Concurrency Controls","text":"<p>For better performance, the framework automatically processes dataset examples concurrently. You can control this behavior to prevent rate limiting or resource exhaustion:</p> <pre><code>experiment = run_experiment(\n    dataset=large_dataset,\n    task=api_intensive_task,\n    evaluators=[evaluator1, evaluator2],\n    # Limit the number of concurrent tasks and evaluations\n    max_concurrency=5\n)\n</code></pre> <p>This is particularly important for: - Tasks that make API calls with rate limits - Resource-intensive processing - Large datasets with many examples</p>"},{"location":"experiments/advanced/#opentelemetry-integrations","title":"OpenTelemetry Integrations","text":"<p>The framework supports OpenTelemetry instrumentation for enhanced tracing and monitoring:</p> <pre><code>from openinference.instrumentation.openai import OpenAIInstrumentor\n\nexperiment = run_experiment(\n    dataset=dataset,\n    task=openai_task,\n    evaluators=[evaluator1, evaluator2],\n    # Add OpenTelemetry instrumentors\n    integrations=[OpenAIInstrumentor()]\n)\n</code></pre> <p>Benefits of OpenTelemetry integration include: - Automatic capture of API calls and parameters - Detailed timing information for performance analysis - Integration with observability platforms</p>"},{"location":"experiments/advanced/#organizing-experiments","title":"Organizing Experiments","text":""},{"location":"experiments/advanced/#custom-experiment-names-and-projects","title":"Custom Experiment Names and Projects","text":"<p>Organize your experiments into projects with descriptive names for better management:</p> <pre><code>experiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[evaluator1, evaluator2],\n    # Organize experiments\n    project_name=\"RAG System Evaluation\",\n    experiment_name=\"baseline-gpt4-retrieval\"\n)\n</code></pre> <p>The framework automatically appends a timestamp to experiment names for uniqueness.</p>"},{"location":"experiments/advanced/#tags-for-filtering-and-organization","title":"Tags for Filtering and Organization","text":"<p>Tags help organize and filter experiment results:</p> <pre><code>experiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[evaluator1, evaluator2],\n    # Add tags for filtering and organization\n    tags={\n        \"model\": \"gpt-4\",\n        \"version\": \"2.0\",\n        \"retrieval_method\": \"bm25\",\n        \"environment\": \"staging\"\n    }\n)\n</code></pre> <p>Important notes about tags:</p> <ul> <li>Tags are propagated to all evaluation results in the experiment</li> <li>They cannot be overridden by tasks or evaluators</li> <li>Use a small set of consistent values for each tag (avoid having too many unique values)</li> <li>Tags are powerful for filtering and grouping in analysis</li> </ul>"},{"location":"experiments/advanced/#experiment-metadata","title":"Experiment Metadata","text":"<p>Experiments automatically capture important metadata, including evaluator weights when specified:</p> <pre><code>from patronus.experiments import run_experiment, FuncEvaluatorAdapter\nfrom patronus.evals import RemoteEvaluator\nfrom patronus import evaluator\n\n@evaluator()\ndef custom_check(row, **kwargs):\n    return True\n\n# Experiment with weighted evaluators\nexperiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[\n        RemoteEvaluator(\"judge\", \"patronus:is-concise\", weight=0.6),\n        FuncEvaluatorAdapter(custom_check, weight=\"0.4\")\n    ]\n)\n\n# Weights are automatically stored in experiment metadata\n# as \"evaluator_weights\": {\n#     \"judge:patronus:is-concise\": \"0.6\",\n#     \"custom_check:\": \"0.4\"\n# }\n</code></pre> <p>Evaluator weights are automatically collected and stored in the experiment's metadata under the <code>evaluator_weights</code> key. This provides a permanent record of how evaluators were weighted in each experiment for reproducibility and analysis.</p> <p>For more details on using evaluator weights, see the Using Evaluators page.</p>"},{"location":"experiments/advanced/#custom-api-configuration","title":"Custom API Configuration","text":"<p>For on-prem environments, you can customize the API configuration:</p> <pre><code>experiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[evaluator1, evaluator2],\n    # Custom API configuration\n    api_key=\"your-api-key\",\n    api_url=\"https://custom-endpoint.patronus.ai\",\n    otel_endpoint=\"https://custom-telemetry.patronus.ai\",\n    timeout_s=120\n)\n</code></pre>"},{"location":"experiments/advanced/#manual-experiment-control","title":"Manual Experiment Control","text":"<p>For fine-grained control over the experiment lifecycle, you can create and run experiments manually:</p> <pre><code>from patronus.experiments import Experiment\n\n# Create the experiment\nexperiment = await Experiment.create(\n    dataset=dataset,\n    task=task,\n    evaluators=evaluators,\n    # Additional configuration...\n)\n\n# Perform custom setup if needed\n# ...\n\n# Run the experiment when ready\nawait experiment.run()\n\n# Export results\nexperiment.to_csv(\"results.csv\")\n</code></pre> <p>This pattern is useful when you need to: - Perform additional setup after experiment creation - Control exactly when execution starts - Implement custom pre- or post-processing</p>"},{"location":"experiments/advanced/#best-practices","title":"Best Practices","text":"<p>When using advanced experiment features:</p> <ol> <li>Start simple: Begin with basic experiments before adding chain complexity</li> <li>Test incrementally: Validate each stage before combining them</li> <li>Monitor resources: Watch for memory usage with large datasets</li> <li>Set appropriate concurrency: Balance throughput against rate limits</li> <li>Use consistent tags: Create a standard tagging system across experiments</li> </ol>"},{"location":"experiments/datasets/","title":"Working with Datasets","text":"<p>Datasets provide the foundation for Patronus experiments, containing the examples that your tasks and evaluators will process. This page explains how to create, load, and work with datasets effectively.</p>"},{"location":"experiments/datasets/#dataset-structure-and-evaluator-compatibility","title":"Dataset Structure and Evaluator Compatibility","text":"<p>Patronus experiments are designed to work with <code>StructuredEvaluator</code> classes, which expect specific input parameters. The standard dataset fields map directly to these parameters, making integration seamless:</p> <ul> <li><code>system_prompt</code>: System instruction for LLM-based tasks</li> <li><code>task_context</code>: Additional information or context (string or list of strings)</li> <li><code>task_metadata</code>: Additional structured information about the task</li> <li><code>task_attachments</code>: Files or other binary data</li> <li><code>task_input</code>: The primary input query or text</li> <li><code>task_output</code>: The model's response or output to evaluate</li> <li><code>gold_answer</code>: The expected correct answer or reference output</li> <li><code>tags</code>: Key-value pairs</li> <li><code>sid</code>: A unique identifier for the example (automatically generated if not provided)</li> </ul> <p>While you can include any custom fields in your dataset, using these standard field names ensures compatibility with structured evaluators without additional configuration.</p>"},{"location":"experiments/datasets/#creating-datasets","title":"Creating Datasets","text":"<p>Patronus accepts datasets in several formats:</p>"},{"location":"experiments/datasets/#list-of-dictionaries","title":"List of Dictionaries","text":"<pre><code>dataset = [\n    {\n        \"task_input\": \"What is machine learning?\",\n        \"gold_answer\": \"Machine learning is a subfield of artificial intelligence...\",\n        \"tags\": {\"category\": \"ai\", \"difficulty\": \"beginner\"},\n        \"difficulty\": \"beginner\"  # Custom field\n    },\n    {\n        \"task_input\": \"Explain quantum computing\",\n        \"gold_answer\": \"Quantum computing uses quantum phenomena...\",\n        \"tags\": {\"category\": \"physics\", \"difficulty\": \"advanced\"},\n        \"difficulty\": \"advanced\"  # Custom field\n    }\n]\n\nexperiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[my_evaluator]\n)\n</code></pre>"},{"location":"experiments/datasets/#pandas-dataframe","title":"Pandas DataFrame","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    \"task_input\": [\"What is Python?\", \"What is JavaScript?\"],\n    \"gold_answer\": [\"Python is a programming language...\", \"JavaScript is a programming language...\"],\n    \"tags\": [{\"type\": \"backend\"}, {\"type\": \"frontend\"}],\n    \"language_type\": [\"backend\", \"frontend\"]  # Custom field\n})\n\nexperiment = run_experiment(dataset=df, ...)\n</code></pre>"},{"location":"experiments/datasets/#csv-or-jsonl-files","title":"CSV or JSONL Files","text":"<pre><code>from patronus.datasets import read_csv, read_jsonl\n\n# Load with default field mappings\ndataset = read_csv(\"questions.csv\")\n\n# Load with custom field mappings\ndataset = read_jsonl(\n    \"custom.jsonl\",\n    task_input_field=\"question\",     # Map \"question\" field to \"task_input\"\n    gold_answer_field=\"answer\",      # Map \"answer\" field to \"gold_answer\"\n    system_prompt_field=\"instruction\", # Map \"instruction\" field to \"system_prompt\"\n    tags_field=\"metadata\"            # Map \"metadata\" field to \"tags\"\n)\n</code></pre>"},{"location":"experiments/datasets/#remote-datasets","title":"Remote Datasets","text":"<p>Patronus allows you to work with datasets stored remotely on the Patronus platform. This is useful for sharing standard datasets across your organization or utilizing pre-built evaluation datasets.</p> <pre><code>from patronus.datasets import RemoteDatasetLoader\n\n# Load a dataset from the Patronus platform using its name\nremote_dataset = RemoteDatasetLoader(\"financebench\")\n\n# Load a dataset from the Patronus platform using its ID\nremote_dataset = RemoteDatasetLoader(by_id=\"d-eo6a5zy3nwach69b\")\n\nexperiment = run_experiment(\n    dataset=remote_dataset,\n    task=my_task,\n    evaluators=[my_evaluator],\n)\n</code></pre> <p>The <code>RemoteDatasetLoader</code> asynchronously fetches the dataset from the Patronus API when the experiment runs. It handles the data mapping automatically, transforming the API response into the standard dataset structure with all the expected fields (<code>system_prompt</code>, <code>task_input</code>, <code>gold_answer</code>, etc.).</p> <p>Remote datasets follow the same structure and field conventions as local datasets, making them interchangeable in your experiment code.</p>"},{"location":"experiments/datasets/#accessing-dataset-fields","title":"Accessing Dataset Fields","text":"<p>During experiment execution, dataset examples are provided as <code>Row</code> objects:</p> <pre><code>def my_task(row, **kwargs):\n    # Access standard fields\n    question = row.task_input\n    reference = row.gold_answer\n    context = row.task_context\n\n    # Access tags\n    if row.tags:\n        category = row.tags.get(\"category\")\n\n    # Access custom fields directly\n    difficulty = row.difficulty  # Access custom field by name\n\n    # Access row ID\n    sample_id = row.sid\n\n    return f\"Answering {difficulty} question (ID: {sample_id}): {question}\"\n</code></pre> <p>The <code>Row</code> object automatically provides attributes for all fields in your dataset, making access straightforward for both standard and custom fields.</p>"},{"location":"experiments/datasets/#using-custom-dataset-schemas","title":"Using Custom Dataset Schemas","text":"<p>If your dataset uses a different schema than the standard field names, you have two options:</p> <ol> <li> <p>Map fields during loading: Use field mapping parameters when loading data    <pre><code>from patronus.datasets import read_csv\n\ndataset = read_csv(\"data.csv\",\n                  task_input_field=\"question\",\n                  gold_answer_field=\"answer\",\n                  tags_field=\"metadata\")\n</code></pre></p> </li> <li> <p>Use evaluator adapters: Create adapters that transform your data structure to match what evaluators expect</p> </li> </ol> <pre><code>from patronus import evaluator\nfrom patronus.experiments import run_experiment, FuncEvaluatorAdapter\n\n@evaluator()\ndef my_evaluator_function(*, expected, actual, context):\n    ...\n\nclass CustomAdapter(FuncEvaluatorAdapter):\n    def transform(self, row, task_result, parent, **kwargs):\n        # Transform dataset fields to evaluator parameters.\n        # The first value is list of positional arguments (*args) passed to the evaluator function.\n        # The second value is named arguments (**kwargs) passed to the evaluator function.\n        return [], {\n            \"expected\": row.reference_answer,  # Map custom field to expected parameter\n            \"actual\": task_result.output if task_result else None,\n            \"context\": row.additional_info    # Map custom field to context parameter\n        }\n\nexperiment = run_experiment(\n    dataset=custom_dataset,\n    evaluators=[CustomAdapter(my_evaluator_function)]\n)\n</code></pre> <p>This adapter approach is particularly important for function-based evaluators, which need to be explicitly adapted for use in experiments.</p>"},{"location":"experiments/datasets/#dataset-ids-and-sample-ids","title":"Dataset IDs and Sample IDs","text":"<p>Each dataset and row can have identifiers that are used for organization and tracing:</p> <pre><code>from patronus.datasets import Dataset\n\n# Dataset with explicit ID\ndataset = Dataset.from_records(\n    records=[...],\n    dataset_id=\"qa-dataset-v1\"\n)\n\n# Dataset with explicit sample IDs\ndataset = Dataset.from_records([\n    {\"sid\": \"q1\", \"task_input\": \"Question 1\", \"gold_answer\": \"Answer 1\"},\n    {\"sid\": \"q2\", \"task_input\": \"Question 2\", \"gold_answer\": \"Answer 2\"}\n])\n</code></pre> <p>If not provided, sample IDs (<code>sid</code>) are automatically generated.</p>"},{"location":"experiments/datasets/#best-practices","title":"Best Practices","text":"<ol> <li>Use standard field names when possible: This minimizes the need for custom adapters</li> <li>Include gold answers: This enables more comprehensive evaluation</li> <li>Use tags for organization: Tags provide a flexible way to categorize examples</li> <li>Keep task inputs focused: Clear, concise inputs lead to better evaluations</li> <li>Add relevant metadata: Additional context helps with result analysis</li> <li>Normalize data before experiments: Pre-process data to ensure consistent format</li> <li>Consider remote datasets for team collaboration: Use the Patronus platform to share standardized datasets</li> </ol> <p>In the next section, we'll explore how to create tasks that process your dataset examples.</p>"},{"location":"experiments/evaluators/","title":"Using Evaluators in Experiments","text":"<p>Evaluators are the core assessment tools in Patronus experiments, measuring the quality of task outputs against defined criteria. This page covers how to use various types of evaluators in the Patronus Experimentation Framework.</p>"},{"location":"experiments/evaluators/#evaluator-types","title":"Evaluator Types","text":"<p>The framework supports several types of evaluators:</p> <ul> <li>Remote Evaluators: Use Patronus's managed evaluation services</li> <li>Custom Evaluators: Your own evaluation logic.<ul> <li>Function-based: Simple functions decorated with @evaluator() that need to be wrapped with FuncEvaluatorAdapter when used in experiments.</li> <li>Class-based: More powerful evaluators created by extending <code>StructuredEvaluator</code> (synchronous) or <code>AsyncStructuredEvaluator</code> (asynchronous) base classes with predefined interfaces.</li> </ul> </li> </ul> <p>Each type has different capabilities and use cases.</p>"},{"location":"experiments/evaluators/#remote-evaluators","title":"Remote Evaluators","text":"<p>Remote evaluators run on Patronus infrastructure and provide standardized, high-quality assessments:</p> <pre><code>from patronus.evals import RemoteEvaluator\nfrom patronus.experiments import run_experiment\n\nexperiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[\n        RemoteEvaluator(\"judge\", \"patronus:is-concise\"),\n        RemoteEvaluator(\"lynx\", \"patronus:hallucination\"),\n        RemoteEvaluator(\"judge\", \"patronus:is-helpful\")\n    ]\n)\n</code></pre>"},{"location":"experiments/evaluators/#class-based-evaluators","title":"Class-Based Evaluators","text":"<p>You can create custom evaluator classes by inheriting from the Patronus base classes:</p> <p>Note: The following example uses the <code>transformers</code> library from Hugging Face. Install it with <code>pip install transformers</code> before running this code.</p> <pre><code>import numpy as np\nfrom transformers import BertTokenizer, BertModel\n\nfrom patronus import StructuredEvaluator, EvaluationResult\nfrom patronus.experiments import run_experiment\n\n\nclass BERTScore(StructuredEvaluator):\n    def __init__(self, pass_threshold: float):\n        self.pass_threshold = pass_threshold\n        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n    def evaluate(self, *, task_output: str, gold_answer: str, **kwargs) -&gt; EvaluationResult:\n        output_toks = self.tokenizer(task_output, return_tensors=\"pt\", padding=True, truncation=True)\n        gold_answer_toks = self.tokenizer(gold_answer, return_tensors=\"pt\", padding=True, truncation=True)\n\n        output_embeds = self.model(**output_toks).last_hidden_state.mean(dim=1).detach().numpy()\n        gold_answer_embeds = self.model(**gold_answer_toks).last_hidden_state.mean(dim=1).detach().numpy()\n\n        score = np.dot(output_embeds, gold_answer_embeds.T) / (\n            np.linalg.norm(output_embeds) * np.linalg.norm(gold_answer_embeds)\n        )\n\n        return EvaluationResult(\n            score=score,\n            pass_=score &gt;= self.pass_threshold,\n            tags={\"pass_threshold\": str(self.pass_threshold)},\n        )\n\n\nexperiment = run_experiment(\n    dataset=[\n        {\n            \"task_output\": \"Translate 'Goodbye' to Spanish.\",\n            \"gold_answer\": \"Adi\u00f3s\",\n        }\n    ],\n    evaluators=[BERTScore(pass_threshold=0.8)],\n)\n</code></pre> <p>Class-based evaluators that inherit from <code>StructuredEvaluator</code> or <code>AsyncStructuredEvaluator</code> are automatically adapted for use in experiments.</p>"},{"location":"experiments/evaluators/#function-evaluators","title":"Function Evaluators","text":"<p>For simpler evaluation logic, you can use function-based evaluators. When using function evaluators in experiments, you must wrap them with <code>FuncEvaluatorAdapter</code>.</p>"},{"location":"experiments/evaluators/#standard-function-adapter","title":"Standard Function Adapter","text":"<p>By default, <code>FuncEvaluatorAdapter</code> expects functions that follow this interface:</p> <pre><code>from typing import Optional\nfrom patronus import evaluator\nfrom patronus.datasets import Row\nfrom patronus.experiments.types import TaskResult, EvalParent\nfrom patronus.evals import EvaluationResult\nfrom patronus.experiments import run_experiment, FuncEvaluatorAdapter\n\n@evaluator()\ndef standard_evaluator(\n    row: Row,\n    task_result: TaskResult,\n    parent: EvalParent,\n    **kwargs\n) -&gt; Optional[EvaluationResult]:\n    \"\"\"\n    Standard interface for function evaluators used with FuncEvaluatorAdapter.\n    \"\"\"\n    if not task_result or not task_result.output:\n        # Skip the evaluation\n        return None\n\n    if row.gold_answer and row.gold_answer.lower() in task_result.output.lower():\n        return EvaluationResult(score=1.0, pass_=True, text_output=\"Contains answer\")\n    else:\n        return EvaluationResult(score=0.0, pass_=False, text_output=\"Missing answer\")\n\n# Use with standard adapter\nexperiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[\n        FuncEvaluatorAdapter(standard_evaluator)\n    ]\n)\n</code></pre>"},{"location":"experiments/evaluators/#custom-function-adapters","title":"Custom Function Adapters","text":"<p>If your evaluator function doesn't match the standard interface, you can create a custom adapter:</p> <pre><code>from patronus import evaluator\nfrom patronus.datasets import Row\nfrom patronus.experiments.types import TaskResult, EvalParent\nfrom patronus.experiments.adapters import FuncEvaluatorAdapter\n\n# An evaluator function with a different interface\n@evaluator()\ndef exact_match(expected: str, actual: str, case_sensitive: bool = False) -&gt; bool:\n    \"\"\"\n    Checks if actual text exactly matches expected text.\n    \"\"\"\n    if not case_sensitive:\n        return expected.lower() == actual.lower()\n    return expected == actual\n\n# Custom adapter to transform experiment arguments to evaluator arguments\nclass ExactMatchAdapter(FuncEvaluatorAdapter):\n    def __init__(self, case_sensitive=False):\n        super().__init__(exact_match)\n        self.case_sensitive = case_sensitive\n\n    def transform(\n        self,\n        row: Row,\n        task_result: TaskResult,\n        parent: EvalParent,\n        **kwargs\n    ) -&gt; tuple[list, dict]:\n        # Create arguments list and dict for the evaluator function\n        args = []  # No positional arguments in this case\n\n        # Create keyword arguments matching the evaluator's parameters\n        evaluator_kwargs = {\n            \"expected\": row.gold_answer,\n            \"actual\": task_result.output if task_result else \"\",\n            \"case_sensitive\": self.case_sensitive\n        }\n\n        return args, evaluator_kwargs\n\n# Use custom adapter in an experiment\nexperiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[\n        ExactMatchAdapter(case_sensitive=False)\n    ]\n)\n</code></pre> <p>The <code>transform()</code> method is the key to adapting any function to the experiment framework. It takes the standard arguments provided by the framework and transforms them into the format your evaluator function expects.</p>"},{"location":"experiments/evaluators/#combining-evaluator-types","title":"Combining Evaluator Types","text":"<p>You can use multiple types of evaluators in a single experiment:</p> <pre><code>experiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[\n        # Remote evaluator\n        RemoteEvaluator(\"judge\", \"factual-accuracy\", weight=0.4),\n\n        # Class-based evaluator\n        BERTScore(pass_threshold=0.7, weight=0.3),\n\n        # Function evaluator with standard adapter\n        FuncEvaluatorAdapter(standard_evaluator, weight=0.2),\n\n        # Function evaluator with custom adapter\n        ExactMatchAdapter(case_sensitive=False, weight=0.1)\n    ]\n)\n</code></pre>"},{"location":"experiments/evaluators/#evaluator-chains","title":"Evaluator Chains","text":"<p>In multi-stage evaluation chains, evaluators from one stage can see the results of previous stages:</p> <pre><code>experiment = run_experiment(\n    dataset=dataset,\n    chain=[\n        # First stage\n        {\n            \"task\": generate_summary,\n            \"evaluators\": [\n                RemoteEvaluator(\"judge\", \"conciseness\"),\n                RemoteEvaluator(\"judge\", \"coherence\")\n            ]\n        },\n        # Second stage - evaluating based on first stage results\n        {\n            \"task\": None,  # No additional processing\n            \"evaluators\": [\n                # This evaluator can see previous evaluations\n                DependentEvaluator()\n            ]\n        }\n    ]\n)\n\n# Example of a function evaluator that uses previous results\n@evaluator()\ndef final_aggregate_evaluator(row, task_result, parent, **kwargs):\n    # Check if we have previous evaluation results\n    if not parent or not parent.evals:\n        return None\n\n    # Access evaluations from previous stage\n    conciseness = parent.evals.get(\"judge:conciseness\")\n    coherence = parent.evals.get(\"judge:coherence\")\n\n    # Use the previous results\n    avg_score = ((conciseness.score or 0) + (coherence.score or 0)) / 2\n    return EvaluationResult(score=avg_score, pass_=avg_score &gt; 0.7)\n</code></pre>"},{"location":"experiments/evaluators/#evaluator-weights-experiments-only","title":"Evaluator Weights (Experiments Only)","text":"<p>Experiments Feature</p> <p>Evaluator weights are only supported when using evaluators within the experiment framework. This feature is not available for standalone evaluator usage.</p> <p>You can assign weights to evaluators to indicate their relative importance in your evaluation strategy. Weights can be provided as either strings or floats representing valid decimal numbers and are automatically stored as experiment metadata.</p> <p>Weights work consistently across all evaluator types but are configured differently depending on whether you're using remote evaluators, function-based evaluators, or class-based evaluators.</p>"},{"location":"experiments/evaluators/#weight-support-by-evaluator-type","title":"Weight Support by Evaluator Type","text":"<p>Each evaluator type handles weight configuration differently:</p>"},{"location":"experiments/evaluators/#remote-evaluators_1","title":"Remote Evaluators","text":"<p>For remote evaluators, pass the <code>weight</code> parameter directly to the <code>RemoteEvaluator</code> constructor:</p> <pre><code>from patronus.evals import RemoteEvaluator\nfrom patronus.experiments import run_experiment\n\n# Remote evaluator with weight (string or float)\npii_evaluator = RemoteEvaluator(\"pii\", \"patronus:pii:1\", weight=\"0.6\")\nconciseness_evaluator = RemoteEvaluator(\"judge\", \"patronus:is-concise\", weight=0.4)\n\nexperiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[pii_evaluator, conciseness_evaluator]\n)\n</code></pre>"},{"location":"experiments/evaluators/#function-based-evaluators","title":"Function-Based Evaluators","text":"<p>For function-based evaluators, pass the <code>weight</code> parameter to the <code>FuncEvaluatorAdapter</code> that wraps your evaluator function:</p> <pre><code>from patronus import evaluator\nfrom patronus.experiments import FuncEvaluatorAdapter, run_experiment\nfrom patronus.datasets import Row\n\n@evaluator()\ndef exact_match(row: Row, **kwargs) -&gt; bool:\n    return row.task_output.lower().strip() == row.gold_answer.lower().strip()\n\n# Function evaluator with weight (string or float)\nexact_match_weighted = FuncEvaluatorAdapter(exact_match, weight=0.7)\n\nexperiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[exact_match_weighted]\n)\n</code></pre>"},{"location":"experiments/evaluators/#class-based-evaluators_1","title":"Class-Based Evaluators","text":"<p>For class-based evaluators, pass the <code>weight</code> parameter to your evaluator's constructor and ensure it's passed to the parent class:</p> <pre><code>from typing import Union\nfrom patronus import StructuredEvaluator, EvaluationResult\nfrom patronus.experiments import run_experiment\n\nclass CustomEvaluator(StructuredEvaluator):\n    def __init__(self, threshold: float, weight: Union[str, float] = None):\n        super().__init__(weight=weight)  # Pass to parent class\n        self.threshold = threshold\n\n    def evaluate(self, *, task_output: str, **kwargs) -&gt; EvaluationResult:\n        score = len(task_output) / 100  # Simple length-based scoring\n        return EvaluationResult(\n            score=score,\n            pass_=score &gt;= self.threshold\n        )\n\n# Class-based evaluator with weight (string or float)\ncustom_evaluator = CustomEvaluator(threshold=0.5, weight=0.3)\n\nexperiment = run_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[custom_evaluator]\n)\n</code></pre>"},{"location":"experiments/evaluators/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example demonstrating weighted evaluators of all three types, based on the patterns shown in the experiment framework:</p> <pre><code>from patronus.experiments import FuncEvaluatorAdapter, run_experiment\nfrom patronus import RemoteEvaluator, EvaluationResult, StructuredEvaluator, evaluator\nfrom patronus.datasets import Row\n\nclass DummyEvaluator(StructuredEvaluator):\n    def evaluate(self, task_output: str, gold_answer: str, **kwargs) -&gt; EvaluationResult:\n        return EvaluationResult(score_raw=1, pass_=True)\n\n@evaluator\ndef exact_match(row: Row, **kwargs) -&gt; bool:\n    return row.task_output.lower().strip() == row.gold_answer.lower().strip()\n\nexperiment = run_experiment(\n    project_name=\"Weighted Evaluation Example\",\n    dataset=[\n        {\n            \"task_input\": \"Please provide your contact details.\",\n            \"task_output\": \"My email is john.doe@example.com and my phone number is 123-456-7890.\",\n            \"gold_answer\": \"My email is john.doe@example.com and my phone number is 123-456-7890.\",\n        },\n        {\n            \"task_input\": \"Share your personal information.\",\n            \"task_output\": \"My name is Jane Doe and I live at 123 Elm Street.\",\n            \"gold_answer\": \"My name is Jane Doe and I live at 123 Elm Street.\",\n        },\n    ],\n    evaluators=[\n        RemoteEvaluator(\"pii\", \"patronus:pii:1\", weight=\"0.3\"),           # Remote evaluator with string weight\n        FuncEvaluatorAdapter(exact_match, weight=\"0.3\"),                   # Function evaluator with string weight\n        DummyEvaluator(weight=\"0.4\"),                                      # Class evaluator with string weight\n    ],\n    experiment_name=\"Weighted Evaluators Demo\"\n)\n</code></pre>"},{"location":"experiments/evaluators/#weight-validation-and-rules","title":"Weight Validation and Rules","text":"<ol> <li>Experiments Only: Weights are exclusively available within the experiment framework - they cannot be used with standalone evaluator calls</li> <li>Valid Format: Weights must be valid decimal numbers provided as either strings or floats (e.g., \"0.3\", 1.0, 0.7)</li> <li>Consistency: The same evaluator (identified by its canonical name) cannot have different weights within the same experiment</li> <li>Automatic Storage: Weights are automatically collected and stored in the experiment's metadata under the \"evaluator_weights\" key</li> <li>Optional: Weights are optional - evaluators without weights will simply not have weight metadata stored</li> <li>Best Practice: Consider making weights sum to 1.0 for clearer interpretation of relative importance</li> </ol>"},{"location":"experiments/evaluators/#error-examples","title":"Error Examples","text":"<pre><code># Invalid weight format - will raise TypeError\nRemoteEvaluator(\"judge\", \"patronus:is-concise\", weight=\"invalid\")\nRemoteEvaluator(\"judge\", \"patronus:is-concise\", weight=[1, 2, 3])  # Lists not supported\n\n# Inconsistent weights for same evaluator - will raise TypeError during experiment\nrun_experiment(\n    dataset=dataset,\n    task=my_task,\n    evaluators=[\n        RemoteEvaluator(\"judge\", \"patronus:is-concise\", weight=0.7),\n        RemoteEvaluator(\"judge\", \"patronus:is-concise\", weight=\"0.3\"),  # Different weight!\n    ]\n)\n</code></pre>"},{"location":"experiments/evaluators/#best-practices","title":"Best Practices","text":"<p>When using evaluators in experiments:</p> <ol> <li>Use the right evaluator type for the job: Remote evaluators for standardized assessments, custom evaluators for specialized logic</li> <li>Focus each evaluator on one aspect: Create multiple focused evaluators rather than one complex evaluator</li> <li>Provide detailed explanations: Include explanations to help understand evaluation results</li> <li>Create custom adapters when needed: Don't force your evaluator functions to match the standard interface if there's a more natural way to express them</li> <li>Handle edge cases gracefully: Consider what happens with empty inputs, very long texts, etc.</li> <li>Reuse evaluators across experiments: Create a library of evaluators for consistent assessment</li> <li>Weight consistency across evaluator types: When using evaluator weights, maintain consistency across experiments regardless of whether you're using remote, function-based, or class-based evaluators</li> <li>Consider weight distribution: When using weights, consider making them sum to 1.0 for clearer interpretation of relative importance (e.g., \"0.4\", \"0.3\", \"0.3\" rather than \"0.1\", \"0.1\", \"0.1\")</li> <li>Document weight rationale: Consider documenting why specific weights were chosen for your evaluation strategy, especially when mixing different evaluator types</li> </ol> <p>Next, we'll explore advanced features of the Patronus Experimentation Framework.</p>"},{"location":"experiments/introduction/","title":"Introduction to Experiments","text":"<p>The Patronus Experimentation Framework provides a systematic way to evaluate, compare, and improve Large Language Model (LLM) applications. By standardizing the evaluation process, the framework enables consistent testing across model versions, prompting strategies, and data inputs.</p>"},{"location":"experiments/introduction/#what-are-experiments","title":"What are Experiments?","text":"<p>In Patronus, an experiment is a structured evaluation that:</p> <ol> <li>Processes a dataset of examples</li> <li>Runs each example through a task function (optional)</li> <li>Evaluates the output using one or more evaluators</li> <li>Records and analyzes the results</li> </ol> <p>This approach provides a comprehensive view of how your LLM application performs across different inputs, making it easier to identify strengths, weaknesses, and areas for improvement.</p>"},{"location":"experiments/introduction/#key-concepts","title":"Key Concepts","text":""},{"location":"experiments/introduction/#dataset","title":"Dataset","text":"<p>A dataset in Patronus consists of examples that your models or systems will process. Each example, represented as a <code>Row</code> object, can contain:</p> <ul> <li>Input data</li> <li>Context information</li> <li>Expected outputs (gold answers)</li> <li>Metadata</li> <li>And more...</li> </ul> <p>Datasets can be loaded from various sources including JSON files, CSV files, Pandas DataFrames, or defined directly in your code.</p>"},{"location":"experiments/introduction/#task","title":"Task","text":"<p>A task is a function that processes each dataset example. Tasks typically:</p> <ul> <li>Receive a <code>Row</code> object from the dataset</li> <li>Perform some processing (like calling an LLM)</li> <li>Return a <code>TaskResult</code> containing the output</li> </ul> <p>Tasks are optional - you can evaluate pre-existing outputs by including them directly in your dataset.</p>"},{"location":"experiments/introduction/#evaluators","title":"Evaluators","text":"<p>Evaluators assess the quality of task outputs based on specific criteria. Patronus supports various types of evaluators:</p> <ul> <li>Remote Evaluators: Use Patronus's managed evaluation services</li> <li>Custom Evaluators: Your own evaluation logic.<ul> <li>Function-based: Simple functions decorated with @evaluator() that need to be wrapped with FuncEvaluatorAdapter when used in experiments.</li> <li>Class-based: More powerful evaluators created by extending <code>StructuredEvaluator</code> (synchronous) or <code>AsyncStructuredEvaluator</code> (asynchronous) base classes with predefined interfaces.</li> </ul> </li> </ul> <p>Each evaluator produces an <code>EvaluationResult</code> containing scores, pass/fail status, explanations, and other metadata.</p> <p>Evaluator Weights: You can assign weights to evaluators to indicate their relative importance in your evaluation strategy. Weights are stored as experiment metadata and can be provided as either strings or floats representing valid decimal numbers. See the Using Evaluators page for detailed information.</p>"},{"location":"experiments/introduction/#chains","title":"Chains","text":"<p>For more complex workflows, Patronus supports multi-stage evaluation chains where the output of one evaluation stage becomes the input for the next. This allows for pipeline-based approaches to LLM evaluation.</p>"},{"location":"experiments/introduction/#why-use-the-experimentation-framework","title":"Why Use the Experimentation Framework?","text":"<p>The Patronus Experimentation Framework offers several advantages over ad-hoc evaluation approaches:</p> <ul> <li>Consistency: Standardized evaluation across models and time</li> <li>Reproducibility: Experiments can be re-run with the same configuration</li> <li>Scalability: Process large datasets efficiently with concurrent execution</li> <li>Comprehensive Analysis: Collect detailed metrics and explanations</li> <li>Integration: Built-in tracing and logging with the broader Patronus ecosystem</li> </ul>"},{"location":"experiments/introduction/#example-basic-experiment","title":"Example: Basic Experiment","text":"<p>Here's a simple example of a Patronus experiment:</p> <pre><code># experiment.py\n\nfrom patronus.evals import RemoteEvaluator\nfrom patronus.experiments import run_experiment\n\n# Define a simple task function\ndef my_task(row, **kwargs):\n    return f\"The answer is: {row.task_input}\"\n\n# Run the experiment\nexperiment = run_experiment(\n    dataset=[\n        {\"task_input\": \"What is 2+2?\", \"gold_answer\": \"4\"},\n        {\"task_input\": \"Who wrote Hamlet?\", \"gold_answer\": \"Shakespeare\"}\n    ],\n    task=my_task,\n    evaluators=[\n        RemoteEvaluator(\"judge\", \"patronus:fuzzy-match\")\n    ]\n)\n\nexperiment.to_csv(\"./experiment-result.csv\")\n</code></pre> <p>You can run the experiment by simply executing the python file:</p> <pre><code>python ./exeriment.py\n</code></pre> <p>The output of the script should look similar to this:</p> <pre><code>==================================\nExperiment  Global/root-1742834029: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04&lt;00:00,  2.44s/sample]\n\npatronus:fuzzy-match (judge) [link_idx=0]\n-----------------------------------------\nCount     : 2\nPass rate : 0\nMean      : 0.0\nMin       : 0.0\n25%       : 0.0\n50%       : 0.0\n75%       : 0.0\nMax       : 0.0\n\nScore distribution\nScore Range          Count      Histogram\n0.00 - 0.20          2          ####################\n0.20 - 0.40          0\n0.40 - 0.60          0\n0.60 - 0.80          0\n0.80 - 1.00          0\n</code></pre> <p>In the following sections, we'll explore how to set up, run, and analyze experiments in detail.</p>"},{"location":"experiments/running/","title":"Running Experiments","text":"<p>This page covers how to set up and run experiments using the Patronus Experimentation Framework.</p>"},{"location":"experiments/running/#basic-experiment-structure","title":"Basic Experiment Structure","text":"<p>A Patronus experiment requires at minimum:</p> <ul> <li>A dataset to process</li> <li>One or more evaluators to assess outputs</li> </ul> <p>Additionally, most experiments will include:</p> <ul> <li>A task function that processes each dataset example</li> <li>Configuration options for tracing, logging, and concurrency</li> </ul>"},{"location":"experiments/running/#setting-up-an-experiment","title":"Setting Up an Experiment","text":""},{"location":"experiments/running/#the-run_experiment-function","title":"The <code>run_experiment</code> Function","text":"<p>The main entry point for the framework is the <code>run_experiment()</code> function:</p> <pre><code>from patronus.experiments import run_experiment\n\nexperiment = run_experiment(\n    dataset=my_dataset,               # Required: What to evaluate\n    task=my_task_function,            # Optional: How to process inputs\n    evaluators=[my_evaluator],        # Required: How to assess outputs\n    tags={\"dataset-version\": \"v1.0\"}, # Optional: Tags for the experiment\n    max_concurrency=10,               # Optional: Control parallel execution\n    project_name=\"My Project\",        # Optional: Override the global project name\n    experiment_name=\"Test Run\"        # Optional: Name this experiment run\n)\n</code></pre>"},{"location":"experiments/running/#creating-a-simple-experiment","title":"Creating a Simple Experiment","text":"<p>Let's walk through a complete example:</p> <pre><code>from patronus import evaluator, RemoteEvaluator\nfrom patronus.experiments import run_experiment, FuncEvaluatorAdapter\n\ndataset = [\n    {\n        \"task_input\": \"What is the capital of France?\",\n        \"gold_answer\": \"Paris\"\n    },\n    {\n        \"task_input\": \"Who wrote Romeo and Juliet?\",\n        \"gold_answer\": \"William Shakespeare\"\n    }\n]\n\n# Define a task (in a real scenario, this would call an LLM)\ndef answer_question(row, **kwargs):\n    if \"France\" in row.task_input:\n        return \"The capital of France is Paris.\"\n    elif \"Romeo and Juliet\" in row.task_input:\n        return \"Romeo and Juliet was written by William Shakespeare.\"\n    return \"I don't know the answer to that question.\"\n\n@evaluator()\ndef contains_answer(task_result, row, **kwargs) -&gt; bool:\n    if not task_result or not row.gold_answer:\n        return False\n    return row.gold_answer.lower() in task_result.output.lower()\n\nrun_experiment(\n    dataset=dataset,\n    task=answer_question,\n    evaluators=[\n        # Use a Patronus-managed evaluator\n        RemoteEvaluator(\"judge\", \"patronus:fuzzy-match\"),\n\n        # Use our custom evaluator\n        FuncEvaluatorAdapter(contains_answer)\n    ],\n    tags={\"model\": \"simulated\", \"version\": \"v1\"}\n)\n</code></pre>"},{"location":"experiments/running/#experiment-execution-flow","title":"Experiment Execution Flow","text":"<p>When you call <code>run_experiment()</code>, the framework follows these steps:</p> <ol> <li>Preparation: Initializes the experiment context and prepares the dataset</li> <li>Processing: For each dataset row:</li> <li>Runs the task function if provided</li> <li>Passes the task output to the evaluators</li> <li>Collects evaluation results</li> <li>Reporting: Generates a summary of evaluation results</li> <li>Return: Returns an <code>Experiment</code> object with the complete results</li> </ol>"},{"location":"experiments/running/#synchronous-vs-asynchronous-execution","title":"Synchronous vs. Asynchronous Execution","text":"<p>The <code>run_experiment()</code> function detects whether it's being called from an async context:</p> <ul> <li>In a synchronous context, it will block until the experiment completes</li> <li>In an async context, it returns an awaitable that can be awaited</li> </ul> <pre><code># Synchronous usage:\nexperiment = run_experiment(dataset, task, evaluators)\n\n# Asynchronous usage:\nexperiment = await run_experiment(dataset, task, evaluators)\n</code></pre>"},{"location":"experiments/running/#manual-experiment-control","title":"Manual Experiment Control","text":"<p>For more control over the experiment lifecycle, you can create and run an experiment manually:</p> <pre><code>from patronus.experiments import Experiment\n\n# Create the experiment\nexperiment = await Experiment.create(\n    dataset=dataset,\n    task=task,\n    evaluators=evaluators,\n    # Additional configuration options...\n)\n\n# Run the experiment when ready\nexperiment = await experiment.run()\n</code></pre> <p>This approach is useful when you need to perform additional setup between experiment creation and execution.</p>"},{"location":"experiments/running/#experiment-results","title":"Experiment Results","text":"<p>After an experiment completes, you can access the results in several ways:</p> <pre><code># Get a Pandas DataFrame\ndf = experiment.to_dataframe()\n\n# Save to CSV\nexperiment.to_csv(\"results.csv\")\n\n# Access the built-in summary\n# (This is automatically printed at the end of the experiment)\n</code></pre> <p>The experiment results include:</p> <ul> <li>Inputs from the dataset</li> <li>Task outputs</li> <li>Evaluation scores and pass/fail statuses</li> <li>Explanations and metadata</li> <li>Performance timing information</li> </ul> <p>In the next sections, we'll explore datasets, tasks, and evaluators in more detail.</p>"},{"location":"experiments/tasks/","title":"Creating Tasks","text":"<p>Tasks in Patronus experiments are functions that process each dataset example and produce outputs that will be evaluated. This page covers how to create and use tasks effectively.</p>"},{"location":"experiments/tasks/#task-function-basics","title":"Task Function Basics","text":"<p>A task function receives a dataset row and produces an output. The simplest task functions look like this:</p> <pre><code>def simple_task(row, **kwargs):\n    # Process the input from the row\n    output = f\"The output is: '{row.task_input}'\"\n\n    # Return the output\n    return output\n</code></pre> <p>The framework automatically converts numeric outputs to <code>TaskResult</code> objects.</p>"},{"location":"experiments/tasks/#task-function-parameters","title":"Task Function Parameters","text":"<p>Task functions always receive these parameters:</p> <ul> <li><code>row</code>: Row - The dataset example to process</li> <li><code>parent</code>: EvalParent - Information from previous chain stages (if any)</li> <li><code>tags</code>: Tags - Tags associated with the experiment and dataset</li> <li><code>**kwargs</code>: Additional keyword arguments</li> </ul> <p>Here's a more complete task function:</p> <pre><code>from patronus.datasets import Row\nfrom patronus.experiments.types import EvalParent\n\ndef complete_task(\n    row: Row,\n    parent: EvalParent = None,\n    tags: dict[str, str] = None,\n    **kwargs\n):\n    # Access dataset fields\n    input_text = row.task_input\n    context = row.task_context\n    system_prompt = row.system_prompt\n    gold_answer = row.gold_answer\n\n    # Access parent information (from previous chain steps)\n    previous_output = None\n    if parent and parent.task:\n        previous_output = parent.task.output\n\n    # Access tags\n    model_name = tags.get(\"model_name\", \"default\")\n\n    # Generate output (in real usage, this would call an LLM)\n    output = f\"Model {model_name} processed: {input_text}\"\n\n    # Return the output\n    return output\n</code></pre>"},{"location":"experiments/tasks/#return-types","title":"Return Types","text":"<p>Task functions can return several types:</p>"},{"location":"experiments/tasks/#string-output","title":"String Output","text":"<p>Here's an improved example for the string return type section that demonstrates a classification task:</p> <pre><code>def classify_sentiment(row: Row, **kwargs) -&gt; str:\n    # Extract the text to classify\n    text = row.task_input\n\n    # Simple rule-based sentiment classifier\n    positive_words = [\"good\", \"great\", \"excellent\", \"happy\", \"positive\"]\n    negative_words = [\"bad\", \"terrible\", \"awful\", \"sad\", \"negative\"]\n\n    text_lower = text.lower()\n    positive_count = sum(word in text_lower for word in positive_words)\n    negative_count = sum(word in text_lower for word in negative_words)\n\n    # Classify based on word counts\n    if positive_count &gt; negative_count:\n        return \"positive\"\n    elif negative_count &gt; positive_count:\n        return \"negative\"\n    else:\n        return \"neutral\"\n</code></pre> <p>The string output represents a specific classification category, which is a common pattern in text classification tasks.</p>"},{"location":"experiments/tasks/#taskresult-object","title":"TaskResult Object","text":"<p>For more control, return a TaskResult object:</p> <pre><code>from patronus.experiments.types import TaskResult\n\ndef task_result(row: Row, **kwargs) -&gt; TaskResult:\n    # Generate output\n    output = f\"Processed: {row.task_input}\"\n\n    # Include metadata about the processing\n    metadata = {\n        \"processing_time_ms\": 42,\n        \"confidence\": 0.95,\n        \"tokens_used\": 150\n    }\n\n    # Add tags for filtering and organization\n    tags = {\n        \"model\": \"gpt-4\",\n        \"temperature\": \"0.7\"\n    }\n\n    # Generate context\n    context = \"Context of the processing process\"\n\n    # Return a complete TaskResult\n    return TaskResult(\n        output=output,\n        metadata=metadata,\n        tags=tags,\n        context=context,\n    )\n</code></pre>"},{"location":"experiments/tasks/#none-skipping-examples","title":"None / Skipping Examples","text":"<p>Return <code>None</code> to skip processing this example:</p> <pre><code>def selective_task(row: Row, **kwargs) -&gt; None:\n    # Skip examples without the required fields\n    if not row.task_input or not row.gold_answer:\n        return None\n\n    # Process valid examples\n    return f\"Processed: {row.task_input}\"\n</code></pre>"},{"location":"experiments/tasks/#calling-llms","title":"Calling LLMs","text":"<p>A common use of tasks is to generate outputs using Large Language Models:</p> <pre><code>from openai import OpenAI\nfrom patronus.datasets import Row\nfrom patronus.experiments.types import TaskResult\n\noai = OpenAI()\n\ndef openai_task(row: Row, **kwargs) -&gt; TaskResult:\n    # Prepare the input for the model\n    system_message = row.system_prompt or \"You are a helpful assistant.\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": row.task_input}\n    ]\n\n    # Call the OpenAI API\n    response = oai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=150\n    )\n\n    # Extract the output\n    output = response.choices[0].message.content\n\n    # Include metadata about the call\n    metadata = {\n        \"model\": response.model,\n        \"tokens\": {\n            \"prompt\": response.usage.prompt_tokens,\n            \"completion\": response.usage.completion_tokens,\n            \"total\": response.usage.total_tokens\n        }\n    }\n\n    return TaskResult(\n        output=output,\n        metadata=metadata\n    )\n</code></pre>"},{"location":"experiments/tasks/#async-tasks","title":"Async Tasks","text":"<p>For better performance, especially with API calls, you can use async tasks:</p> <pre><code>import asyncio\nfrom openai import AsyncOpenAI\nfrom patronus.datasets import Row\nfrom patronus.experiments.types import TaskResult\n\noai = AsyncOpenAI()\n\nasync def async_openai_task(\n    row: Row,\n    parent: EvalParent = None,\n    tags: dict[str, str] = None,\n    **kwargs\n) -&gt; TaskResult:\n    # Create async client\n\n    # Prepare the input\n    system_message = row.system_prompt or \"You are a helpful assistant.\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": row.task_input}\n    ]\n\n    # Call the OpenAI API asynchronously\n    response = await oai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=150\n    )\n\n    # Extract and return the output\n    output = response.choices[0].message.content\n\n    return TaskResult(\n        output=output,\n        metadata={\"model\": response.model}\n    )\n</code></pre> <p>The Patronus framework automatically handles both synchronous and asynchronous tasks.</p>"},{"location":"experiments/tasks/#using-parent-information","title":"Using Parent Information","text":"<p>In multi-stage chains, tasks can access the results of previous stages:</p> <pre><code>from patronus.datasets import Row\nfrom patronus.experiments.types import EvalParent\n\ndef second_stage_task(\n    row: Row,\n    parent: EvalParent,\n    tags: dict[str, str] = None,\n    **kwargs\n) -&gt; str:\n    # Access previous task output\n    if parent and parent.task:\n        previous_output = parent.task.output\n        return f\"Building on previous output: {previous_output}\"\n\n    # Fallback if no previous output\n    return f\"Starting fresh: {row.task_input}\"\n</code></pre>"},{"location":"experiments/tasks/#error-handling","title":"Error Handling","text":"<p>Task functions should handle exceptions appropriately:</p> <pre><code>from patronus import get_logger\nfrom patronus.datasets import Row\n\ndef robust_task(row: Row, **kwargs):\n    try:\n        # Attempt to process\n        if row.task_input:\n            return f\"Processed: {row.task_input}\"\n        else:\n            # Skip if input is missing\n            return None\n    except Exception as e:\n        # Log the error\n        get_logger().exception(f\"Error processing row {row.sid}: {e}\")\n        # Skip this example\n        return None\n</code></pre> <p>If an unhandled exception occurs, the experiment will log the error and skip that example.</p>"},{"location":"experiments/tasks/#task-tracing","title":"Task Tracing","text":"<p>Tasks are automatically traced with the Patronus tracing system. You can add additional tracing:</p> <pre><code>from patronus.tracing import start_span\nfrom patronus.datasets import Row\n\ndef traced_task(row: Row, **kwargs):\n    # Outer span is created automatically by the framework\n\n    # Create spans for subtasks\n    with start_span(\"Preprocessing\"):\n        # Preprocessing logic...\n        preprocessed = preprocess(row.task_input)\n\n    with start_span(\"Model Call\"):\n        # Model call logic...\n        output = call_model(preprocessed)\n\n    with start_span(\"Postprocessing\"):\n        # Postprocessing logic...\n        final_output = postprocess(output)\n\n    return final_output\n</code></pre> <p>This helps with debugging and performance analysis.</p>"},{"location":"experiments/tasks/#best-practices","title":"Best Practices","text":"<p>When creating task functions:</p> <ol> <li>Handle missing data gracefully: Check for required fields and handle missing data</li> <li>Include useful metadata: Add information about processing steps, model parameters, etc.</li> <li>Use async for API calls: Async tasks significantly improve performance for API-dependent workflows</li> <li>Add explanatory tags: Tags help with filtering and analyzing results</li> <li>Add tracing spans: For complex processing, add spans to help with debugging and optimization</li> <li>Keep functions focused: Tasks should have a clear purpose; use chains for multi-step processes</li> </ol> <p>Next, we'll explore how to use evaluators in experiments to assess task outputs.</p>"},{"location":"getting-started/initialization/","title":"Initialization","text":""},{"location":"getting-started/initialization/#api-key","title":"API Key","text":"<p>To use the Patronus SDK, you'll need an API key from the Patronus platform. If you don't have one yet:</p> <ol> <li>Sign up at https://app.patronus.ai</li> <li>Navigate to \"API Keys\"</li> <li>Create a new API key</li> </ol>"},{"location":"getting-started/initialization/#configuration","title":"Configuration","text":"<p>There are several ways to configure the Patronus SDK:</p>"},{"location":"getting-started/initialization/#environment-variables","title":"Environment Variables","text":"<p>Set your API key as an environment variable:</p> <pre><code>export PATRONUS_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"getting-started/initialization/#configuration-file","title":"Configuration File","text":"<p>Create a <code>patronus.yaml</code> file in your project directory:</p> <pre><code>api_key: \"your-api-key\"\nproject_name: \"Global\"\napp: \"default\"\n</code></pre>"},{"location":"getting-started/initialization/#direct-configuration","title":"Direct Configuration","text":"<p>Pass configuration values directly when initializing the SDK:</p> <pre><code>import patronus\n\npatronus.init(\n    api_key=\"your-api-key\",\n    project_name=\"Global\",\n    app=\"default\",\n)\n</code></pre>"},{"location":"getting-started/initialization/#verification","title":"Verification","text":"<p>To verify your installation and configuration:</p> <pre><code>import patronus\n\npatronus.init()\n\n# Create a simple tracer\n@patronus.traced()\ndef test_function():\n    return \"Installation successful!\"\n\n# Call the function to test tracing\nresult = test_function()\nprint(result)\n</code></pre> <p>If no errors occur, your Patronus SDK is correctly installed and configured.</p>"},{"location":"getting-started/initialization/#advanced","title":"Advanced","text":""},{"location":"getting-started/initialization/#return-value","title":"Return Value","text":"<p>The <code>patronus.init()</code> function returns a <code>PatronusContext</code> object that serves as the central access point for all SDK components and functionality. Additionally, <code>patronus.init()</code> automatically sets this context globally, making it accessible throughout your application:</p> <pre><code>import patronus\n\n# Capture the returned context\npatronus_context = patronus.init()  # Also sets context globally\n\n# Direct access is possible but not typically needed\ntracer_provider = patronus_context.tracer_provider\napi_client = patronus_context.api_client\nscope = patronus_context.scope\n</code></pre> <p>See the <code>PatronusContext</code> API reference for the complete list of available components and their descriptions.</p> <p>This context is particularly useful when integrating with OpenTelemetry instrumentation libraries that require explicit tracer provider configuration, such as in distributed tracing scenarios.</p>"},{"location":"getting-started/initialization/#manual-context-management","title":"Manual Context Management","text":"<p>For advanced use cases, you can build and manage contexts manually using <code>build_context()</code> and the context manager pattern:</p> <pre><code>from patronus.init import build_context\nfrom patronus import context\n\n# Build a context manually with custom configuration\ncustom_context = build_context(...)\n\n# Use the context temporarily without setting it globally\nwith context._CTX_PAT.using(custom_context):\n    # All Patronus SDK operations within this block use custom_context\n    result = some_patronus_operation()\n# Context reverts to previous state after exiting the block\n</code></pre> <p>This pattern is particularly useful when you need to send data to multiple projects within the same process, or when building testing frameworks that require isolated contexts.</p>"},{"location":"getting-started/initialization/#next-steps","title":"Next Steps","text":"<p>Now that you've installed the Patronus SDK, proceed to the Quickstart guide to learn how to use it effectively.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>The Patronus SDK provides tools for evaluating, monitoring, and improving LLM applications.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>A package manager (uv or pip)</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":""},{"location":"getting-started/installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>uv is a fast Python package installer and resolver:</p> <pre><code>uv add patronus\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install patronus\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#for-experiments","title":"For Experiments","text":"<p>To use Patronus experiments functionality (including pandas support):</p> <pre><code># Using uv\nuv add \"patronus[experiments]\"\n\n# Using pip\npip install \"patronus[experiments]\"\n</code></pre>"},{"location":"getting-started/installation/#quick-start-with-examples","title":"Quick Start with Examples","text":"<p>If you'd like to see Patronus in action quickly, check out our examples. These examples demonstrate how to use Patronus with various LLM frameworks and APIs.</p> <p>For instance, to run the Smolagents weather example:</p> <pre><code># Export required API keys\nexport PATRONUS_API_KEY=your-api-key\nexport OPENAI_API_KEY=your-api-key\n\n# Run the example with uv\nuv run --no-cache --with \"patronus-examples[smolagents]\" \\\n    -m patronus_examples.tracking.smolagents_weather\n</code></pre> <p>See the examples documentation for more detailed information on running and understanding the available examples.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with the Patronus SDK through three practical examples. We'll explore tracing, evaluation, and experimentation to give you a hands-on introduction to the core features.</p>"},{"location":"getting-started/quickstart/#initialization","title":"Initialization","text":"<p>Before running any of the examples, initialize the Patronus SDK:</p> <pre><code>import os\nimport patronus\n\n# Initialize with your API key\npatronus.init(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"PATRONUS_API_KEY\")\n)\n</code></pre> <p>You can also use a configuration file instead of direct initialization:</p> <pre><code># patronus.yaml\n\napi_key: \"your-api-key\"\nproject_name:  \"Global\"\napp: \"default\"\n</code></pre> <p>For experiments, you don't need to explicitly call <code>init()</code> as <code>run_experiment()</code> handles initialization automatically.</p>"},{"location":"getting-started/quickstart/#example-1-tracing-with-a-functional-evaluator","title":"Example 1: Tracing with a Functional Evaluator","text":"<p>This example demonstrates how to trace function execution and create a simple functional evaluator.</p> <pre><code>import patronus\nfrom patronus import evaluator, traced\n\npatronus.init()\n\n@evaluator()\ndef exact_match(expected: str, actual: str) -&gt; bool:\n    return expected.strip() == actual.strip()\n\n@traced()\ndef process_query(query: str) -&gt; str:\n    # In a real application, this would call an LLM\n    return f\"Processed response for: {query}\"\n\n# Use the traced function and evaluator together\n@traced()\ndef main():\n    query = \"What is machine learning?\"\n    response = process_query(query)\n    print(f\"Response: {response}\")\n\n    expected_response = \"Processed response for: What is machine learning?\"\n    result = exact_match(expected_response, response)\n    print(f\"Evaluation result: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>In this example:</p> <ol> <li>We created a simple <code>exact_match</code> evaluator using the <code>@evaluator()</code> decorator</li> <li>We traced the <code>process_query</code> function using the <code>@traced()</code> decorator</li> <li>We ran an evaluation by calling the evaluator function directly</li> </ol> <p>The tracing will automatically capture execution details, timing, and results, making them available in the Patronus platform.</p>"},{"location":"getting-started/quickstart/#example-2-using-a-patronus-evaluator","title":"Example 2: Using a Patronus Evaluator","text":"<p>This example shows how to use a Patronus Evaluator to assess model outputs for hallucinations.</p> <pre><code>import patronus\nfrom patronus import traced\nfrom patronus.evals import RemoteEvaluator\n\npatronus.init()\n\n\n@traced()\ndef generate_insurance_response(query: str) -&gt; str:\n    # In a real application, this would call an LLM\n    return \"To even qualify for our car insurance policy, you need to have a valid driver's license that expires later than 2028.\"\n\n\n@traced(\"Quickstart: detect hallucination\")\ndef main():\n    check_hallucinates = RemoteEvaluator(\"lynx\", \"patronus:hallucination\")\n\n    context = \"\"\"\n    To qualify for our car insurance policy, you need a way to show competence\n    in driving which can be accomplished through a valid driver's license.\n    You must have multiple years of experience and cannot be graduating from driving school before or on 2028.\n    \"\"\"\n\n    query = \"What is the car insurance policy?\"\n    response = generate_insurance_response(query)\n    print(f\"Query: {query}\")\n    print(f\"Response: {response}\")\n\n    # Evaluate the response for hallucinations\n    resp = check_hallucinates.evaluate(\n        task_input=query,\n        task_context=context,\n        task_output=response\n    )\n\n    # Print the evaluation results\n    print(f\"\"\"\nHallucination evaluation:\nPassed: {resp.pass_}\nScore: {resp.score}\nExplanation: {resp.explanation}\n\"\"\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>In this example:</p> <ol> <li>We created a traced function generate_insurance_response to simulate an LLM response</li> <li>We used the Patronus Lynx Evaluator</li> <li>We evaluated whether the response contains information not supported by the context</li> <li>We displayed the detailed evaluation results</li> </ol> <p>Patronus Evaluators run on Patronus infrastructure and provide sophisticated assessment capabilities without requiring you to implement complex evaluation logic.</p>"},{"location":"getting-started/quickstart/#example-3-running-an-experiment-with-openai","title":"Example 3: Running an Experiment with OpenAI","text":"<p>This example demonstrates how to run a comprehensive experiment to evaluate OpenAI model performance across multiple samples and criteria.</p> <p>Before running Example 3, you'll need to install Pandas and the OpenAI SDK and OpenInference instrumentation:</p> <pre><code>pip install pandas openai openinference-instrumentation-openai\n</code></pre> <p>The OpenInference instrumentation automatically adds spans for all OpenAI API calls, capturing prompts, responses, and model parameters without any code changes. These details will appear in your Patronus traces for complete visibility into model interactions.</p> <pre><code>from typing import Optional\nimport os\n\nimport patronus\nfrom patronus.evals import evaluator, RemoteEvaluator, EvaluationResult\nfrom patronus.experiments import run_experiment, FuncEvaluatorAdapter, Row, TaskResult\nfrom openai import OpenAI\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\noai = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\npatronus.init()\n\n\n@evaluator()\ndef fuzzy_match(row: Row, task_result: TaskResult, **kwargs) -&gt; Optional[EvaluationResult]:\n    if not row.gold_answer or not task_result:\n        return None\n\n    gold_answer = row.gold_answer.lower()\n    response = task_result.output.lower()\n\n    key_terms = [term.strip() for term in gold_answer.split(',')]\n    matches = sum(1 for term in key_terms if term in response)\n    match_ratio = matches / len(key_terms) if key_terms else 0\n\n    # Return a score between 0-1 indicating match quality\n    return EvaluationResult(\n        pass_=match_ratio &gt; 0.7,\n        score=match_ratio,\n    )\n\n\ndef rag_task(row, **kwargs):\n    # In a real RAG system, this would retrieve context before calling the LLM\n    prompt = f\"\"\"\n    Based on the following context, answer the question.\n\n    Context:\n    {row.task_context}\n\n    Question: {row.task_input}\n\n    Answer:\n    \"\"\"\n\n    # Call OpenAI to generate a response\n    response = oai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\",\n             \"content\": \"You are a helpful assistant that answers questions based only on the provided context.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3,\n        max_tokens=150\n    )\n\n    return response.choices[0].message.content\n\n\ntest_data = [\n    {\n        \"task_input\": \"What is the main impact of climate change on coral reefs?\",\n        \"task_context\": \"\"\"\n        Climate change affects coral reefs through several mechanisms. Rising sea temperatures can cause coral bleaching,\n        where corals expel their symbiotic algae and turn white, often leading to death. Ocean acidification, caused by\n        increased CO2 absorption, makes it harder for corals to build their calcium carbonate structures. Sea level rise\n        can reduce light availability for photosynthesis. More frequent and intense storms damage reef structures. The\n        combination of these stressors is devastating to coral reef ecosystems worldwide.\n        \"\"\",\n        \"gold_answer\": \"coral bleaching, ocean acidification, reduced calcification, habitat destruction\"\n    },\n    {\n        \"task_input\": \"How do quantum computers differ from classical computers?\",\n        \"task_context\": \"\"\"\n        Classical computers process information in bits (0s and 1s), while quantum computers use quantum bits or qubits.\n        Qubits can exist in multiple states simultaneously thanks to superposition, allowing quantum computers to process\n        vast amounts of information in parallel. Quantum entanglement enables qubits to be correlated in ways impossible\n        for classical bits. While classical computers excel at everyday tasks, quantum computers potentially have advantages\n        for specific problems like cryptography, simulation of quantum systems, and certain optimization tasks. However,\n        quantum computers face significant challenges including qubit stability, error correction, and scaling up to useful sizes.\n        \"\"\",\n        \"gold_answer\": \"qubits instead of bits, superposition, entanglement, parallel processing\"\n    }\n]\n\nevaluators = [\n    FuncEvaluatorAdapter(fuzzy_match),\n    RemoteEvaluator(\"answer-relevance\", \"patronus:answer-relevance\")\n]\n\n# Run the experiment with OpenInference instrumentation\nprint(\"Running RAG evaluation experiment...\")\nexperiment = run_experiment(\n    dataset=test_data,\n    task=rag_task,\n    evaluators=evaluators,\n    tags={\"system\": \"rag-prototype\", \"model\": \"gpt-3.5-turbo\"},\n    integrations=[OpenAIInstrumentor()]\n)\n\n# Export results to CSV (optional)\n# experiment.to_csv(\"rag_evaluation_results.csv\")\n</code></pre> <p>In this example:</p> <ol> <li>We defined a task function <code>answer_questions</code> that generates responses for our experiment</li> <li>We created a custom evaluator <code>contains_key_information</code> to check for specific content</li> <li>We set up an experiment with multiple evaluators (both remote and custom)</li> <li>We ran the experiment across a dataset of questions</li> </ol> <p>Experiments provide a powerful way to systematically evaluate your LLM applications across multiple samples and criteria, helping you identify strengths and weaknesses in your models.</p>"},{"location":"integrations/agents/","title":"Agent Integrations","text":"<p>The Patronus SDK provides integrations with various agent frameworks to enable observability, evaluation, and experimentation with agent-based LLM applications.</p>"},{"location":"integrations/agents/#pydantic-ai","title":"Pydantic AI","text":"<p>Pydantic AI is a framework for building AI agents with type-safe tools and structured outputs. The Patronus SDK provides a dedicated integration that automatically instruments all Pydantic AI agents for observability.</p>"},{"location":"integrations/agents/#installation","title":"Installation","text":"<p>Make sure you have both the Patronus SDK and Pydantic AI installed:</p> <pre><code>pip install patronus pydantic-ai\n</code></pre>"},{"location":"integrations/agents/#usage","title":"Usage","text":"<p>To enable Pydantic AI integration with Patronus:</p> <pre><code>from patronus import init\nfrom patronus.integrations.pydantic_ai import PydanticAIIntegrator\n\n# Initialize Patronus with the Pydantic AI integration\npatronus_ctx = init(\n    integrations=[PydanticAIIntegrator()]\n)\n\n# Now all Pydantic AI agents will automatically send telemetry to Patronus\n</code></pre>"},{"location":"integrations/agents/#configuration-options","title":"Configuration Options","text":"<p>The <code>PydanticAIIntegrator</code> accepts the following parameters:</p> <ul> <li><code>event_mode</code>: Controls how agent events are captured<ul> <li><code>\"logs\"</code> (default): Captures events as logs, which works best with the Patronus Platform</li> <li><code>\"attributes\"</code>: Captures events as span attributes</li> </ul> </li> </ul> <p>Example with custom configuration:</p> <pre><code>from patronus import init\nfrom patronus.integrations.pydantic_ai import PydanticAIIntegrator\n\npatronus_ctx = init(\n    integrations=[PydanticAIIntegrator(event_mode=\"logs\")]\n)\n</code></pre>"},{"location":"integrations/llms/","title":"LLM Integrations","text":"<p>The Patronus SDK provides integrations with various LLM providers to enable observability, evaluation, and experimentation with LLM applications.</p>"},{"location":"integrations/llms/#opentelemetry-llm-instrumentors","title":"OpenTelemetry LLM Instrumentors","text":"<p>Patronus supports any OpenTelemetry-based LLM instrumentation. This allows you to easily capture telemetry data from your LLM interactions and send it to the Patronus platform for analysis.</p> <p>A popular option for LLM instrumentation is OpenInference, which provides instrumentors for multiple LLM providers.</p>"},{"location":"integrations/llms/#anthropic-claude-integration","title":"Anthropic Claude Integration","text":"<p>To instrument Anthropic's Claude API calls:</p> <pre><code># Install the required package\npip install openinference-instrumentation-anthropic\n</code></pre> <pre><code>from patronus import init\nfrom openinference.instrumentation.anthropic import AnthropicInstrumentor\n\n# Initialize Patronus with Anthropic instrumentation\npatronus_ctx = init(\n    integrations=[AnthropicInstrumentor()]\n)\n\n# Now all Claude API calls will be automatically instrumented\n# and the telemetry will be sent to Patronus\n</code></pre>"},{"location":"integrations/llms/#openai-integration","title":"OpenAI Integration","text":"<p>To instrument OpenAI API calls:</p> <pre><code># Install the required package\npip install openinference-instrumentation-openai\n</code></pre> <pre><code>from patronus import init\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\n# Initialize Patronus with OpenAI instrumentation\npatronus_ctx = init(\n    integrations=[OpenAIInstrumentor()]\n)\n\n# Now all OpenAI API calls will be automatically instrumented\n# and the telemetry will be sent to Patronus\n</code></pre>"},{"location":"integrations/llms/#using-multiple-llm-instrumentors","title":"Using Multiple LLM Instrumentors","text":"<p>You can combine multiple instrumentors to capture telemetry from different LLM providers:</p> <pre><code>from patronus import init\nfrom openinference.instrumentation.anthropic import AnthropicInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\n# Initialize Patronus with multiple LLM instrumentors\npatronus_ctx = init(\n    project_name=\"my-multi-llm-project\",\n    app=\"llm-application\",\n    integrations=[\n        AnthropicInstrumentor(),\n        OpenAIInstrumentor()\n    ]\n)\n\n# Now both Anthropic and OpenAI API calls will be automatically instrumented\n</code></pre>"},{"location":"observability/configuration/","title":"Observability Configuration","text":""},{"location":"observability/configuration/#exporter-protocols","title":"Exporter Protocols","text":"<p>The SDK supports two OTLP exporter protocols:</p> Protocol Value Default Endpoint Available Ports gRPC <code>grpc</code> <code>https://otel.patronus.ai:4317</code> 4317 HTTP <code>http/protobuf</code> <code>https://otel.patronus.ai:4318</code> 4318, 443"},{"location":"observability/configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"observability/configuration/#1-patronus-configuration","title":"1. Patronus Configuration","text":"<pre><code>patronus.init(\n    otel_endpoint=\"https://otel.patronus.ai:4318\",\n    otel_exporter_otlp_protocol=\"http/protobuf\"\n)\n</code></pre> <pre><code># patronus.yaml\notel_endpoint: \"https://otel.patronus.ai:4318\"\notel_exporter_otlp_protocol: \"http/protobuf\"\n</code></pre> <pre><code>export PATRONUS_OTEL_ENDPOINT=\"https://otel.patronus.ai:4318\"\nexport PATRONUS_OTEL_EXPORTER_OTLP_PROTOCOL=\"http/protobuf\"\n</code></pre>"},{"location":"observability/configuration/#2-opentelemetry-environment-variables","title":"2. OpenTelemetry Environment Variables","text":"<pre><code># General (applies to all signals)\nexport OTEL_EXPORTER_OTLP_PROTOCOL=\"grpc\"\n\n# Signal-specific\nexport OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=\"http/protobuf\"\nexport OTEL_EXPORTER_OTLP_LOGS_PROTOCOL=\"grpc\"\n</code></pre>"},{"location":"observability/configuration/#configuration-priority","title":"Configuration Priority","text":"<ol> <li>Function parameters</li> <li>Environment variables (<code>PATRONUS_OTEL_EXPORTER_OTLP_PROTOCOL</code>)</li> <li>Configuration file (<code>patronus.yaml</code>)</li> <li><code>OTEL_EXPORTER_OTLP_TRACES_PROTOCOL</code> / <code>OTEL_EXPORTER_OTLP_LOGS_PROTOCOL</code></li> <li><code>OTEL_EXPORTER_OTLP_PROTOCOL</code></li> <li>Default: <code>grpc</code></li> </ol>"},{"location":"observability/configuration/#endpoint-configuration","title":"Endpoint Configuration","text":""},{"location":"observability/configuration/#custom-endpoints","title":"Custom Endpoints","text":"<pre><code>patronus.init(\n    otel_endpoint=\"https://collector.example.com:4317\",\n    otel_exporter_otlp_protocol=\"grpc\"\n)\n</code></pre>"},{"location":"observability/configuration/#connection-security","title":"Connection Security","text":"<p>Security is determined by the URL scheme for both gRPC and HTTP protocols:</p> <ul> <li><code>https://</code> - Secure connection (TLS)</li> <li><code>http://</code> - Insecure connection</li> </ul> <pre><code># Secure gRPC\npatronus.init(otel_endpoint=\"https://collector.example.com:4317\")\n\n# Insecure gRPC\npatronus.init(otel_endpoint=\"http://collector.example.com:4317\")\n\n# Secure HTTP\npatronus.init(\n    otel_endpoint=\"https://collector.example.com:4318\",\n    otel_exporter_otlp_protocol=\"http/protobuf\"\n)\n\n# Insecure HTTP\npatronus.init(\n    otel_endpoint=\"http://collector.example.com:4318\",\n    otel_exporter_otlp_protocol=\"http/protobuf\"\n)\n</code></pre>"},{"location":"observability/configuration/#http-path-handling","title":"HTTP Path Handling","text":"<p>For HTTP protocol, paths are automatically appended:</p> <ul> <li>Traces: <code>&lt;endpoint&gt;/v1/traces</code></li> <li>Logs: <code>&lt;endpoint&gt;/v1/logs</code></li> </ul>"},{"location":"observability/configuration/#examples","title":"Examples","text":""},{"location":"observability/configuration/#http-protocol-with-custom-endpoint","title":"HTTP Protocol with Custom Endpoint","text":"<pre><code>patronus.init(\n    otel_endpoint=\"http://internal-collector:8080\",\n    otel_exporter_otlp_protocol=\"http/protobuf\"\n)\n</code></pre>"},{"location":"observability/configuration/#http-protocol-on-standard-https-port","title":"HTTP Protocol on Standard HTTPS Port","text":"<pre><code>patronus.init(\n    otel_endpoint=\"https://otel.example.com:443\",\n    otel_exporter_otlp_protocol=\"http/protobuf\"\n)\n</code></pre>"},{"location":"observability/configuration/#grpc-with-insecure-connection","title":"gRPC with Insecure Connection","text":"<pre><code>patronus.init(\n    otel_endpoint=\"http://internal-collector:4317\",\n    otel_exporter_otlp_protocol=\"grpc\"\n)\n</code></pre>"},{"location":"observability/configuration/#mixed-protocols","title":"Mixed Protocols","text":"<pre><code>export OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=\"http/protobuf\"\nexport OTEL_EXPORTER_OTLP_LOGS_PROTOCOL=\"grpc\"\n</code></pre>"},{"location":"observability/logging/","title":"Logging","text":"<p>Logging is an essential feature of the Patronus SDK that allows you to record events, debug information, and track the execution of your LLM applications. This page covers how to set up and use logging in your code.</p> <p>Configuration</p> <p>For information about configuring observability features, including exporter protocols and endpoints, see the Observability Configuration guide.</p>"},{"location":"observability/logging/#getting-started-with-logging","title":"Getting Started with Logging","text":"<p>The Patronus SDK provides a simple logging interface that integrates with Python's standard logging module while also automatically exporting logs to the Patronus AI Platform:</p> <pre><code>import patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\n# Basic logging\nlog.info(\"Processing user query\")\n\n# Different log levels are available\nlog.debug(\"Detailed debug information\")\nlog.warning(\"Something might be wrong\")\nlog.error(\"An error occurred\")\nlog.critical(\"System cannot continue\")\n</code></pre>"},{"location":"observability/logging/#configuring-console-output","title":"Configuring Console Output","text":"<p>By default, Patronus logs are sent to the Patronus AI Platform but are not printed to the console. To display logs in your console output, you can add a standard Python logging handler:</p> <pre><code>import sys\nimport logging\nimport patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\n# Add a console handler to see logs in your terminal\nconsole_handler = logging.StreamHandler(sys.stdout)\nlog.addHandler(console_handler)\n\n# Now logs will appear in both console and Patronus Platform\nlog.info(\"This message appears in the console and is sent to Patronus\")\n</code></pre> <p>You can also customize the format of console logs:</p> <pre><code>import sys\nimport logging\nimport patronus\n\npatronus.init()\nlog = patronus.get_logger()\n\nformatter = logging.Formatter('[%(asctime)s] %(levelname)-8s: %(message)s')\n\nconsole_handler = logging.StreamHandler(sys.stdout)\nconsole_handler.setFormatter(formatter)\nlog.addHandler(console_handler)\n\n# Logs will now include timestamp and level\nlog.info(\"Formatted log message\")\n</code></pre>"},{"location":"observability/logging/#advanced-configuration","title":"Advanced Configuration","text":"<p>Patronus integrates with Python's logging module, allowing for advanced configuration options. The SDK uses two main loggers:</p> <ul> <li><code>patronus.sdk</code> - For client-emitted messages that are automatically exported to the Patronus AI Platform</li> <li><code>patronus.core</code> - For library-emitted messages related to the SDK's internal operations</li> </ul> <p>Here's how to configure these loggers using standard library methods:</p> <pre><code>import logging\nimport patronus\n\n# Initialize Patronus before configuring logging\npatronus.init()\n\n# Configure the root Patronus logger\npatronus_root_logger = logging.getLogger(\"patronus\")\npatronus_root_logger.setLevel(logging.WARNING)  # Set base level for all Patronus loggers\n\n# Add a console handler with custom formatting\nconsole_handler = logging.StreamHandler()\nformatter = logging.Formatter(\n    fmt='[%(asctime)s] %(levelname)-8s %(name)s: %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nconsole_handler.setFormatter(formatter)\npatronus_root_logger.addHandler(console_handler)\n\n# Configure specific loggers\npatronus_core_logger = logging.getLogger(\"patronus.core\")\npatronus_core_logger.setLevel(logging.WARNING)  # Only show warnings and above for internal SDK messages\n\npatronus_sdk_logger = logging.getLogger(\"patronus.sdk\")\npatronus_sdk_logger.setLevel(logging.INFO)  # Show info and above for your application logs\n</code></pre>"},{"location":"observability/logging/#logging-with-traces","title":"Logging with Traces","text":"<p>Patronus logging integrates seamlessly with the tracing system, allowing you to correlate logs with specific spans in your application flow:</p> <pre><code>import patronus\nfrom patronus import traced, start_span\n\npatronus.init()\nlog = patronus.get_logger()\n\n@traced()\ndef process_user_query(query):\n    log.info(\"Processing query\")\n\n    with start_span(\"Query Analysis\"):\n        log.info(\"Analyzing query intent\")\n        ...\n\n    with start_span(\"Response Generation\"):\n        log.info(\"Generating LLM response\")\n        ...\n\n    return \"Response to: \" + query\n\n# Logs will be associated with the appropriate spans\nresult = process_user_query(\"Tell me about machine learning\")\n</code></pre>"},{"location":"observability/tracing/","title":"Tracing","text":"<p>Tracing is a core feature of the Patronus SDK that allows you to monitor and understand the behavior of your LLM applications. This page covers how to set up and use tracing in your code.</p> <p>Configuration</p> <p>For information about configuring observability features, including exporter protocols and endpoints, see the Observability Configuration guide.</p>"},{"location":"observability/tracing/#getting-started-with-tracing","title":"Getting Started with Tracing","text":"<p>Tracing in Patronus works through two main mechanisms:</p> <ol> <li>Function decorators: Easily trace entire functions</li> <li>Context managers: Trace specific code blocks within functions</li> </ol>"},{"location":"observability/tracing/#using-the-traced-decorator","title":"Using the <code>@traced()</code> Decorator","text":"<p>The simplest way to add tracing is with the <code>@traced()</code> decorator:</p> <pre><code>import patronus\nfrom patronus import traced\n\npatronus.init()\n\n@traced()\ndef generate_response(prompt: str) -&gt; str:\n    # Your LLM call or processing logic here\n    return f\"Response to: {prompt}\"\n\n# Call the traced function\nresult = generate_response(\"Tell me about machine learning\")\n</code></pre>"},{"location":"observability/tracing/#decorator-options","title":"Decorator Options","text":"<p>The <code>@traced()</code> decorator accepts several parameters for customization:</p> <pre><code>@traced(\n    span_name=\"Custom span name\",   # Default: function name\n    log_args=True,                  # Whether to log function arguments\n    log_results=True,               # Whether to log function return values\n    log_exceptions=True,            # Whether to log exceptions\n    disable_log=False,              # Completely disable logging (maintains spans)\n    attributes={\"key\": \"value\"}     # Custom attributes to add to the span\n)\ndef my_function():\n    pass\n</code></pre> <p>See the API Reference for complete details.</p>"},{"location":"observability/tracing/#using-the-start_span-context-manager","title":"Using the <code>start_span()</code> Context Manager","text":"<p>For more granular control, use the <code>start_span()</code> context manager to trace specific blocks of code:</p> <pre><code>import patronus\nfrom patronus.tracing import start_span\n\npatronus.init()\n\ndef complex_workflow(data):\n    # First phase\n    with start_span(\"Data preparation\", attributes={\"data_size\": len(data)}):\n        prepared_data = preprocess(data)\n\n    # Second phase\n    with start_span(\"Model inference\"):\n        results = run_model(prepared_data)\n\n    # Third phase\n    with start_span(\"Post-processing\"):\n        final_results = postprocess(results)\n\n    return final_results\n</code></pre>"},{"location":"observability/tracing/#context-manager-options","title":"Context Manager Options","text":"<p>The <code>start_span()</code> context manager accepts these parameters:</p> <pre><code>with start_span(\n    \"Span name\",                        # Name of the span (required)\n    record_exception=False,             # Whether to record exceptions\n    attributes={\"custom\": \"attribute\"}  # Custom attributes to add\n) as span:\n    # Your code here\n    # You can also add attributes during execution:\n    span.set_attribute(\"dynamic_value\", 42)\n</code></pre> <p>See the API Reference for complete details.</p>"},{"location":"observability/tracing/#custom-attributes","title":"Custom Attributes","text":"<p>Both tracing methods allow you to add custom attributes that provide additional context for your traces:</p> <pre><code>@traced(attributes={\n    \"model\": \"gpt-4\",\n    \"version\": \"1.0\",\n    \"temperature\": 0.7\n})\ndef generate_with_gpt4(prompt):\n    # Function implementation\n    pass\n\n# Or with context manager\nwith start_span(\"Query processing\", attributes={\n    \"query_type\": \"search\",\n    \"filters_applied\": True,\n    \"result_limit\": 10\n}):\n    # Processing code\n    pass\n</code></pre>"},{"location":"observability/tracing/#distributed-tracing","title":"Distributed Tracing","text":"<p>The Patronus SDK is built on OpenTelemetry and automatically supports context propagation across distributed services. This enables you to trace requests as they flow through multiple services in your application architecture. The OpenTelemetry Python Contrib repository provides instrumentation for many popular frameworks and libraries.</p>"},{"location":"observability/tracing/#example-fastapi-services-with-context-propagation","title":"Example: FastAPI Services with Context Propagation","text":"<p>First, install the required dependencies:</p> <pre><code>uv add opentelemetry-instrumentation-httpx \\\n    opentelemetry-instrumentation-fastapi \\\n    fastapi[all] \\\n    patronus\n</code></pre> <p>Here's a complete example showing two FastAPI services with automatic trace context propagation:</p> <p>Backend Service (<code>service_backend.py</code>):</p> <pre><code>import patronus\nfrom fastapi import FastAPI\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\n\n# Initialize Patronus SDK\npatronus_context = patronus.init(service=\"backend\")\n\napp = FastAPI(title=\"Backend Service\")\n\n@app.get(\"/hello/{name}\")\nasync def hello_backend(name: str):\n    return {\n        \"message\": f\"Hello {name} from Backend Service!\",\n        \"service\": \"backend\"\n    }\n\n# Instrument FastAPI after Patronus initialization\nFastAPIInstrumentor.instrument_app(app, tracer_provider=patronus_context.tracer_provider)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n</code></pre> <p>Gateway Service (<code>service_gateway.py</code>):</p> <pre><code>import httpx\nimport patronus\nfrom fastapi import FastAPI\nfrom opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\n\n# Initialize Patronus SDK with HTTPX instrumentation\npatronus_context = patronus.init(\n    service=\"gateway\",\n    integrations=[\n        HTTPXClientInstrumentor(),\n    ]\n)\n\napp = FastAPI(title=\"Gateway Service\")\n\n@app.get(\"/hello/{name}\")\nasync def hello_gateway(name: str):\n    # This HTTP call will automatically propagate trace context\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"http://localhost:8001/hello/{name}\")\n        backend_data = response.json()\n\n    return {\n        \"gateway_message\": f\"Gateway received request for {name}\",\n        \"backend_response\": backend_data\n    }\n\n# Instrument FastAPI after Patronus initialization\nFastAPIInstrumentor.instrument_app(app, tracer_provider=patronus_context.tracer_provider)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"observability/tracing/#running-the-example","title":"Running the Example","text":"<p>First, export your Patronus API key:</p> <pre><code>export PATRONUS_API_KEY=\"your-api-key\"\n</code></pre> <p>Then run the services:</p> <ol> <li>Start the backend: <code>python service_backend.py</code></li> <li>Start the gateway: <code>python service_gateway.py</code></li> <li>Make a request: <code>curl http://localhost:8000/hello/world</code></li> </ol> <p>After making the request, you should see the connected traces in the Patronus Platform showing the complete request flow from gateway to backend service.</p>"},{"location":"observability/tracing/#important-notes","title":"Important Notes","text":"<ul> <li>FastAPI instrumenter requires manual setup with <code>FastAPIInstrumentor.instrument_app()</code> after Patronus initialization</li> <li>Pass the <code>tracer_provider</code> from Patronus context to ensure proper integration</li> <li>Trace context is automatically propagated through HTTP headers when services are properly instrumented</li> </ul>"},{"location":"prompts/","title":"Prompt Management","text":"<p>The Patronus SDK provides tools to version, retrieve, and render prompts in your LLM applications.</p>"},{"location":"prompts/#quick-start","title":"Quick Start","text":""},{"location":"prompts/#creating-a-prompt","title":"Creating a Prompt","text":"<pre><code>import patronus\nimport textwrap\nfrom patronus.prompts import Prompt, push_prompt\n\npatronus.init()\n\n# Create a new prompt\nprompt = Prompt(\n    name=\"support/troubleshooting/login-issues\",\n    body=textwrap.dedent(\"\"\"\n        You are a support specialist for {product_name}.\n        ISSUE: {issue_description}\n        TIER: {subscription_tier}\n\n        Provide a solution for this {issue_type} problem. Be concise.\n        Include steps and end with an offer for further help.\n        \"\"\"),\n    description=\"Support prompt for login issues\",\n    metadata={\"temperature\": 0.7, \"tone\": \"helpful\"}\n)\n\n# Push the prompt to Patronus\nloaded_prompt = push_prompt(prompt)\n\n# Render the prompt\nrendered = prompt.render(\n    issue_description=\"Cannot log in with correct credentials\",\n    product_name=\"CloudWorks\",\n    subscription_tier=\"Business\",\n    issue_type=\"authentication\"\n)\nprint(rendered)\n</code></pre>"},{"location":"prompts/#loading-a-prompt","title":"Loading a Prompt","text":"<pre><code>import patronus\nfrom patronus.prompts import load_prompt\n\npatronus.init()\n\n# Get the latest version of the prompt we just created\nprompt = load_prompt(name=\"support/troubleshooting/login-issues\")\n\n# Access metadata\nprint(prompt.metadata)\n\n# Render the prompt with different parameters\nrendered = prompt.render(\n    issue_description=\"Password reset link not working\",\n    product_name=\"CloudWorks\",\n    subscription_tier=\"Enterprise\",\n    issue_type=\"password reset\"\n)\nprint(rendered)\n</code></pre>"},{"location":"prompts/#loading-prompts","title":"Loading Prompts","text":"<p>Use <code>load_prompt</code> to retrieve prompts from the Patronus platform:</p> <pre><code>import patronus\nfrom patronus.prompts import load_prompt\n\npatronus.init()\n\n# Load an instruction prompt that doesn't need any parameters\nprompt = load_prompt(name=\"content/writing/blog-instructions\")\nrendered = prompt.render()\nprint(rendered)\n</code></pre> <p>For async applications:</p> <pre><code>from patronus.prompts import aload_prompt\n\nprompt = await aload_prompt(name=\"content/writing/blog-instructions\")\n</code></pre>"},{"location":"prompts/#loading-specific-versions","title":"Loading Specific Versions","text":"<p>Retrieve prompts by revision number or label:</p> <pre><code># Load a specific revision\nprompt = load_prompt(name=\"content/blog/technical-explainer\", revision=3)\n\n# Load by label (production environment)\nprompt = load_prompt(name=\"legal/contracts/privacy-policy\", label=\"production\")\n</code></pre>"},{"location":"prompts/#creating-and-updating-prompts","title":"Creating and Updating Prompts","text":"<p>Create new prompts using <code>push_prompt</code>:</p> <pre><code>from patronus.prompts import Prompt, push_prompt\n\nnew_prompt = Prompt(\n    name=\"dev/bug-fix/python-error\",\n    body=\"Fix this Python code error: {error_message}. Code: ```python\\n{code_snippet}\\n```\",\n    description=\"Template for Python debugging assistance\",\n    metadata={\n        \"creator\": \"dev-team\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 250\n    }\n)\n\nloaded_prompt = push_prompt(new_prompt)\n</code></pre> <p>For async applications:</p> <pre><code>from patronus.prompts import apush_prompt\n\nloaded_prompt = await apush_prompt(new_prompt)\n</code></pre> <p>The <code>push_prompt</code> function automatically handles duplicate detection - if a prompt with identical content already exists, it returns the existing revision instead of creating a new one.</p>"},{"location":"prompts/#rendering-prompts","title":"Rendering Prompts","text":"<p>Render prompts with variables:</p> <pre><code>rendered = prompt.render(user_query=\"How do I optimize database performance?\", expertise_level=\"intermediate\")\n</code></pre>"},{"location":"prompts/#template-engines","title":"Template Engines","text":"<p>Patronus supports multiple template engines:</p> <pre><code># F-string templating (default)\nrendered = prompt.with_engine(\"f-string\").render(**kwargs)\n\n# Mustache templating\nrendered = prompt.with_engine(\"mustache\").render(**kwargs)\n\n# Jinja2 templating\nrendered = prompt.with_engine(\"jinja2\").render(**kwargs)\n</code></pre>"},{"location":"prompts/#working-with-labels","title":"Working with Labels","text":"<p>Labels provide stable references to specific revisions:</p> <pre><code>from patronus import context\n\nclient = context.get_api_client().prompts\n\n# Add audience-specific labels\nclient.add_label(\n    prompt_id=\"prompt_123\",\n    revision=3,\n    label=\"technical-audience\"\n)\n\n# Update label to point to a new revision\nclient.add_label(\n    prompt_id=\"prompt_123\",\n    revision=5,\n    label=\"technical-audience\"\n)\n\n# Add environment label\nclient.add_label(\n    prompt_id=\"prompt_456\",\n    revision=2,\n    label=\"production\"\n)\n</code></pre>"},{"location":"prompts/#metadata-usage","title":"Metadata Usage","text":"<p>Prompt revisions support arbitrary metadata:</p> <pre><code>from patronus.prompts import Prompt, push_prompt, load_prompt\n\n# Create with metadata\nprompt_with_meta = Prompt(\n    name=\"research/data-analysis/summarize-findings\",\n    body=\"Analyze the {data_type} data and summarize the key {metric_type} trends in {time_period}.\",\n    metadata={\n        \"models\": [\"gpt-4\", \"claude-3\"],\n        \"created_by\": \"data-team\",\n        \"tags\": [\"data\", \"analysis\"]\n    }\n)\n\nloaded_prompt = push_prompt(prompt_with_meta)\n\n# Access metadata\nprompt = load_prompt(name=\"research/data-analysis/summarize-findings\")\nsupported_models = prompt.metadata.get(\"models\", [])\ncreator = prompt.metadata.get(\"created_by\", \"unknown\")\n\nprint(f\"Prompt supports models: {', '.join(supported_models)}\")\nprint(f\"Created by: {creator}\")\n</code></pre>"},{"location":"prompts/#using-multiple-prompts-together","title":"Using Multiple Prompts Together","text":"<p>Complex applications often use multiple prompts together:</p> <pre><code>import patronus\nfrom patronus.prompts import load_prompt\nimport openai\n\npatronus.init()\n\n# Load different prompt components\nsystem_prompt = load_prompt(name=\"support/chat/system\")\nuser_query_template = load_prompt(name=\"support/chat/user-message\")\nresponse_formatter = load_prompt(name=\"support/chat/response-format\")\n\n# Create OpenAI client\nclient = openai.OpenAI()\n\n# Combine the prompts in a chat completion\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt.render(\n            product_name=\"CloudWorks Pro\",\n            available_features=[\"file sharing\", \"collaboration\", \"automation\"],\n            knowledge_cutoff=\"2024-05-01\"\n        )},\n        {\"role\": \"user\", \"content\": user_query_template.render(\n            user_name=\"Alex\",\n            user_tier=\"premium\",\n            user_query=\"How do I share files with external users?\"\n        )}\n    ],\n    temperature=0.7,\n    max_tokens=500\n)\n\n# Post-process the response using another prompt\nformatted_response = response_formatter.render(\n    raw_response=response.choices[0].message.content,\n    user_name=\"Alex\",\n    add_examples=True\n)\n</code></pre>"},{"location":"prompts/#naming-conventions","title":"Naming Conventions","text":"<p>Use a descriptive, hierarchical naming structure similar to file paths. This makes prompts easier to organize, find, and manage:</p> <pre><code>[domain]/[use-case]/[component]/[prompt-type]\n</code></pre> <p>Where <code>[prompt-type]</code> indicates the intended role of the prompt in an LLM conversation (optional but recommended):</p> <ul> <li><code>system</code> - Sets the overall behavior, persona, or context for the model</li> <li><code>instruction</code> - Provides specific instructions for a task</li> <li><code>user</code> - Represents a user message template</li> <li><code>assistant</code> - Template for assistant responses</li> <li><code>few-shot</code> - Contains examples of input/output pairs</li> </ul> <p>Examples:</p> <ul> <li><code>support/troubleshooting/diagnostic-questions/system</code></li> <li><code>marketing/email-campaigns/follow-up-template/instruction</code></li> <li><code>dev/code-generation/python-function/instruction</code></li> <li><code>finance/report/quarterly-analysis</code></li> <li><code>content/blog-post/technical-tutorial/few-shot</code></li> <li><code>legal/contracts/terms-of-service-v2/system</code></li> </ul> <p>Including the prompt type in the name helps team members quickly understand the intended usage context in multi-prompt conversations.</p>"},{"location":"prompts/#consistent-prefixes","title":"Consistent Prefixes","text":"<p>Use consistent prefixes for prompts that work together in the same feature:</p> <pre><code># Onboarding chat prompts share the prefix onboarding/chat/\nonboarding/chat/welcome/system\nonboarding/chat/questions/user\nonboarding/chat/intro/assistant\n\n# Support classifier prompts\nsupport/classifier/system\nsupport/classifier/categories/instruction\n</code></pre> <p>This approach simplifies filtering and management of related prompts, making it easier to maintain and evolve complete prompt flows as your library grows.</p>"},{"location":"prompts/#configuration","title":"Configuration","text":"<p>The default template engine can be configured during initialization:</p> <pre><code>import patronus\n\npatronus.init(\n    # Default template engine for all prompts\n    prompt_templating_engine=\"mustache\"\n)\n</code></pre> <p>For additional configuration options, see the Configuration page.</p>"},{"location":"prompts/#using-with-llms","title":"Using with LLMs","text":"<p>Prompts can be used with any LLM provider:</p> <pre><code>import patronus\nfrom patronus.prompts import load_prompt\nimport anthropic\n\npatronus.init()\n\nsystem_prompt = load_prompt(name=\"support/knowledge-base/technical-assistance\")\n\nclient = anthropic.Anthropic()\n\nresponse = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    system=system_prompt.render(\n        product_name=\"CloudWorks Pro\",\n        user_tier=\"enterprise\",\n        available_features=[\"advanced monitoring\", \"auto-scaling\", \"SSO integration\"]\n    ),\n    messages=[\n        {\"role\": \"user\", \"content\": \"How do I configure the load balancer for high availability?\"}\n    ]\n)\n</code></pre>"},{"location":"prompts/#additional-resources","title":"Additional Resources","text":"<p>While the SDK provides high-level, convenient access to Patronus functionality, you can also use the lower-level APIs for more direct control:</p> <ul> <li>REST API documentation - For direct HTTP access to the Patronus platform</li> <li>Patronus API Python library - A typed Python client for the REST API with both synchronous and asynchronous support</li> </ul>"}]}